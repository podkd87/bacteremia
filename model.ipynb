{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #\"last expr -> all로 바꾸면 전체가 나온다. \"\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import RMSprop,adam\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras_radam import RAdam\n",
    "from keras import layers\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import model_from_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from big_generator import file_generator \n",
    "from big_generator import file_generator_valid\n",
    "from big_generator import file_generator_cul\n",
    "from big_generator import generator_cul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = pd.read_csv(\"../learning/datafile/pd_baseline.csv\")\n",
    "baseline=baseline.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>admday</th>\n",
       "      <th>opday</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1166795967</td>\n",
       "      <td>20160815</td>\n",
       "      <td>20160816</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.413528</td>\n",
       "      <td>-1.099884</td>\n",
       "      <td>-1.087010</td>\n",
       "      <td>0.718659</td>\n",
       "      <td>0.324344</td>\n",
       "      <td>-0.807898</td>\n",
       "      <td>-2.006887</td>\n",
       "      <td>-0.548219</td>\n",
       "      <td>-1.284691</td>\n",
       "      <td>0.522189</td>\n",
       "      <td>6.899251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.689499</td>\n",
       "      <td>5.359809</td>\n",
       "      <td>9.749975</td>\n",
       "      <td>3.880541</td>\n",
       "      <td>5.110438</td>\n",
       "      <td>3.511160</td>\n",
       "      <td>4.511642</td>\n",
       "      <td>4.585694</td>\n",
       "      <td>4.943656</td>\n",
       "      <td>6.817242</td>\n",
       "      <td>8.336339</td>\n",
       "      <td>0.469939</td>\n",
       "      <td>0.744466</td>\n",
       "      <td>0.843393</td>\n",
       "      <td>0.651387</td>\n",
       "      <td>0.364311</td>\n",
       "      <td>2.386570</td>\n",
       "      <td>2.788405</td>\n",
       "      <td>2.472394</td>\n",
       "      <td>2.965544</td>\n",
       "      <td>2.699179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1166798192</td>\n",
       "      <td>20080712</td>\n",
       "      <td>20080714</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.954871</td>\n",
       "      <td>1.134563</td>\n",
       "      <td>1.207545</td>\n",
       "      <td>-0.929201</td>\n",
       "      <td>-1.071646</td>\n",
       "      <td>-1.836597</td>\n",
       "      <td>-1.640118</td>\n",
       "      <td>-1.309416</td>\n",
       "      <td>-0.516697</td>\n",
       "      <td>1.730198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.324238</td>\n",
       "      <td>4.610660</td>\n",
       "      <td>10.773179</td>\n",
       "      <td>8.050953</td>\n",
       "      <td>9.421379</td>\n",
       "      <td>4.675070</td>\n",
       "      <td>4.939130</td>\n",
       "      <td>6.741050</td>\n",
       "      <td>4.571187</td>\n",
       "      <td>15.599707</td>\n",
       "      <td>2.511181</td>\n",
       "      <td>9.069263</td>\n",
       "      <td>0.469939</td>\n",
       "      <td>0.744466</td>\n",
       "      <td>0.843393</td>\n",
       "      <td>0.651387</td>\n",
       "      <td>0.364311</td>\n",
       "      <td>4.311703</td>\n",
       "      <td>2.793397</td>\n",
       "      <td>2.057836</td>\n",
       "      <td>4.779967</td>\n",
       "      <td>2.363882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1166798478</td>\n",
       "      <td>20110510</td>\n",
       "      <td>20110513</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.684199</td>\n",
       "      <td>-0.976661</td>\n",
       "      <td>-0.954782</td>\n",
       "      <td>0.012433</td>\n",
       "      <td>2.278730</td>\n",
       "      <td>-1.420494</td>\n",
       "      <td>-1.786826</td>\n",
       "      <td>-0.979085</td>\n",
       "      <td>-0.708695</td>\n",
       "      <td>1.181103</td>\n",
       "      <td>5.156261</td>\n",
       "      <td>16.632401</td>\n",
       "      <td>14.205712</td>\n",
       "      <td>10.985126</td>\n",
       "      <td>23.161686</td>\n",
       "      <td>4.702001</td>\n",
       "      <td>4.761768</td>\n",
       "      <td>0.952568</td>\n",
       "      <td>8.202775</td>\n",
       "      <td>10.063358</td>\n",
       "      <td>6.281622</td>\n",
       "      <td>3.439430</td>\n",
       "      <td>1.209064</td>\n",
       "      <td>0.469939</td>\n",
       "      <td>0.744466</td>\n",
       "      <td>0.843393</td>\n",
       "      <td>0.651387</td>\n",
       "      <td>0.364311</td>\n",
       "      <td>2.386570</td>\n",
       "      <td>2.788405</td>\n",
       "      <td>2.472394</td>\n",
       "      <td>2.965544</td>\n",
       "      <td>2.699179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1166799387</td>\n",
       "      <td>20150319</td>\n",
       "      <td>20150324</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.503752</td>\n",
       "      <td>-1.214892</td>\n",
       "      <td>-1.242573</td>\n",
       "      <td>-0.458384</td>\n",
       "      <td>-0.140986</td>\n",
       "      <td>0.405736</td>\n",
       "      <td>0.780561</td>\n",
       "      <td>0.256065</td>\n",
       "      <td>0.891293</td>\n",
       "      <td>-0.246544</td>\n",
       "      <td>5.156261</td>\n",
       "      <td>16.632401</td>\n",
       "      <td>14.205712</td>\n",
       "      <td>10.985126</td>\n",
       "      <td>23.161686</td>\n",
       "      <td>4.702001</td>\n",
       "      <td>4.761768</td>\n",
       "      <td>0.952568</td>\n",
       "      <td>8.202775</td>\n",
       "      <td>10.063358</td>\n",
       "      <td>6.281622</td>\n",
       "      <td>3.439430</td>\n",
       "      <td>1.209064</td>\n",
       "      <td>0.469939</td>\n",
       "      <td>0.744466</td>\n",
       "      <td>0.843393</td>\n",
       "      <td>0.651387</td>\n",
       "      <td>0.364311</td>\n",
       "      <td>2.386570</td>\n",
       "      <td>2.788405</td>\n",
       "      <td>2.472394</td>\n",
       "      <td>2.965544</td>\n",
       "      <td>2.699179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1166799993</td>\n",
       "      <td>20151218</td>\n",
       "      <td>20151228</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.649885</td>\n",
       "      <td>0.663074</td>\n",
       "      <td>0.130138</td>\n",
       "      <td>-0.792448</td>\n",
       "      <td>0.567554</td>\n",
       "      <td>0.560499</td>\n",
       "      <td>-0.318424</td>\n",
       "      <td>-0.836695</td>\n",
       "      <td>-1.674190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.490709</td>\n",
       "      <td>11.508369</td>\n",
       "      <td>6.690341</td>\n",
       "      <td>7.921564</td>\n",
       "      <td>8.227402</td>\n",
       "      <td>4.967529</td>\n",
       "      <td>2.086301</td>\n",
       "      <td>3.116609</td>\n",
       "      <td>7.571434</td>\n",
       "      <td>8.872069</td>\n",
       "      <td>4.365661</td>\n",
       "      <td>11.989615</td>\n",
       "      <td>0.469939</td>\n",
       "      <td>0.744466</td>\n",
       "      <td>0.843393</td>\n",
       "      <td>0.651387</td>\n",
       "      <td>0.364311</td>\n",
       "      <td>3.293770</td>\n",
       "      <td>1.712832</td>\n",
       "      <td>1.584709</td>\n",
       "      <td>6.133195</td>\n",
       "      <td>4.380298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          idx    admday     opday  3  4  5  6  7         8         9  \\\n",
       "0  1166795967  20160815  20160816  1  1  0  0  0  0.413528 -1.099884   \n",
       "1  1166798192  20080712  20080714  0  0  1  0  0  0.954871  1.134563   \n",
       "2  1166798478  20110510  20110513  0  0  0  1  0  0.684199 -0.976661   \n",
       "3  1166799387  20150319  20150324  1  0  0  1  0  0.503752 -1.214892   \n",
       "4  1166799993  20151218  20151228  1  0  1  0  0  0.142857  0.649885   \n",
       "\n",
       "         10        11        12        13        14        15        16  \\\n",
       "0 -1.087010  0.718659  0.324344 -0.807898 -2.006887 -0.548219 -1.284691   \n",
       "1  1.207545 -0.929201 -1.071646 -1.836597 -1.640118 -1.309416 -0.516697   \n",
       "2 -0.954782  0.012433  2.278730 -1.420494 -1.786826 -0.979085 -0.708695   \n",
       "3 -1.242573 -0.458384 -0.140986  0.405736  0.780561  0.256065  0.891293   \n",
       "4  0.663074  0.130138 -0.792448  0.567554  0.560499 -0.318424 -0.836695   \n",
       "\n",
       "         17        18         19         20         21         22        23  \\\n",
       "0  0.522189  6.899251   0.000000  25.689499   5.359809   9.749975  3.880541   \n",
       "1  1.730198  0.000000  23.324238   4.610660  10.773179   8.050953  9.421379   \n",
       "2  1.181103  5.156261  16.632401  14.205712  10.985126  23.161686  4.702001   \n",
       "3 -0.246544  5.156261  16.632401  14.205712  10.985126  23.161686  4.702001   \n",
       "4 -1.674190  0.000000  11.490709  11.508369   6.690341   7.921564  8.227402   \n",
       "\n",
       "         24        25        26         27         28        29         30  \\\n",
       "0  5.110438  3.511160  4.511642   4.585694   4.943656  6.817242   8.336339   \n",
       "1  4.675070  4.939130  6.741050   4.571187  15.599707  2.511181   9.069263   \n",
       "2  4.761768  0.952568  8.202775  10.063358   6.281622  3.439430   1.209064   \n",
       "3  4.761768  0.952568  8.202775  10.063358   6.281622  3.439430   1.209064   \n",
       "4  4.967529  2.086301  3.116609   7.571434   8.872069  4.365661  11.989615   \n",
       "\n",
       "         31        32        33        34        35        36        37  \\\n",
       "0  0.469939  0.744466  0.843393  0.651387  0.364311  2.386570  2.788405   \n",
       "1  0.469939  0.744466  0.843393  0.651387  0.364311  4.311703  2.793397   \n",
       "2  0.469939  0.744466  0.843393  0.651387  0.364311  2.386570  2.788405   \n",
       "3  0.469939  0.744466  0.843393  0.651387  0.364311  2.386570  2.788405   \n",
       "4  0.469939  0.744466  0.843393  0.651387  0.364311  3.293770  1.712832   \n",
       "\n",
       "         38        39        40  \n",
       "0  2.472394  2.965544  2.699179  \n",
       "1  2.057836  4.779967  2.363882  \n",
       "2  2.472394  2.965544  2.699179  \n",
       "3  2.472394  2.965544  2.699179  \n",
       "4  1.584709  6.133195  4.380298  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "positive_folder = '../learning/dataset/time3_pos_total200107/'\n",
    "negative_folder = '../learning/dataset/time3_neg_total200107/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(positive_folder+\"list_time_X.txt\", \"rb\") as fp:\n",
    "    list_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"list_time_y.txt\", \"rb\") as fp:\n",
    "    list_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"list_time_static.txt\", \"rb\") as fp:\n",
    "    list_time_static=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_X.txt\", \"rb\") as fp:\n",
    "    valid_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_y.txt\", \"rb\") as fp:\n",
    "    valid_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_static.txt\", \"rb\") as fp:\n",
    "    valid_time_static=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_X.txt\", \"rb\") as fp:\n",
    "    test_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_y.txt\", \"rb\") as fp:\n",
    "    test_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_static.txt\", \"rb\") as fp:\n",
    "    test_time_static=pickle.load( fp)\n",
    "\n",
    "with open(negative_folder+\"list_time_Xn.txt\", \"rb\") as fp:\n",
    "    list_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"list_time_yn.txt\", \"rb\") as fp:\n",
    "    list_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"list_time_nstatic.txt\", \"rb\") as fp:\n",
    "    list_time_nstatic=pickle.load( fp)\n",
    "\n",
    "with open(negative_folder+\"valid_time_Xn.txt\", \"rb\") as fp:\n",
    "    valid_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"valid_time_yn.txt\", \"rb\") as fp:\n",
    "    valid_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"valid_time_staticn.txt\", \"rb\") as fp:\n",
    "    valid_time_staticn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_Xn.txt\", \"rb\") as fp:\n",
    "    test_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_yn.txt\", \"rb\") as fp:\n",
    "    test_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_staticn.txt\", \"rb\") as fp:\n",
    "    test_time_staticn=pickle.load( fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, window,  time_it, feature,\n",
    "                 true_X_time, true_X_static,true_y_total,\n",
    "                 false_X_time, false_X_static, false_y_total,\n",
    "                 list_time_Xn, list_time_yn, list_time_nstatic,\n",
    "                size, fraction, ratio, repeat):\n",
    "        'Initialization'\n",
    "        self.window = window\n",
    "        self.time_it = time_it\n",
    "        self.feature = feature\n",
    "        self.true_X_time = true_X_time\n",
    "        self.true_X_static = true_X_static\n",
    "        self.true_y_total = true_y_total\n",
    "        self.false_X_time = false_X_time\n",
    "        self.false_X_static = false_X_static\n",
    "        self.false_y_total = false_y_total\n",
    "        self.list_time_Xn = list_time_Xn\n",
    "        self.list_time_yn = list_time_yn\n",
    "        self.list_time_nstatic = list_time_nstatic\n",
    "        self.size = size\n",
    "        self.fraction = fraction\n",
    "        self.ratio = ratio\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_time_Xn) / 40))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        file_generator1 = file_generator(window = self.window,\n",
    "                                       time_it =self.time_it,\n",
    "                                       feature = self.feature,\n",
    "                                       true_X_time = self.true_X_time,\n",
    "                                       true_X_static = self.true_X_static,\n",
    "                                       true_y_total = self.true_y_total,\n",
    "\n",
    "                                       false_X_time = self.false_X_time,\n",
    "                                       false_X_static = self.false_X_static,\n",
    "                                       false_y_total = self.false_y_total,\n",
    "\n",
    "                                       list_time_Xn = self.list_time_Xn,\n",
    "                                       list_time_yn = self.list_time_yn,\n",
    "                                       list_time_nstatic = self.list_time_nstatic,\n",
    "                                        random_sampling_size = self.size,\n",
    "                                         fraction_per_case = self.fraction,\n",
    "                                         true_false_ratio = self.ratio,\n",
    "                                        repeat = self.repeat)\n",
    "        # Generate data\n",
    "        batch_X, batch_static_X, batch_y = file_generator1.get_data(index)\n",
    "\n",
    "        return ({\"time\":batch_X, \"static\":batch_static_X}, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the time_it, window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_it = 0\n",
    "window = 30\n",
    "# windows = [3,6,12,18,24,30,36]\n",
    "feature = 100\n",
    "\n",
    "\n",
    "\n",
    "activation_set = [['tanh','relu','tanh','relu','sigmoid'],\n",
    "                 ['relu','relu','relu','relu','sigmoid']]\n",
    "                 \n",
    "learning_rate = (0.003,0.0003)\n",
    "\n",
    "# layer_set = ((16,8,8,16),\n",
    "#             (16,8,8,8),\n",
    "#             (8,8,8,8),\n",
    "#             (32,16,16,16),\n",
    "#             (32,16,8,16),\n",
    "#             (128,128,128,64))\n",
    "\n",
    "layer_set = ((32,16,16,16),\n",
    "            (32,16,8,16),\n",
    "            (128,128,128,64),\n",
    "            (256,128,256,128))\n",
    "\n",
    "history1 = []\n",
    "history_val_auc = []\n",
    "history_val_precision = []\n",
    "history_test_auc = []\n",
    "history_test_precision = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_foler = './train/'\n",
    "if not os.path.exists(file_foler):\n",
    "  os.mkdir(file_foler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for window in windows:\n",
    "def time_series_generator(x,y):\n",
    "    Xn=[]\n",
    "    yn=[]\n",
    "    for n in range(len(x)-time_it):\n",
    "        if n+1>window:\n",
    "            X_train=x[n+1-window:n+1]\n",
    "        else:\n",
    "            X_train=x[0:n+1]\n",
    "            X_train=np.pad(X_train, mode='constant', pad_width=((0,window-X_train.shape[0]),(0,0)),\\\n",
    "                           constant_values=-5)\n",
    "\n",
    "        Xn.append(X_train)\n",
    "        y_train=y[n+time_it]\n",
    "        yn.append(y_train)\n",
    "\n",
    "    return Xn, yn\n",
    "\n",
    "Xt_time = []\n",
    "Xt_static = []\n",
    "yt_time = []\n",
    "\n",
    "## positive data gathering\n",
    "for idx in range(len(list_time_X)):\n",
    "    Xt_time_i = list_time_X[idx]\n",
    "    yt_time_i = list_time_y[idx]\n",
    "    Xt, yt = time_series_generator(Xt_time_i,yt_time_i)\n",
    "    time_static_data = list_time_static[idx]\n",
    "\n",
    "    for n in range(len(Xt)):\n",
    "        Xt_time.append(Xt[n])\n",
    "        Xt_static.append(time_static_data)\n",
    "        yt_time.append(yt[n])\n",
    "\n",
    "positive_index = [i for i,result in enumerate(yt_time) if result==1]\n",
    "negative_index = [i for i,result in enumerate(yt_time) if result==0]\n",
    "\n",
    "positive_x = [Xt_time[idx] for idx in positive_index]\n",
    "positive_x_static = [Xt_static[idx] for idx in positive_index]\n",
    "positive_y = [yt_time[idx] for idx in positive_index]\n",
    "\n",
    "negative_x = [Xt_time[idx] for idx in negative_index]\n",
    "negative_x_static = [Xt_static[idx] for idx in negative_index]\n",
    "negative_y = [yt_time[idx] for idx in negative_index]\n",
    "\n",
    "true_X_time = np.array(positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "true_X_static = np.array(positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "true_y_total = np.array(positive_y, dtype=\"float32\").reshape(-1,)\n",
    "false_X_time = np.array(negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "false_X_static = np.array(negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "false_y_total = np.array(negative_y, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "def val_generator(x,y):\n",
    "    Xn=[]\n",
    "    yn=[]\n",
    "    for n in range(len(x)-time_it):\n",
    "        if n+1>window:\n",
    "            X_train=x[n+1-window:n+1]\n",
    "        else:\n",
    "            X_train=x[0:n+1]\n",
    "            X_train=np.pad(X_train, mode='constant', pad_width=((0,window-X_train.shape[0]),(0,0)),\\\n",
    "                           constant_values=-5)\n",
    "\n",
    "        Xn.append(X_train)\n",
    "        y_train=y[n+time_it]\n",
    "        yn.append(y_train)\n",
    "\n",
    "    return Xn, yn\n",
    "\n",
    "val_Xt_time = []\n",
    "val_Xt_static = []\n",
    "val_yt_time = []\n",
    "\n",
    "## positive data gathering\n",
    "for idx in range(len(valid_time_X)):\n",
    "    Xt_time_i = valid_time_X[idx]\n",
    "    yt_time_i = valid_time_y[idx]\n",
    "    Xt, yt = val_generator(Xt_time_i,yt_time_i)\n",
    "    time_static_data = valid_time_static[idx]\n",
    "\n",
    "    for n in range(len(Xt)):\n",
    "        val_Xt_time.append(Xt[n])\n",
    "        val_Xt_static.append(time_static_data)\n",
    "        val_yt_time.append(yt[n])\n",
    "\n",
    "val_positive_index = [idx for idx,result in enumerate(val_yt_time) if result==1]\n",
    "val_negative_index = [idx for idx,result in enumerate(val_yt_time) if result==0]\n",
    "\n",
    "val_positive_x = [val_Xt_time[idx] for idx in val_positive_index]\n",
    "val_positive_x_static = [val_Xt_static[idx] for idx in val_positive_index]\n",
    "val_positive_y = [val_yt_time[idx] for idx in val_positive_index]\n",
    "\n",
    "val_negative_x = [val_Xt_time[idx] for idx in val_negative_index]\n",
    "val_negative_x_static = [val_Xt_static[idx] for idx in val_negative_index]\n",
    "val_negative_y = [val_yt_time[idx] for idx in val_negative_index]\n",
    "\n",
    "val_true_X_time = np.array(val_positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "val_true_X_static = np.array(val_positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "val_true_y_total = np.array(val_positive_y, dtype=\"float32\").reshape(-1,)\n",
    "val_false_X_time = np.array(val_negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "val_false_X_static = np.array(val_negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "val_false_y_total = np.array(val_negative_y, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "### valid data generator\n",
    "\n",
    "test_generator = file_generator_valid(window = window,\n",
    "                                       time_it =time_it, \n",
    "                                       feature = feature,\n",
    "                                       list_time_Xt = test_time_X, \n",
    "                                       list_time_yt = test_time_y, \n",
    "                                       list_time_tstatic = test_time_static, \n",
    "                                       list_time_Xn = test_time_Xn, \n",
    "                                       list_time_yn = test_time_yn, \n",
    "                                       list_time_nstatic = test_time_staticn)\n",
    "\n",
    "test_X, test_static_X, test_y = test_generator.get_data()\n",
    "\n",
    "params = {\"window\" : window,\n",
    "           \"time_it\" :time_it, \n",
    "          \"feature\": feature,\n",
    "           \"true_X_time\" : true_X_time, \n",
    "           \"true_X_static\" : true_X_static, \n",
    "           \"true_y_total\" : true_y_total, \n",
    "\n",
    "           \"false_X_time\" : false_X_time, \n",
    "           \"false_X_static\" : false_X_static, \n",
    "           \"false_y_total\" : false_y_total, \n",
    "\n",
    "           \"list_time_Xn\" : list_time_Xn, \n",
    "           \"list_time_yn\" : list_time_yn, \n",
    "           \"list_time_nstatic\" : list_time_nstatic,\n",
    "           \"size\" : 2500,\n",
    "           \"fraction\" : 0.2,\n",
    "            \"ratio\" : 1,\n",
    "           \"repeat\":3}\n",
    "valparams = {\"window\" : window,\n",
    "           \"time_it\" :time_it, \n",
    "          \"feature\": feature,\n",
    "           \"true_X_time\" : val_true_X_time, \n",
    "           \"true_X_static\" : val_true_X_static, \n",
    "           \"true_y_total\" : val_true_y_total, \n",
    "\n",
    "           \"false_X_time\" : val_false_X_time, \n",
    "           \"false_X_static\" : val_false_X_static, \n",
    "           \"false_y_total\" : val_false_y_total, \n",
    "\n",
    "           \"list_time_Xn\" : valid_time_Xn, \n",
    "           \"list_time_yn\" : valid_time_yn, \n",
    "           \"list_time_nstatic\" : valid_time_staticn,\n",
    "            \"size\" : 600,\n",
    "            \"fraction\" : 0.2,\n",
    "             \"ratio\" : 1,\n",
    "            \"repeat\":3}\n",
    "\n",
    "filepath = \".hdf5\"\n",
    "\n",
    "\n",
    "traingen = DataGenerator(**params)\n",
    "valid_gen = DataGenerator(**valparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7251"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:53: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:53: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 36s 2s/step - loss: 0.7523 - acc: 0.6346 - val_loss: 0.3335 - val_acc: 0.8680\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4884 - acc: 0.8038 - val_loss: 0.3065 - val_acc: 0.8697\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3834 - acc: 0.8700 - val_loss: 0.2889 - val_acc: 0.8803\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.3211 - acc: 0.8909 - val_loss: 0.2671 - val_acc: 0.9050\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.2861 - acc: 0.8999 - val_loss: 0.2733 - val_acc: 0.9005\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.2613 - acc: 0.9052 - val_loss: 0.2811 - val_acc: 0.8926\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.2455 - acc: 0.9084 - val_loss: 0.2668 - val_acc: 0.8950\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2320 - acc: 0.9120 - val_loss: 0.2796 - val_acc: 0.8938\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.2203 - acc: 0.9157 - val_loss: 0.2767 - val_acc: 0.8909\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2095 - acc: 0.9189 - val_loss: 0.2915 - val_acc: 0.8921\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.1999 - acc: 0.9220 - val_loss: 0.2631 - val_acc: 0.9044\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 34s 2s/step - loss: 0.1901 - acc: 0.9260 - val_loss: 0.2766 - val_acc: 0.8984\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1845 - acc: 0.9279 - val_loss: 0.2737 - val_acc: 0.9019\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1763 - acc: 0.9318 - val_loss: 0.2725 - val_acc: 0.9052\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1733 - acc: 0.9324 - val_loss: 0.3061 - val_acc: 0.8984\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1673 - acc: 0.9355 - val_loss: 0.3405 - val_acc: 0.8960\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1629 - acc: 0.9375 - val_loss: 0.3053 - val_acc: 0.9029\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1587 - acc: 0.9398 - val_loss: 0.3285 - val_acc: 0.8949\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1557 - acc: 0.9404 - val_loss: 0.3543 - val_acc: 0.8931\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1509 - acc: 0.9430 - val_loss: 0.3591 - val_acc: 0.8934\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1472 - acc: 0.9444 - val_loss: 0.3548 - val_acc: 0.8962\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "#GRU layer\n",
    "time_input= Input(shape=(None, feature), name='time')\n",
    "x0=layers.Masking(mask_value=-5)(time_input)\n",
    "\n",
    "x1=layers.Dense(layer_set[1][0], activation=activation_set[0][0])(x0)\n",
    "x11=layers.BatchNormalization()(x1)\n",
    "x12=layers.Dropout(0.5)(x11)\n",
    "\n",
    "x2=layers.GRU(layer_set[1][1], activation=activation_set[0][1], return_sequences=False)(x12)\n",
    "# x3 = layers.Dense(8)(x2)\n",
    "x4=layers.BatchNormalization()(x2)\n",
    "x5=layers.Dropout(0.5)(x4)\n",
    "# x6=layers.Dense(64)(x5)\n",
    "#static layer\n",
    "static_input=Input(shape=(38,),  name='static')\n",
    "x31 = layers.Dense(layer_set[1][2],activation=activation_set[0][2])(static_input)\n",
    "x32=layers.BatchNormalization()(x31)\n",
    "x33=layers.Dropout(0.5)(x32)\n",
    "#합친 모양\n",
    "concatenated = layers.concatenate([x5,x33], axis=-1)\n",
    "\n",
    "x7=layers.Dense(layer_set[1][3], activation=activation_set[0][3])(concatenated)\n",
    "x8=layers.BatchNormalization()(x7)\n",
    "x9=layers.Dropout(0.5)(x8)\n",
    "x10=layers.Dense(1, activation=activation_set[0][4])(x9)\n",
    "\n",
    "model=Model([time_input,static_input], x10)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=learning_rate[j]), metrics=['accuracy'])\n",
    "\n",
    "MODEL_SAVE_FOLDER_PATH = file_foler+'_{}_{}_{}/'.format(0,1,1)\n",
    "filename = 'rnn_{}_{}_{}'.format(0,1,1)\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "  os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "model_path = MODEL_SAVE_FOLDER_PATH + filename+filepath\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=0,\n",
    "                             save_best_only=True, mode='min')\n",
    "\n",
    "rnn_operating_code=model.to_json()\n",
    "with open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"w\") as json_file : \n",
    "    json_file.write(rnn_operating_code)\n",
    "history = History()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=2, mode='auto')\n",
    "\n",
    "rnn_train = model.fit_generator(generator= traingen,\n",
    "                                 validation_data= valid_gen,\n",
    "                                steps_per_epoch=int(np.floor(len(baseline)*0.8/1300)),\n",
    "                                validation_steps=10,\n",
    "                                nb_epoch = 500, verbose=1, \n",
    "                                callbacks = [history, checkpoint\n",
    "                                             ,early_stopping\n",
    "                                            ],\n",
    "                                workers=-1, use_multiprocessing=True\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC of model :  0.9780392439841585\n",
      "AUPRC of model :  0.17217443445493316\n"
     ]
    }
   ],
   "source": [
    "json_file = open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"r\")\n",
    "loaded_model_json = json_file.read() \n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=.003), metrics=['accuracy'])\n",
    "loaded_model.load_weights(model_path)\n",
    "\n",
    "model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "roc_val_test = roc_auc_score(test_y, model_test)\n",
    "#mAP\n",
    "test_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "print(\"AUROC of model : \",roc_val_test)\n",
    "print(\"AUPRC of model : \", test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.read_csv(\"table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIRS_sensitivity = table.loc[table['criteria']==\"SIRS(score)\"].sensitivity\n",
    "SIRS_specificity = table.loc[table['criteria']==\"SIRS(score)\"].specificity\n",
    "SIRS_precision = table.loc[table['criteria']==\"SIRS(score)\"].precision\n",
    "mews_sen =table.loc[table['criteria']==\"mews(score)\"].sensitivity\n",
    "mews_spe = table.loc[table['criteria']==\"mews(score)\"].specificity\n",
    "mews_pre = table.loc[table['criteria']==\"mews(score)\"].precision\n",
    "sofa_sen = table.loc[table['criteria']==\"sofa(score)_x\"].sensitivity\n",
    "sofa_spe = table.loc[table['criteria']==\"sofa(score)_x\"].specificity\n",
    "sofa_pre = table.loc[table['criteria']==\"sofa(score)_x\"].precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.001999\n",
       "1    0.004749\n",
       "2    0.011343\n",
       "3    0.029098\n",
       "4    0.051306\n",
       "Name: precision, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SIRS_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd653bb10>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd655a1d0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd655a610>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd655ad90>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd655af90>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.6, 0.02, 'SOFA')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd64f37d0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.463, 0.03, 'SIRS')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd64f3d10>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.116, 0.05, 'MEWS')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Sensitivity (recall)')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Positive predictive value (precision)')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Precision-Recall curve')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fccd64f3dd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(test_y, model_test)\n",
    "# precision, recall, _ = precision_recall_curve(a2, y_pred_val)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.plot(recall, precision, color='navy', label = \"Our model\")\n",
    "plt.plot(SIRS_sensitivity, SIRS_precision, 'orange', label = \"SIRS\")\n",
    "plt.plot(mews_sen, mews_pre, \"deepskyblue\", label = \"MEWS\")\n",
    "plt.plot(sofa_sen, sofa_pre, \"green\", label = \"SOFA\")\n",
    "#SOFA click\n",
    "plt.plot(0.60, 0.006, 'green', marker='o')\n",
    "plt.annotate(\"SOFA\",(0.6, 0.02))\n",
    "#SIRS click\n",
    "plt.plot(0.463, 0.011, 'orange', marker='o')\n",
    "plt.annotate(\"SIRS\",(0.463, 0.03))\n",
    "#MEWS click\n",
    "plt.plot(0.116, 0.031, 'deepskyblue', marker='o')\n",
    "plt.annotate(\"MEWS\",(0.116, 0.05))\n",
    "\n",
    "plt.xlabel('Sensitivity (recall)', fontsize= 12)\n",
    "plt.ylabel('Positive predictive value (precision)', fontsize= 12)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve', fontsize= 14)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd65ecb10>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd65f92d0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd65f98d0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd65f91d0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd65f9e90>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.22799999999999998, 0.57, 'SOFA')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd65fa850>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.020000000000000018, 0.48, 'SIRS')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fccd65fa9d0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.020000000000000018, 0.116, 'MEWS')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '1-specificity')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'sensitivity')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fccd65faf90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'AUROC curve')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fpr, tpr, _ = roc_curve(test_y, model_test)\n",
    "auc = np.trapz(tpr,fpr)\n",
    "\n",
    "# Print ROC curve\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.plot(fpr, tpr, color='navy', label=\"Our model\")\n",
    "plt.plot((1-SIRS_specificity), SIRS_sensitivity , 'orange', label = \"SIRS\")\n",
    "plt.plot((1-mews_spe), mews_sen, \"deepskyblue\", label = \"MEWS\")\n",
    "plt.plot((1-sofa_spe), sofa_sen, \"green\", label = \"SOFA\")\n",
    "\n",
    "#SOFA click\n",
    "plt.plot((1-0.792), 0.600, 'green', marker = 'o')\n",
    "plt.annotate(\"SOFA\",((1-0.772), 0.57))\n",
    "#SIRS click\n",
    "plt.plot((1-0.919), 0.463, 'orange', marker = 'o')\n",
    "plt.annotate(\"SIRS\",((1-0.98), 0.48))\n",
    "#MEWS click\n",
    "plt.plot((1-0.993), 0.116, 'deepskyblue', marker = \"o\")\n",
    "plt.annotate(\"MEWS\",((1-0.98), 0.116))\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.xlabel('1-specificity', fontsize= 12)\n",
    "plt.ylabel('sensitivity', fontsize= 12)\n",
    "plt.legend()\n",
    "\n",
    "plt.title('AUROC curve', fontsize= 14)\n",
    "plt.savefig(MODEL_SAVE_FOLDER_PATH+filename+'roc.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### occlusion analysis\n",
    "list of column\n",
    "* \"Vital_Sign\"\n",
    "* \"hct\"\n",
    "* \"Electrolyte\"\n",
    "* \"Kidney related value\"\n",
    "* \"Inflammatory_marker\"\n",
    "* \"WBC\"\n",
    "* \"ABGA\"\n",
    "* \"Liver_function_test\"\n",
    "* time-invariant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = file_generator_valid(window = window,\n",
    "                                       time_it =time_it, \n",
    "                                       feature = feature,\n",
    "                                       list_time_Xt = test_time_X, \n",
    "                                       list_time_yt = test_time_y, \n",
    "                                       list_time_tstatic = test_time_static, \n",
    "                                       list_time_Xn = test_time_Xn, \n",
    "                                       list_time_yn = test_time_yn, \n",
    "                                       list_time_nstatic = test_time_staticn)\n",
    "\n",
    "test_X, test_static_X, test_y = test_generator.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = [4,16,28,40,52,64,76,88]\n",
    "n=12\n",
    "\n",
    "original_test_X = test_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_result = []\n",
    "list_of_column = [\"Vital_Sign\",\"hct\",\"Electrolyte\",\"Kidney related value\",\"Inflammatory_marker\",\n",
    "                 \"WBC\",\"ABGA\",\"Liver_function_test\"]\n",
    "number = 0\n",
    "for point in start:\n",
    "    test_X = original_test_X.copy()\n",
    "    test_X[:,:,point:point+n][test_X[:,:,point:point+n]!=-5]=0\n",
    "    model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "    roc_val_test = roc_auc_score(test_y, model_test)\n",
    "    test_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "    # roc_val, average_precision, \n",
    "    outcome_result.append({\"group\":list_of_column[number],\n",
    "                           \"AUROC\":roc_val_test ,\n",
    "                           \"AUPRC\":test_precision})\n",
    "    number +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "occluding_X=test_static_X.copy()\n",
    "occluding_X[:]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9689598913542313, 0.14265733519591678)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AUROC\n",
    "# model_val = loaded_model.predict({\"time\":valid_X, \"static\":valid_static_X})\n",
    "# roc_val = roc_auc_score(valid_y, model_val)\n",
    "model_test = loaded_model.predict({\"time\":test_X, \"static\":occluding_X})\n",
    "roc_val_test = roc_auc_score(test_y, model_test)\n",
    "#mAP\n",
    "# average_precision = average_precision_score(valid_y, model_val)\n",
    "test_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "# roc_val, average_precision, \n",
    "roc_val_test , test_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Vital_Sign</td>\n",
       "      <td>0.854117</td>\n",
       "      <td>0.051757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Kidney related value</td>\n",
       "      <td>0.946621</td>\n",
       "      <td>0.064617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>WBC</td>\n",
       "      <td>0.955581</td>\n",
       "      <td>0.075492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Electrolyte</td>\n",
       "      <td>0.962377</td>\n",
       "      <td>0.107562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hct</td>\n",
       "      <td>0.965683</td>\n",
       "      <td>0.108604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>ABGA</td>\n",
       "      <td>0.968850</td>\n",
       "      <td>0.124117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Inflammatory_marker</td>\n",
       "      <td>0.975475</td>\n",
       "      <td>0.139766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>time_invariant_value</td>\n",
       "      <td>0.968960</td>\n",
       "      <td>0.142657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Liver_function_test</td>\n",
       "      <td>0.976142</td>\n",
       "      <td>0.150692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  group     AUROC     AUPRC\n",
       "0            Vital_Sign  0.854117  0.051757\n",
       "3  Kidney related value  0.946621  0.064617\n",
       "5                   WBC  0.955581  0.075492\n",
       "2           Electrolyte  0.962377  0.107562\n",
       "1                   hct  0.965683  0.108604\n",
       "6                  ABGA  0.968850  0.124117\n",
       "4   Inflammatory_marker  0.975475  0.139766\n",
       "8  time_invariant_value  0.968960  0.142657\n",
       "7   Liver_function_test  0.976142  0.150692"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_result.append({\"group\":\"time_invariant_value\",\n",
    "                     \"AUROC\":roc_val_test,\n",
    "                     \"AUPRC\":test_precision})\n",
    "pd.DataFrame(outcome_result).sort_values(\"AUPRC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpr</th>\n",
       "      <th>tpr</th>\n",
       "      <th>1-fpr</th>\n",
       "      <th>tf</th>\n",
       "      <th>thresholds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>0.084781</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.915219</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.081195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fpr       tpr     1-fpr        tf  thresholds\n",
       "3940  0.084781  0.915222  0.915219  0.000002    0.081195"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds =roc_curve(test_y, model_test)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "i = np.arange(len(tpr)) # index for df\n",
    "roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})\n",
    "roc.ix[(roc.tf-0).abs().argsort()[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc.ix[(roc.tf-0).abs().argsort()[:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>1-recall</th>\n",
       "      <th>tf</th>\n",
       "      <th>thresholds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>108058</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.693642</td>\n",
       "      <td>0.306358</td>\n",
       "      <td>-0.256358</td>\n",
       "      <td>0.564122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        precision    recall  1-recall        tf  thresholds\n",
       "108058       0.05  0.693642  0.306358 -0.256358    0.564122"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(test_y, model_test)\n",
    "# roc_auc = auc(precision, recall)\n",
    "\n",
    "i = np.arange(len(recall)-1) # index for df\n",
    "PRgraph = pd.DataFrame({'precision' : pd.Series(precision[:-1], index=i),'recall' : pd.Series(recall[:-1], index = i), '1-recall' : pd.Series(1-recall[:-1], index = i), 'tf' : pd.Series(recall[:-1] - (1-precision[:-1]), index = i), 'thresholds' : pd.Series(thresholds, index = i)})\n",
    "PRgraph.ix[(PRgraph.precision==0.05)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8hour, 16hour prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "positive_folder = './dataset/time3_pos_total200107/'\n",
    "negative_folder = './dataset/time3_neg_total200107/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(positive_folder+\"list_time_X.txt\", \"rb\") as fp:\n",
    "    list_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"list_time_y.txt\", \"rb\") as fp:\n",
    "    list_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"list_time_static.txt\", \"rb\") as fp:\n",
    "    list_time_static=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_X.txt\", \"rb\") as fp:\n",
    "    valid_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_y.txt\", \"rb\") as fp:\n",
    "    valid_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_static.txt\", \"rb\") as fp:\n",
    "    valid_time_static=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_X.txt\", \"rb\") as fp:\n",
    "    test_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_y.txt\", \"rb\") as fp:\n",
    "    test_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_static.txt\", \"rb\") as fp:\n",
    "    test_time_static=pickle.load( fp)\n",
    "\n",
    "with open(negative_folder+\"list_time_Xn.txt\", \"rb\") as fp:\n",
    "    list_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"list_time_yn.txt\", \"rb\") as fp:\n",
    "    list_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"list_time_nstatic.txt\", \"rb\") as fp:\n",
    "    list_time_nstatic=pickle.load( fp)\n",
    "\n",
    "with open(negative_folder+\"valid_time_Xn.txt\", \"rb\") as fp:\n",
    "    valid_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"valid_time_yn.txt\", \"rb\") as fp:\n",
    "    valid_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"valid_time_staticn.txt\", \"rb\") as fp:\n",
    "    valid_time_staticn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_Xn.txt\", \"rb\") as fp:\n",
    "    test_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_yn.txt\", \"rb\") as fp:\n",
    "    test_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_staticn.txt\", \"rb\") as fp:\n",
    "    test_time_staticn=pickle.load( fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, window,  time_it, feature,\n",
    "                 true_X_time, true_X_static,true_y_total,\n",
    "                 false_X_time, false_X_static, false_y_total,\n",
    "                 list_time_Xn, list_time_yn, list_time_nstatic,\n",
    "                size, fraction, ratio, repeat):\n",
    "        'Initialization'\n",
    "        self.window = window\n",
    "        self.time_it = time_it\n",
    "        self.feature = feature\n",
    "        self.true_X_time = true_X_time\n",
    "        self.true_X_static = true_X_static\n",
    "        self.true_y_total = true_y_total\n",
    "        self.false_X_time = false_X_time\n",
    "        self.false_X_static = false_X_static\n",
    "        self.false_y_total = false_y_total\n",
    "        self.list_time_Xn = list_time_Xn\n",
    "        self.list_time_yn = list_time_yn\n",
    "        self.list_time_nstatic = list_time_nstatic\n",
    "        self.size = size\n",
    "        self.fraction = fraction\n",
    "        self.ratio = ratio\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_time_Xn) / 40))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        file_generator1 = file_generator(window = self.window,\n",
    "                                       time_it =self.time_it,\n",
    "                                       feature = self.feature,\n",
    "                                       true_X_time = self.true_X_time,\n",
    "                                       true_X_static = self.true_X_static,\n",
    "                                       true_y_total = self.true_y_total,\n",
    "\n",
    "                                       false_X_time = self.false_X_time,\n",
    "                                       false_X_static = self.false_X_static,\n",
    "                                       false_y_total = self.false_y_total,\n",
    "\n",
    "                                       list_time_Xn = self.list_time_Xn,\n",
    "                                       list_time_yn = self.list_time_yn,\n",
    "                                       list_time_nstatic = self.list_time_nstatic,\n",
    "                                        random_sampling_size = self.size,\n",
    "                                         fraction_per_case = self.fraction,\n",
    "                                         true_false_ratio = self.ratio,\n",
    "                                        repeat = self.repeat)\n",
    "        # Generate data\n",
    "        batch_X, batch_static_X, batch_y = file_generator1.get_data(index)\n",
    "\n",
    "        return ({\"time\":batch_X, \"static\":batch_static_X}, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_foler = './window_200119/'\n",
    "if not os.path.exists(file_foler):\n",
    "  os.mkdir(file_foler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_it = 0\n",
    "windows = [3,6,12,18,24,30,36]\n",
    "feature = 100\n",
    "\n",
    "\n",
    "\n",
    "activation_set = [['tanh','relu','tanh','relu','sigmoid'],\n",
    "                 ['relu','relu','relu','relu','sigmoid']]\n",
    "                 \n",
    "learning_rate = (0.003,0.0003)\n",
    "\n",
    "# layer_set = ((16,8,8,16),\n",
    "#             (16,8,8,8),\n",
    "#             (8,8,8,8),\n",
    "#             (32,16,16,16),\n",
    "#             (32,16,8,16),\n",
    "#             (128,128,128,64))\n",
    "\n",
    "layer_set = ((32,16,16,16),\n",
    "            (32,16,8,16),\n",
    "            (128,128,128,64),\n",
    "            (256,128,256,128))\n",
    "\n",
    "history1 = []\n",
    "history_val_auc = []\n",
    "history_val_precision = []\n",
    "history_test_auc = []\n",
    "history_test_precision = []\n",
    "\n",
    "activation_i = 0\n",
    "activation_j = 1\n",
    "activation_k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7260"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 10s 443ms/step - loss: 1.0235 - acc: 0.5562 - val_loss: 0.5877 - val_acc: 0.7262\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.8973 - acc: 0.6009 - val_loss: 0.5227 - val_acc: 0.8008\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.8108 - acc: 0.6392 - val_loss: 0.4853 - val_acc: 0.8297\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.7463 - acc: 0.6710 - val_loss: 0.4867 - val_acc: 0.8406\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.6899 - acc: 0.7024 - val_loss: 0.4289 - val_acc: 0.8680\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.6452 - acc: 0.7283 - val_loss: 0.4097 - val_acc: 0.8760\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.6064 - acc: 0.7528 - val_loss: 0.3929 - val_acc: 0.8869\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.5738 - acc: 0.7721 - val_loss: 0.3825 - val_acc: 0.8907\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.5420 - acc: 0.7898 - val_loss: 0.3658 - val_acc: 0.9005\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.5169 - acc: 0.8068 - val_loss: 0.3631 - val_acc: 0.9021\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.4955 - acc: 0.8211 - val_loss: 0.3457 - val_acc: 0.8990\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.4737 - acc: 0.8327 - val_loss: 0.3325 - val_acc: 0.8970\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.4595 - acc: 0.8417 - val_loss: 0.3259 - val_acc: 0.9004\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.4442 - acc: 0.8502 - val_loss: 0.3215 - val_acc: 0.8922\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.4312 - acc: 0.8564 - val_loss: 0.3108 - val_acc: 0.8923\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.4203 - acc: 0.8620 - val_loss: 0.3093 - val_acc: 0.8913\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.4117 - acc: 0.8668 - val_loss: 0.3096 - val_acc: 0.8969\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.4035 - acc: 0.8702 - val_loss: 0.3005 - val_acc: 0.8954\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.3922 - acc: 0.8741 - val_loss: 0.3031 - val_acc: 0.8937\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3877 - acc: 0.8765 - val_loss: 0.2875 - val_acc: 0.8955\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3782 - acc: 0.8797 - val_loss: 0.2828 - val_acc: 0.8947\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3747 - acc: 0.8806 - val_loss: 0.2836 - val_acc: 0.9007\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.3668 - acc: 0.8826 - val_loss: 0.2816 - val_acc: 0.8942\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3619 - acc: 0.8840 - val_loss: 0.2791 - val_acc: 0.9028\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3566 - acc: 0.8855 - val_loss: 0.2814 - val_acc: 0.8980\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3516 - acc: 0.8866 - val_loss: 0.2715 - val_acc: 0.9035\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 7s 309ms/step - loss: 0.3449 - acc: 0.8882 - val_loss: 0.2702 - val_acc: 0.9041\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.3418 - acc: 0.8889 - val_loss: 0.2745 - val_acc: 0.8984\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3381 - acc: 0.8896 - val_loss: 0.2658 - val_acc: 0.8979\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3310 - acc: 0.8907 - val_loss: 0.2668 - val_acc: 0.9013\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3287 - acc: 0.8916 - val_loss: 0.2601 - val_acc: 0.9001\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.3274 - acc: 0.8910 - val_loss: 0.2646 - val_acc: 0.8977\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3230 - acc: 0.8924 - val_loss: 0.2631 - val_acc: 0.8958\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.3177 - acc: 0.8934 - val_loss: 0.2557 - val_acc: 0.9005\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 7s 317ms/step - loss: 0.3153 - acc: 0.8930 - val_loss: 0.2566 - val_acc: 0.8986\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3133 - acc: 0.8936 - val_loss: 0.2603 - val_acc: 0.8949\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3112 - acc: 0.8937 - val_loss: 0.2578 - val_acc: 0.8964\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3072 - acc: 0.8943 - val_loss: 0.2547 - val_acc: 0.8997\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3033 - acc: 0.8948 - val_loss: 0.2546 - val_acc: 0.8973\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.3019 - acc: 0.8954 - val_loss: 0.2554 - val_acc: 0.8984\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.3003 - acc: 0.8957 - val_loss: 0.2536 - val_acc: 0.8971\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2956 - acc: 0.8965 - val_loss: 0.2525 - val_acc: 0.9003\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2947 - acc: 0.8957 - val_loss: 0.2432 - val_acc: 0.9029\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2900 - acc: 0.8968 - val_loss: 0.2476 - val_acc: 0.9024\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2893 - acc: 0.8971 - val_loss: 0.2448 - val_acc: 0.9008\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2858 - acc: 0.8982 - val_loss: 0.2495 - val_acc: 0.9004\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.2854 - acc: 0.8981 - val_loss: 0.2456 - val_acc: 0.9000\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2807 - acc: 0.8984 - val_loss: 0.2446 - val_acc: 0.9044\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2793 - acc: 0.8985 - val_loss: 0.2424 - val_acc: 0.9035\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2797 - acc: 0.8983 - val_loss: 0.2402 - val_acc: 0.9026\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2770 - acc: 0.8988 - val_loss: 0.2435 - val_acc: 0.9009\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2758 - acc: 0.8989 - val_loss: 0.2400 - val_acc: 0.9032\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2741 - acc: 0.8988 - val_loss: 0.2387 - val_acc: 0.9019\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2703 - acc: 0.9001 - val_loss: 0.2418 - val_acc: 0.9023\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2693 - acc: 0.8995 - val_loss: 0.2386 - val_acc: 0.9033\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2653 - acc: 0.9011 - val_loss: 0.2356 - val_acc: 0.9042\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 7s 318ms/step - loss: 0.2666 - acc: 0.9005 - val_loss: 0.2418 - val_acc: 0.9020\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2631 - acc: 0.9015 - val_loss: 0.2343 - val_acc: 0.9027\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2629 - acc: 0.9014 - val_loss: 0.2353 - val_acc: 0.9019\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2579 - acc: 0.9025 - val_loss: 0.2355 - val_acc: 0.9022\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.2576 - acc: 0.9028 - val_loss: 0.2394 - val_acc: 0.9038\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2570 - acc: 0.9028 - val_loss: 0.2359 - val_acc: 0.9000\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2557 - acc: 0.9030 - val_loss: 0.2331 - val_acc: 0.9022\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.2537 - acc: 0.9034 - val_loss: 0.2376 - val_acc: 0.9046\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2522 - acc: 0.9035 - val_loss: 0.2399 - val_acc: 0.9026\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2497 - acc: 0.9050 - val_loss: 0.2444 - val_acc: 0.9016\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 7s 317ms/step - loss: 0.2486 - acc: 0.9047 - val_loss: 0.2362 - val_acc: 0.9037\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2484 - acc: 0.9046 - val_loss: 0.2360 - val_acc: 0.9019\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 7s 308ms/step - loss: 0.2475 - acc: 0.9048 - val_loss: 0.2365 - val_acc: 0.9011\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2454 - acc: 0.9053 - val_loss: 0.2296 - val_acc: 0.9066\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.2440 - acc: 0.9058 - val_loss: 0.2356 - val_acc: 0.9020\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2434 - acc: 0.9061 - val_loss: 0.2306 - val_acc: 0.9035\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2400 - acc: 0.9072 - val_loss: 0.2346 - val_acc: 0.9036\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 7s 317ms/step - loss: 0.2400 - acc: 0.9069 - val_loss: 0.2269 - val_acc: 0.9052\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2398 - acc: 0.9064 - val_loss: 0.2288 - val_acc: 0.9031\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2373 - acc: 0.9078 - val_loss: 0.2340 - val_acc: 0.9048\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2371 - acc: 0.9074 - val_loss: 0.2330 - val_acc: 0.9036\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.2348 - acc: 0.9087 - val_loss: 0.2346 - val_acc: 0.9049\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2328 - acc: 0.9089 - val_loss: 0.2338 - val_acc: 0.9028\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2338 - acc: 0.9085 - val_loss: 0.2377 - val_acc: 0.9019\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 7s 310ms/step - loss: 0.2301 - acc: 0.9105 - val_loss: 0.2354 - val_acc: 0.9023\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2291 - acc: 0.9102 - val_loss: 0.2328 - val_acc: 0.9053\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2295 - acc: 0.9103 - val_loss: 0.2388 - val_acc: 0.9037\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2248 - acc: 0.9118 - val_loss: 0.2399 - val_acc: 0.9023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddbf155390>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddbf1559d0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddbf14f410>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.976641173874518, AP:0.13882905761826614')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7260"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 13s 602ms/step - loss: 0.9723 - acc: 0.5050 - val_loss: 0.7817 - val_acc: 0.5183\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.8743 - acc: 0.5422 - val_loss: 0.7213 - val_acc: 0.5921\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 10s 466ms/step - loss: 0.8028 - acc: 0.5774 - val_loss: 0.5554 - val_acc: 0.7629\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.7472 - acc: 0.6092 - val_loss: 0.4768 - val_acc: 0.8459\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.7021 - acc: 0.6370 - val_loss: 0.4204 - val_acc: 0.8680\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.6659 - acc: 0.6620 - val_loss: 0.3991 - val_acc: 0.8762\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.6335 - acc: 0.6855 - val_loss: 0.3871 - val_acc: 0.8661\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.6079 - acc: 0.7070 - val_loss: 0.3765 - val_acc: 0.8639\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 10s 475ms/step - loss: 0.5817 - acc: 0.7270 - val_loss: 0.3623 - val_acc: 0.8604\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.5585 - acc: 0.7455 - val_loss: 0.3575 - val_acc: 0.8594\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.5384 - acc: 0.7630 - val_loss: 0.3487 - val_acc: 0.8512\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.5209 - acc: 0.7784 - val_loss: 0.3416 - val_acc: 0.8545\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.5024 - acc: 0.7931 - val_loss: 0.3352 - val_acc: 0.8565\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.4858 - acc: 0.8061 - val_loss: 0.3324 - val_acc: 0.8585\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.4696 - acc: 0.8187 - val_loss: 0.3239 - val_acc: 0.8648\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.4568 - acc: 0.8281 - val_loss: 0.3119 - val_acc: 0.8717\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.4440 - acc: 0.8381 - val_loss: 0.3076 - val_acc: 0.8768\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.4330 - acc: 0.8465 - val_loss: 0.3076 - val_acc: 0.8820\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.4235 - acc: 0.8532 - val_loss: 0.2926 - val_acc: 0.8916\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.4111 - acc: 0.8592 - val_loss: 0.2915 - val_acc: 0.8921\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.4005 - acc: 0.8653 - val_loss: 0.2917 - val_acc: 0.8934\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.3919 - acc: 0.8700 - val_loss: 0.2871 - val_acc: 0.8953\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3826 - acc: 0.8748 - val_loss: 0.2827 - val_acc: 0.8979\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.3758 - acc: 0.8779 - val_loss: 0.2783 - val_acc: 0.8990\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.3666 - acc: 0.8812 - val_loss: 0.2811 - val_acc: 0.8955\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.3584 - acc: 0.8850 - val_loss: 0.2819 - val_acc: 0.8970\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.3510 - acc: 0.8866 - val_loss: 0.2831 - val_acc: 0.8951\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 10s 477ms/step - loss: 0.3468 - acc: 0.8891 - val_loss: 0.2744 - val_acc: 0.8977\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3392 - acc: 0.8915 - val_loss: 0.2710 - val_acc: 0.9006\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.3332 - acc: 0.8933 - val_loss: 0.2674 - val_acc: 0.9042\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3264 - acc: 0.8953 - val_loss: 0.2685 - val_acc: 0.9056\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3220 - acc: 0.8972 - val_loss: 0.2716 - val_acc: 0.9038\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.3145 - acc: 0.8989 - val_loss: 0.2671 - val_acc: 0.9065\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3107 - acc: 0.9001 - val_loss: 0.2663 - val_acc: 0.9078\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.3058 - acc: 0.9007 - val_loss: 0.2576 - val_acc: 0.9082\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.3022 - acc: 0.9022 - val_loss: 0.2576 - val_acc: 0.9062\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2970 - acc: 0.9037 - val_loss: 0.2566 - val_acc: 0.9078\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2921 - acc: 0.9042 - val_loss: 0.2630 - val_acc: 0.9051\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2900 - acc: 0.9049 - val_loss: 0.2518 - val_acc: 0.9089\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2851 - acc: 0.9063 - val_loss: 0.2480 - val_acc: 0.9083\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2810 - acc: 0.9075 - val_loss: 0.2491 - val_acc: 0.9091\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.2775 - acc: 0.9080 - val_loss: 0.2477 - val_acc: 0.9065\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2738 - acc: 0.9090 - val_loss: 0.2412 - val_acc: 0.9087\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 10s 476ms/step - loss: 0.2716 - acc: 0.9089 - val_loss: 0.2565 - val_acc: 0.9050\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2702 - acc: 0.9094 - val_loss: 0.2491 - val_acc: 0.9052\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2648 - acc: 0.9105 - val_loss: 0.2459 - val_acc: 0.9080\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2644 - acc: 0.9105 - val_loss: 0.2470 - val_acc: 0.9070\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 10s 467ms/step - loss: 0.2607 - acc: 0.9110 - val_loss: 0.2464 - val_acc: 0.9061\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2589 - acc: 0.9117 - val_loss: 0.2410 - val_acc: 0.9084\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2567 - acc: 0.9124 - val_loss: 0.2394 - val_acc: 0.9068\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2542 - acc: 0.9130 - val_loss: 0.2429 - val_acc: 0.9085\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2515 - acc: 0.9132 - val_loss: 0.2387 - val_acc: 0.9068\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.2498 - acc: 0.9136 - val_loss: 0.2378 - val_acc: 0.9080\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2474 - acc: 0.9143 - val_loss: 0.2392 - val_acc: 0.9074\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.2448 - acc: 0.9148 - val_loss: 0.2400 - val_acc: 0.9103\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2443 - acc: 0.9145 - val_loss: 0.2396 - val_acc: 0.9073\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2429 - acc: 0.9146 - val_loss: 0.2375 - val_acc: 0.9080\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.2400 - acc: 0.9159 - val_loss: 0.2402 - val_acc: 0.9080\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2386 - acc: 0.9155 - val_loss: 0.2383 - val_acc: 0.9074\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.2351 - acc: 0.9169 - val_loss: 0.2347 - val_acc: 0.9069\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2350 - acc: 0.9163 - val_loss: 0.2364 - val_acc: 0.9072\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.2319 - acc: 0.9181 - val_loss: 0.2424 - val_acc: 0.9081\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2303 - acc: 0.9179 - val_loss: 0.2379 - val_acc: 0.9038\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2280 - acc: 0.9185 - val_loss: 0.2414 - val_acc: 0.9047\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 10s 466ms/step - loss: 0.2274 - acc: 0.9183 - val_loss: 0.2404 - val_acc: 0.9045\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.2251 - acc: 0.9190 - val_loss: 0.2528 - val_acc: 0.9004\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2239 - acc: 0.9194 - val_loss: 0.2356 - val_acc: 0.9065\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2235 - acc: 0.9196 - val_loss: 0.2436 - val_acc: 0.9040\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2192 - acc: 0.9211 - val_loss: 0.2402 - val_acc: 0.9035\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2183 - acc: 0.9210 - val_loss: 0.2413 - val_acc: 0.9052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd8cf0f250>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd8cf0f850>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdd8cf0f910>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9772220156094413, AP:0.14655615978660674')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7260"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 20s 909ms/step - loss: 1.1156 - acc: 0.4821 - val_loss: 0.8597 - val_acc: 0.4757\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.9870 - acc: 0.5227 - val_loss: 0.6496 - val_acc: 0.6589\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.8981 - acc: 0.5552 - val_loss: 0.5394 - val_acc: 0.7475\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 17s 780ms/step - loss: 0.8314 - acc: 0.5848 - val_loss: 0.4931 - val_acc: 0.7861\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.7738 - acc: 0.6128 - val_loss: 0.4405 - val_acc: 0.8224\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.7242 - acc: 0.6379 - val_loss: 0.3937 - val_acc: 0.8513\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.6836 - acc: 0.6614 - val_loss: 0.3704 - val_acc: 0.8565\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.6492 - acc: 0.6838 - val_loss: 0.3575 - val_acc: 0.8536\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.6156 - acc: 0.7055 - val_loss: 0.3445 - val_acc: 0.8580\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.5881 - acc: 0.7239 - val_loss: 0.3375 - val_acc: 0.8601\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.5642 - acc: 0.7408 - val_loss: 0.3369 - val_acc: 0.8655\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.5419 - acc: 0.7562 - val_loss: 0.3277 - val_acc: 0.8809\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 17s 770ms/step - loss: 0.5232 - acc: 0.7723 - val_loss: 0.3274 - val_acc: 0.8843\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.5045 - acc: 0.7866 - val_loss: 0.3266 - val_acc: 0.8863\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.4898 - acc: 0.7977 - val_loss: 0.3187 - val_acc: 0.8953\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.4748 - acc: 0.8098 - val_loss: 0.3206 - val_acc: 0.8951\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.4623 - acc: 0.8189 - val_loss: 0.3203 - val_acc: 0.8951\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.4533 - acc: 0.8269 - val_loss: 0.3176 - val_acc: 0.8955\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.4413 - acc: 0.8347 - val_loss: 0.3180 - val_acc: 0.8962\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.4296 - acc: 0.8436 - val_loss: 0.3085 - val_acc: 0.8971\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.4200 - acc: 0.8491 - val_loss: 0.3173 - val_acc: 0.8971\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.4133 - acc: 0.8539 - val_loss: 0.3096 - val_acc: 0.8957\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.4030 - acc: 0.8596 - val_loss: 0.3120 - val_acc: 0.8996\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.3963 - acc: 0.8640 - val_loss: 0.3047 - val_acc: 0.8978\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.3883 - acc: 0.8677 - val_loss: 0.3039 - val_acc: 0.9028\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.3798 - acc: 0.8721 - val_loss: 0.2964 - val_acc: 0.9021\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 17s 768ms/step - loss: 0.3722 - acc: 0.8755 - val_loss: 0.2969 - val_acc: 0.9057\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.3669 - acc: 0.8782 - val_loss: 0.2912 - val_acc: 0.9049\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 17s 780ms/step - loss: 0.3611 - acc: 0.8803 - val_loss: 0.2941 - val_acc: 0.9034\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.3549 - acc: 0.8830 - val_loss: 0.2879 - val_acc: 0.9065\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 17s 770ms/step - loss: 0.3480 - acc: 0.8853 - val_loss: 0.2898 - val_acc: 0.9022\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.3450 - acc: 0.8865 - val_loss: 0.2821 - val_acc: 0.9027\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.3406 - acc: 0.8876 - val_loss: 0.2802 - val_acc: 0.9075\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.3348 - acc: 0.8893 - val_loss: 0.2861 - val_acc: 0.9043\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.3292 - acc: 0.8913 - val_loss: 0.2848 - val_acc: 0.9079\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.3255 - acc: 0.8928 - val_loss: 0.2745 - val_acc: 0.9070\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.3215 - acc: 0.8935 - val_loss: 0.2836 - val_acc: 0.9055\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.3171 - acc: 0.8945 - val_loss: 0.2812 - val_acc: 0.9077\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.3118 - acc: 0.8965 - val_loss: 0.2715 - val_acc: 0.9102\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.3078 - acc: 0.8971 - val_loss: 0.2759 - val_acc: 0.9103\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.3032 - acc: 0.8983 - val_loss: 0.2680 - val_acc: 0.9088\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 17s 779ms/step - loss: 0.2997 - acc: 0.8992 - val_loss: 0.2681 - val_acc: 0.9101\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 17s 779ms/step - loss: 0.2946 - acc: 0.9000 - val_loss: 0.2612 - val_acc: 0.9104\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.2921 - acc: 0.9007 - val_loss: 0.2652 - val_acc: 0.9091\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.2876 - acc: 0.9021 - val_loss: 0.2646 - val_acc: 0.9084\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.2840 - acc: 0.9030 - val_loss: 0.2544 - val_acc: 0.9122\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.2790 - acc: 0.9046 - val_loss: 0.2572 - val_acc: 0.9112\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2772 - acc: 0.9045 - val_loss: 0.2525 - val_acc: 0.9113\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2727 - acc: 0.9058 - val_loss: 0.2531 - val_acc: 0.9096\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2696 - acc: 0.9062 - val_loss: 0.2508 - val_acc: 0.9106\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2677 - acc: 0.9071 - val_loss: 0.2507 - val_acc: 0.9106\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2642 - acc: 0.9081 - val_loss: 0.2538 - val_acc: 0.9077\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2604 - acc: 0.9091 - val_loss: 0.2425 - val_acc: 0.9075\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2576 - acc: 0.9099 - val_loss: 0.2529 - val_acc: 0.9091\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.2559 - acc: 0.9106 - val_loss: 0.2460 - val_acc: 0.9090\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.2542 - acc: 0.9106 - val_loss: 0.2438 - val_acc: 0.9115\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2494 - acc: 0.9121 - val_loss: 0.2465 - val_acc: 0.9090\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 17s 782ms/step - loss: 0.2474 - acc: 0.9128 - val_loss: 0.2414 - val_acc: 0.9079\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.2439 - acc: 0.9136 - val_loss: 0.2465 - val_acc: 0.9066\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.2440 - acc: 0.9130 - val_loss: 0.2375 - val_acc: 0.9088\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2415 - acc: 0.9140 - val_loss: 0.2367 - val_acc: 0.9082\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2385 - acc: 0.9143 - val_loss: 0.2411 - val_acc: 0.9064\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2368 - acc: 0.9152 - val_loss: 0.2403 - val_acc: 0.9083\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2360 - acc: 0.9153 - val_loss: 0.2394 - val_acc: 0.9091\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2342 - acc: 0.9160 - val_loss: 0.2337 - val_acc: 0.9089\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.2339 - acc: 0.9156 - val_loss: 0.2388 - val_acc: 0.9095\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.2280 - acc: 0.9174 - val_loss: 0.2356 - val_acc: 0.9063\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.2279 - acc: 0.9182 - val_loss: 0.2298 - val_acc: 0.9087\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2236 - acc: 0.9194 - val_loss: 0.2382 - val_acc: 0.9091\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2250 - acc: 0.9187 - val_loss: 0.2416 - val_acc: 0.9103\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2209 - acc: 0.9202 - val_loss: 0.2336 - val_acc: 0.9090\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.2202 - acc: 0.9198 - val_loss: 0.2356 - val_acc: 0.9080\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2193 - acc: 0.9204 - val_loss: 0.2357 - val_acc: 0.9091\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.2169 - acc: 0.9208 - val_loss: 0.2378 - val_acc: 0.9107\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.2147 - acc: 0.9221 - val_loss: 0.2334 - val_acc: 0.9095\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2145 - acc: 0.9213 - val_loss: 0.2320 - val_acc: 0.9105\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2125 - acc: 0.9219 - val_loss: 0.2432 - val_acc: 0.9091\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2120 - acc: 0.9219 - val_loss: 0.2345 - val_acc: 0.9104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddba22bad0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddba23e150>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddba23e610>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9778939925221608, AP:0.15259699828410422')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7260"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 27s 1s/step - loss: 0.8891 - acc: 0.5667 - val_loss: 0.5267 - val_acc: 0.7816\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.7921 - acc: 0.6143 - val_loss: 0.4426 - val_acc: 0.8308\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.7193 - acc: 0.6605 - val_loss: 0.4188 - val_acc: 0.8255\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.6626 - acc: 0.7024 - val_loss: 0.4226 - val_acc: 0.8202\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.6164 - acc: 0.7381 - val_loss: 0.4238 - val_acc: 0.8216\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.5795 - acc: 0.7679 - val_loss: 0.4149 - val_acc: 0.8282\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.5502 - acc: 0.7923 - val_loss: 0.4066 - val_acc: 0.8341\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.5229 - acc: 0.8121 - val_loss: 0.3978 - val_acc: 0.8414\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.5057 - acc: 0.8262 - val_loss: 0.3955 - val_acc: 0.8443\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4871 - acc: 0.8376 - val_loss: 0.3765 - val_acc: 0.8526\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4680 - acc: 0.8484 - val_loss: 0.3651 - val_acc: 0.8600\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4568 - acc: 0.8553 - val_loss: 0.3628 - val_acc: 0.8697\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4427 - acc: 0.8625 - val_loss: 0.3486 - val_acc: 0.8768\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4311 - acc: 0.8675 - val_loss: 0.3402 - val_acc: 0.8786\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4184 - acc: 0.8724 - val_loss: 0.3281 - val_acc: 0.8843\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4090 - acc: 0.8765 - val_loss: 0.3260 - val_acc: 0.8847\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4016 - acc: 0.8787 - val_loss: 0.3211 - val_acc: 0.8849\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 23s 1s/step - loss: 0.3907 - acc: 0.8818 - val_loss: 0.3152 - val_acc: 0.8848\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 23s 1s/step - loss: 0.3800 - acc: 0.8849 - val_loss: 0.3208 - val_acc: 0.8821\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3748 - acc: 0.8871 - val_loss: 0.3183 - val_acc: 0.8831\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3646 - acc: 0.8888 - val_loss: 0.3014 - val_acc: 0.8861\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3585 - acc: 0.8903 - val_loss: 0.3013 - val_acc: 0.8880\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3498 - acc: 0.8929 - val_loss: 0.2983 - val_acc: 0.8886\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3448 - acc: 0.8936 - val_loss: 0.2904 - val_acc: 0.8927\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3385 - acc: 0.8945 - val_loss: 0.2921 - val_acc: 0.8943\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3297 - acc: 0.8973 - val_loss: 0.2926 - val_acc: 0.8917\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3237 - acc: 0.8985 - val_loss: 0.2878 - val_acc: 0.8949\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3185 - acc: 0.8994 - val_loss: 0.2897 - val_acc: 0.8936\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3147 - acc: 0.8997 - val_loss: 0.2826 - val_acc: 0.8937\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3100 - acc: 0.9006 - val_loss: 0.2794 - val_acc: 0.8950\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3063 - acc: 0.9011 - val_loss: 0.2779 - val_acc: 0.8986\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3013 - acc: 0.9026 - val_loss: 0.2839 - val_acc: 0.8930\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2974 - acc: 0.9031 - val_loss: 0.2791 - val_acc: 0.8960\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2955 - acc: 0.9033 - val_loss: 0.2717 - val_acc: 0.9010\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2916 - acc: 0.9037 - val_loss: 0.2727 - val_acc: 0.8967\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2871 - acc: 0.9054 - val_loss: 0.2725 - val_acc: 0.8958\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2828 - acc: 0.9057 - val_loss: 0.2655 - val_acc: 0.8978\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2770 - acc: 0.9073 - val_loss: 0.2608 - val_acc: 0.9006\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2774 - acc: 0.9071 - val_loss: 0.2653 - val_acc: 0.8977\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2745 - acc: 0.9072 - val_loss: 0.2591 - val_acc: 0.9012\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2722 - acc: 0.9075 - val_loss: 0.2590 - val_acc: 0.8977\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2687 - acc: 0.9082 - val_loss: 0.2661 - val_acc: 0.8964\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2661 - acc: 0.9084 - val_loss: 0.2550 - val_acc: 0.8975\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2650 - acc: 0.9083 - val_loss: 0.2592 - val_acc: 0.9003\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2612 - acc: 0.9094 - val_loss: 0.2557 - val_acc: 0.9022\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2613 - acc: 0.9090 - val_loss: 0.2550 - val_acc: 0.8971\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2573 - acc: 0.9096 - val_loss: 0.2496 - val_acc: 0.9034\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2554 - acc: 0.9100 - val_loss: 0.2556 - val_acc: 0.8988\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2530 - acc: 0.9106 - val_loss: 0.2489 - val_acc: 0.9020\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2516 - acc: 0.9108 - val_loss: 0.2475 - val_acc: 0.9035\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2479 - acc: 0.9121 - val_loss: 0.2496 - val_acc: 0.9007\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2484 - acc: 0.9110 - val_loss: 0.2572 - val_acc: 0.8956\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2455 - acc: 0.9123 - val_loss: 0.2550 - val_acc: 0.8977\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2428 - acc: 0.9127 - val_loss: 0.2530 - val_acc: 0.8973\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2416 - acc: 0.9130 - val_loss: 0.2527 - val_acc: 0.8990\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2407 - acc: 0.9128 - val_loss: 0.2534 - val_acc: 0.8957\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2375 - acc: 0.9135 - val_loss: 0.2480 - val_acc: 0.8971\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2385 - acc: 0.9132 - val_loss: 0.2476 - val_acc: 0.8986\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2348 - acc: 0.9143 - val_loss: 0.2602 - val_acc: 0.8958\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2335 - acc: 0.9142 - val_loss: 0.2561 - val_acc: 0.8950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb7468750>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb7468d50>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddb7468950>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9779521546493605, AP:0.14983381135490878')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7270"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 33s 2s/step - loss: 0.9065 - acc: 0.5445 - val_loss: 0.6091 - val_acc: 0.6765\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.7921 - acc: 0.5930 - val_loss: 0.4949 - val_acc: 0.8106\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.7328 - acc: 0.6255 - val_loss: 0.4639 - val_acc: 0.8435\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.6870 - acc: 0.6529 - val_loss: 0.4644 - val_acc: 0.8506\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.6528 - acc: 0.6782 - val_loss: 0.4408 - val_acc: 0.8676\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.6234 - acc: 0.6995 - val_loss: 0.4226 - val_acc: 0.8775\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5950 - acc: 0.7221 - val_loss: 0.4129 - val_acc: 0.8784\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5737 - acc: 0.7423 - val_loss: 0.3979 - val_acc: 0.8836\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5526 - acc: 0.7590 - val_loss: 0.3902 - val_acc: 0.8877\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5323 - acc: 0.7767 - val_loss: 0.3713 - val_acc: 0.8883\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5161 - acc: 0.7923 - val_loss: 0.3649 - val_acc: 0.8899\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4971 - acc: 0.8072 - val_loss: 0.3495 - val_acc: 0.8903\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4800 - acc: 0.8201 - val_loss: 0.3463 - val_acc: 0.8923\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4674 - acc: 0.8302 - val_loss: 0.3395 - val_acc: 0.8936\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4543 - acc: 0.8414 - val_loss: 0.3247 - val_acc: 0.8908\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4409 - acc: 0.8498 - val_loss: 0.3236 - val_acc: 0.8870\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4282 - acc: 0.8579 - val_loss: 0.3197 - val_acc: 0.8997\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4174 - acc: 0.8641 - val_loss: 0.3074 - val_acc: 0.9014\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4070 - acc: 0.8694 - val_loss: 0.3134 - val_acc: 0.9000\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3958 - acc: 0.8748 - val_loss: 0.3008 - val_acc: 0.9029\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3876 - acc: 0.8785 - val_loss: 0.2985 - val_acc: 0.9006\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3773 - acc: 0.8822 - val_loss: 0.2992 - val_acc: 0.9042\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3687 - acc: 0.8854 - val_loss: 0.3031 - val_acc: 0.8986\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3630 - acc: 0.8885 - val_loss: 0.2954 - val_acc: 0.9033\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3537 - acc: 0.8905 - val_loss: 0.2866 - val_acc: 0.9043\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3500 - acc: 0.8922 - val_loss: 0.2911 - val_acc: 0.9010\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3432 - acc: 0.8939 - val_loss: 0.2795 - val_acc: 0.9042\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3381 - acc: 0.8949 - val_loss: 0.2839 - val_acc: 0.9060\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3304 - acc: 0.8974 - val_loss: 0.2790 - val_acc: 0.9047\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3269 - acc: 0.8983 - val_loss: 0.2858 - val_acc: 0.9055\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3213 - acc: 0.8990 - val_loss: 0.2789 - val_acc: 0.9020\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3139 - acc: 0.9012 - val_loss: 0.2718 - val_acc: 0.9040\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3104 - acc: 0.9018 - val_loss: 0.2692 - val_acc: 0.9045\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3059 - acc: 0.9026 - val_loss: 0.2735 - val_acc: 0.9028\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3028 - acc: 0.9027 - val_loss: 0.2652 - val_acc: 0.9054\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2978 - acc: 0.9034 - val_loss: 0.2644 - val_acc: 0.9045\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2940 - acc: 0.9046 - val_loss: 0.2660 - val_acc: 0.9029\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2924 - acc: 0.9049 - val_loss: 0.2634 - val_acc: 0.9061\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2894 - acc: 0.9045 - val_loss: 0.2632 - val_acc: 0.9051\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2881 - acc: 0.9048 - val_loss: 0.2566 - val_acc: 0.9055\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2829 - acc: 0.9060 - val_loss: 0.2475 - val_acc: 0.9073\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2803 - acc: 0.9062 - val_loss: 0.2565 - val_acc: 0.9064\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2790 - acc: 0.9061 - val_loss: 0.2571 - val_acc: 0.9046\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2775 - acc: 0.9068 - val_loss: 0.2476 - val_acc: 0.9044\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2739 - acc: 0.9063 - val_loss: 0.2477 - val_acc: 0.9075\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2734 - acc: 0.9069 - val_loss: 0.2466 - val_acc: 0.9059\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2695 - acc: 0.9080 - val_loss: 0.2506 - val_acc: 0.9068\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2661 - acc: 0.9084 - val_loss: 0.2432 - val_acc: 0.9053\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2640 - acc: 0.9086 - val_loss: 0.2420 - val_acc: 0.9069\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2619 - acc: 0.9086 - val_loss: 0.2461 - val_acc: 0.9043\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2598 - acc: 0.9099 - val_loss: 0.2438 - val_acc: 0.9079\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2578 - acc: 0.9096 - val_loss: 0.2422 - val_acc: 0.9049\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2560 - acc: 0.9103 - val_loss: 0.2421 - val_acc: 0.9049\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2562 - acc: 0.9096 - val_loss: 0.2381 - val_acc: 0.9059\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2539 - acc: 0.9099 - val_loss: 0.2339 - val_acc: 0.9062\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2522 - acc: 0.9105 - val_loss: 0.2378 - val_acc: 0.9056\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2486 - acc: 0.9111 - val_loss: 0.2379 - val_acc: 0.9041\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2475 - acc: 0.9112 - val_loss: 0.2333 - val_acc: 0.9082\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2473 - acc: 0.9113 - val_loss: 0.2381 - val_acc: 0.9048\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2465 - acc: 0.9116 - val_loss: 0.2401 - val_acc: 0.9039\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2426 - acc: 0.9123 - val_loss: 0.2400 - val_acc: 0.9060\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2413 - acc: 0.9127 - val_loss: 0.2325 - val_acc: 0.9086\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2393 - acc: 0.9132 - val_loss: 0.2334 - val_acc: 0.9067\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2387 - acc: 0.9131 - val_loss: 0.2372 - val_acc: 0.9060\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2354 - acc: 0.9146 - val_loss: 0.2355 - val_acc: 0.9048\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2356 - acc: 0.9135 - val_loss: 0.2342 - val_acc: 0.9035\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2342 - acc: 0.9142 - val_loss: 0.2312 - val_acc: 0.9083\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2320 - acc: 0.9149 - val_loss: 0.2314 - val_acc: 0.9064\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2320 - acc: 0.9147 - val_loss: 0.2349 - val_acc: 0.9075\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2306 - acc: 0.9150 - val_loss: 0.2343 - val_acc: 0.9054\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2296 - acc: 0.9157 - val_loss: 0.2355 - val_acc: 0.9057\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2280 - acc: 0.9160 - val_loss: 0.2306 - val_acc: 0.9060\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2277 - acc: 0.9157 - val_loss: 0.2360 - val_acc: 0.9068\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2248 - acc: 0.9163 - val_loss: 0.2314 - val_acc: 0.9064\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2236 - acc: 0.9166 - val_loss: 0.2385 - val_acc: 0.9049\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2220 - acc: 0.9172 - val_loss: 0.2291 - val_acc: 0.9082\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2231 - acc: 0.9161 - val_loss: 0.2332 - val_acc: 0.9068\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2199 - acc: 0.9179 - val_loss: 0.2311 - val_acc: 0.9066\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2197 - acc: 0.9173 - val_loss: 0.2300 - val_acc: 0.9062\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2183 - acc: 0.9184 - val_loss: 0.2333 - val_acc: 0.9050\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2197 - acc: 0.9176 - val_loss: 0.2306 - val_acc: 0.9058\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2182 - acc: 0.9176 - val_loss: 0.2327 - val_acc: 0.9062\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2154 - acc: 0.9187 - val_loss: 0.2249 - val_acc: 0.9067\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2143 - acc: 0.9190 - val_loss: 0.2319 - val_acc: 0.9066\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2152 - acc: 0.9184 - val_loss: 0.2347 - val_acc: 0.9048\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2136 - acc: 0.9192 - val_loss: 0.2287 - val_acc: 0.9075\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2127 - acc: 0.9200 - val_loss: 0.2313 - val_acc: 0.9061\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2112 - acc: 0.9194 - val_loss: 0.2284 - val_acc: 0.9076\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2090 - acc: 0.9210 - val_loss: 0.2373 - val_acc: 0.9073\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2077 - acc: 0.9211 - val_loss: 0.2282 - val_acc: 0.9067\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2079 - acc: 0.9214 - val_loss: 0.2297 - val_acc: 0.9056\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2062 - acc: 0.9212 - val_loss: 0.2275 - val_acc: 0.9063\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2061 - acc: 0.9217 - val_loss: 0.2233 - val_acc: 0.9102\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2075 - acc: 0.9210 - val_loss: 0.2260 - val_acc: 0.9086\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2042 - acc: 0.9218 - val_loss: 0.2252 - val_acc: 0.9120\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2029 - acc: 0.9224 - val_loss: 0.2285 - val_acc: 0.9090\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2023 - acc: 0.9226 - val_loss: 0.2303 - val_acc: 0.9033\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2024 - acc: 0.9222 - val_loss: 0.2213 - val_acc: 0.9076\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2004 - acc: 0.9237 - val_loss: 0.2332 - val_acc: 0.9031\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2000 - acc: 0.9229 - val_loss: 0.2297 - val_acc: 0.9060\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1987 - acc: 0.9242 - val_loss: 0.2225 - val_acc: 0.9101\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1970 - acc: 0.9246 - val_loss: 0.2246 - val_acc: 0.9092\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1963 - acc: 0.9244 - val_loss: 0.2251 - val_acc: 0.9098\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1950 - acc: 0.9248 - val_loss: 0.2287 - val_acc: 0.9087\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1964 - acc: 0.9241 - val_loss: 0.2350 - val_acc: 0.9039\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1946 - acc: 0.9252 - val_loss: 0.2223 - val_acc: 0.9091\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1934 - acc: 0.9256 - val_loss: 0.2351 - val_acc: 0.9085\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1937 - acc: 0.9251 - val_loss: 0.2366 - val_acc: 0.9083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb4c4cf90>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb4c53610>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddb4c53850>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9710349666186499, AP:0.1612372173839275')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7270"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 39s 2s/step - loss: 1.0158 - acc: 0.4996 - val_loss: 0.7104 - val_acc: 0.5446\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.9310 - acc: 0.5271 - val_loss: 0.6083 - val_acc: 0.6688\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.8686 - acc: 0.5494 - val_loss: 0.5286 - val_acc: 0.7462\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.8131 - acc: 0.5753 - val_loss: 0.4409 - val_acc: 0.8002\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.7630 - acc: 0.6026 - val_loss: 0.3924 - val_acc: 0.8207\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.7187 - acc: 0.6297 - val_loss: 0.3793 - val_acc: 0.8295\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6814 - acc: 0.6550 - val_loss: 0.3796 - val_acc: 0.8320\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6485 - acc: 0.6784 - val_loss: 0.3748 - val_acc: 0.8340\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6152 - acc: 0.7016 - val_loss: 0.3646 - val_acc: 0.8437\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5909 - acc: 0.7214 - val_loss: 0.3575 - val_acc: 0.8469\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5649 - acc: 0.7416 - val_loss: 0.3420 - val_acc: 0.8553\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5441 - acc: 0.7583 - val_loss: 0.3325 - val_acc: 0.8715\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5275 - acc: 0.7733 - val_loss: 0.3285 - val_acc: 0.8740\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5067 - acc: 0.7878 - val_loss: 0.3192 - val_acc: 0.8789\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4928 - acc: 0.7997 - val_loss: 0.3140 - val_acc: 0.8845\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4758 - acc: 0.8116 - val_loss: 0.3121 - val_acc: 0.8875\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4631 - acc: 0.8219 - val_loss: 0.3132 - val_acc: 0.8878\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4514 - acc: 0.8315 - val_loss: 0.3026 - val_acc: 0.8890\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4365 - acc: 0.8398 - val_loss: 0.3000 - val_acc: 0.8903\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4259 - acc: 0.8476 - val_loss: 0.2992 - val_acc: 0.8885\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4157 - acc: 0.8539 - val_loss: 0.3016 - val_acc: 0.8896\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4043 - acc: 0.8607 - val_loss: 0.2996 - val_acc: 0.8881\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3944 - acc: 0.8665 - val_loss: 0.3004 - val_acc: 0.8896\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3853 - acc: 0.8711 - val_loss: 0.2940 - val_acc: 0.8929\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3767 - acc: 0.8751 - val_loss: 0.2999 - val_acc: 0.8867\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3686 - acc: 0.8791 - val_loss: 0.2981 - val_acc: 0.8934\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3616 - acc: 0.8825 - val_loss: 0.2918 - val_acc: 0.8967\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3530 - acc: 0.8853 - val_loss: 0.2879 - val_acc: 0.8923\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3478 - acc: 0.8874 - val_loss: 0.2851 - val_acc: 0.8981\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3407 - acc: 0.8900 - val_loss: 0.2791 - val_acc: 0.8966\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3341 - acc: 0.8930 - val_loss: 0.2781 - val_acc: 0.9009\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3274 - acc: 0.8950 - val_loss: 0.2755 - val_acc: 0.8966\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3225 - acc: 0.8961 - val_loss: 0.2780 - val_acc: 0.8943\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3172 - acc: 0.8989 - val_loss: 0.2803 - val_acc: 0.8935\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3128 - acc: 0.8993 - val_loss: 0.2672 - val_acc: 0.9013\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3091 - acc: 0.9007 - val_loss: 0.2700 - val_acc: 0.9011\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3027 - acc: 0.9025 - val_loss: 0.2668 - val_acc: 0.8970\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3029 - acc: 0.9023 - val_loss: 0.2715 - val_acc: 0.8979\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2962 - acc: 0.9042 - val_loss: 0.2609 - val_acc: 0.9002\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2922 - acc: 0.9049 - val_loss: 0.2711 - val_acc: 0.8970\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2890 - acc: 0.9060 - val_loss: 0.2619 - val_acc: 0.9002\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2878 - acc: 0.9060 - val_loss: 0.2638 - val_acc: 0.8966\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2812 - acc: 0.9073 - val_loss: 0.2715 - val_acc: 0.8944\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2791 - acc: 0.9083 - val_loss: 0.2720 - val_acc: 0.8945\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2766 - acc: 0.9083 - val_loss: 0.2529 - val_acc: 0.9010\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2730 - acc: 0.9091 - val_loss: 0.2512 - val_acc: 0.9018\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2722 - acc: 0.9091 - val_loss: 0.2553 - val_acc: 0.9018\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2667 - acc: 0.9109 - val_loss: 0.2514 - val_acc: 0.9029\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2664 - acc: 0.9109 - val_loss: 0.2541 - val_acc: 0.9017\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2629 - acc: 0.9110 - val_loss: 0.2622 - val_acc: 0.8978\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2602 - acc: 0.9118 - val_loss: 0.2542 - val_acc: 0.9011\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2597 - acc: 0.9122 - val_loss: 0.2671 - val_acc: 0.8966\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2571 - acc: 0.9125 - val_loss: 0.2518 - val_acc: 0.9028\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2529 - acc: 0.9131 - val_loss: 0.2553 - val_acc: 0.9027\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2522 - acc: 0.9129 - val_loss: 0.2574 - val_acc: 0.9011\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2511 - acc: 0.9138 - val_loss: 0.2486 - val_acc: 0.9058\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2479 - acc: 0.9143 - val_loss: 0.2593 - val_acc: 0.8975\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2457 - acc: 0.9141 - val_loss: 0.2532 - val_acc: 0.9024\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2444 - acc: 0.9150 - val_loss: 0.2550 - val_acc: 0.8999\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2438 - acc: 0.9154 - val_loss: 0.2636 - val_acc: 0.8996\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2410 - acc: 0.9157 - val_loss: 0.2541 - val_acc: 0.9052\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2398 - acc: 0.9164 - val_loss: 0.2656 - val_acc: 0.9011\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2369 - acc: 0.9162 - val_loss: 0.2544 - val_acc: 0.9026\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2344 - acc: 0.9168 - val_loss: 0.2474 - val_acc: 0.9075\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2350 - acc: 0.9170 - val_loss: 0.2610 - val_acc: 0.9008\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2312 - acc: 0.9182 - val_loss: 0.2618 - val_acc: 0.9027\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2323 - acc: 0.9179 - val_loss: 0.2501 - val_acc: 0.9064\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2272 - acc: 0.9193 - val_loss: 0.2573 - val_acc: 0.9057\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2275 - acc: 0.9190 - val_loss: 0.2585 - val_acc: 0.9023\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2256 - acc: 0.9190 - val_loss: 0.2578 - val_acc: 0.9028\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2232 - acc: 0.9199 - val_loss: 0.2526 - val_acc: 0.9057\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2243 - acc: 0.9196 - val_loss: 0.2517 - val_acc: 0.9070\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2212 - acc: 0.9202 - val_loss: 0.2611 - val_acc: 0.9038\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2205 - acc: 0.9205 - val_loss: 0.2615 - val_acc: 0.9013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb21c2f10>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb21d4590>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddb21d4650>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9742290035302044, AP:0.15433801758943058')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7270"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 44s 2s/step - loss: 1.0635 - acc: 0.5150 - val_loss: 0.5241 - val_acc: 0.7364\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.9688 - acc: 0.5406 - val_loss: 0.4736 - val_acc: 0.7775\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.8907 - acc: 0.5680 - val_loss: 0.4167 - val_acc: 0.8252\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.8237 - acc: 0.5942 - val_loss: 0.3682 - val_acc: 0.8407\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.7640 - acc: 0.6205 - val_loss: 0.3560 - val_acc: 0.8381\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.7170 - acc: 0.6438 - val_loss: 0.3483 - val_acc: 0.8413\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.6746 - acc: 0.6657 - val_loss: 0.3397 - val_acc: 0.8472\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.6402 - acc: 0.6873 - val_loss: 0.3384 - val_acc: 0.8491\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.6108 - acc: 0.7076 - val_loss: 0.3310 - val_acc: 0.8540\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5871 - acc: 0.7258 - val_loss: 0.3288 - val_acc: 0.8566\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5597 - acc: 0.7446 - val_loss: 0.3251 - val_acc: 0.8701\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5404 - acc: 0.7599 - val_loss: 0.3195 - val_acc: 0.8762\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5204 - acc: 0.7750 - val_loss: 0.3180 - val_acc: 0.8799\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5048 - acc: 0.7881 - val_loss: 0.3179 - val_acc: 0.8869\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4874 - acc: 0.7996 - val_loss: 0.3174 - val_acc: 0.8892\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4702 - acc: 0.8121 - val_loss: 0.3199 - val_acc: 0.8901\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4584 - acc: 0.8211 - val_loss: 0.3213 - val_acc: 0.8900\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4446 - acc: 0.8302 - val_loss: 0.3181 - val_acc: 0.8907\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.4326 - acc: 0.8385 - val_loss: 0.3189 - val_acc: 0.8900\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4222 - acc: 0.8459 - val_loss: 0.3175 - val_acc: 0.8924\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4110 - acc: 0.8523 - val_loss: 0.3157 - val_acc: 0.8959\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4027 - acc: 0.8564 - val_loss: 0.3014 - val_acc: 0.8942\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3940 - acc: 0.8619 - val_loss: 0.3098 - val_acc: 0.8952\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3823 - acc: 0.8667 - val_loss: 0.3001 - val_acc: 0.8951\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3748 - acc: 0.8714 - val_loss: 0.2950 - val_acc: 0.8978\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3658 - acc: 0.8749 - val_loss: 0.2869 - val_acc: 0.9011\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3587 - acc: 0.8784 - val_loss: 0.2883 - val_acc: 0.8996\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3529 - acc: 0.8807 - val_loss: 0.2856 - val_acc: 0.8998\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3445 - acc: 0.8841 - val_loss: 0.2877 - val_acc: 0.8985\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.3392 - acc: 0.8865 - val_loss: 0.2812 - val_acc: 0.9006\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3348 - acc: 0.8880 - val_loss: 0.2830 - val_acc: 0.8993\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3271 - acc: 0.8906 - val_loss: 0.2800 - val_acc: 0.9018\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3207 - acc: 0.8927 - val_loss: 0.2808 - val_acc: 0.9044\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3177 - acc: 0.8936 - val_loss: 0.2750 - val_acc: 0.9009\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3109 - acc: 0.8958 - val_loss: 0.2705 - val_acc: 0.9049\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3078 - acc: 0.8963 - val_loss: 0.2706 - val_acc: 0.9011\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3036 - acc: 0.8972 - val_loss: 0.2742 - val_acc: 0.8998\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3013 - acc: 0.8983 - val_loss: 0.2695 - val_acc: 0.8990\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2956 - acc: 0.8997 - val_loss: 0.2787 - val_acc: 0.8985\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2923 - acc: 0.9010 - val_loss: 0.2643 - val_acc: 0.9004\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2903 - acc: 0.9015 - val_loss: 0.2705 - val_acc: 0.8989\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.2854 - acc: 0.9023 - val_loss: 0.2640 - val_acc: 0.9012\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.2844 - acc: 0.9032 - val_loss: 0.2603 - val_acc: 0.9002\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2790 - acc: 0.9041 - val_loss: 0.2661 - val_acc: 0.8995\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2763 - acc: 0.9044 - val_loss: 0.2631 - val_acc: 0.8978\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2751 - acc: 0.9044 - val_loss: 0.2659 - val_acc: 0.9013\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2718 - acc: 0.9055 - val_loss: 0.2600 - val_acc: 0.9003\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2686 - acc: 0.9063 - val_loss: 0.2531 - val_acc: 0.9003\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2673 - acc: 0.9069 - val_loss: 0.2519 - val_acc: 0.9024\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2648 - acc: 0.9074 - val_loss: 0.2609 - val_acc: 0.8961\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2608 - acc: 0.9087 - val_loss: 0.2508 - val_acc: 0.8994\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2587 - acc: 0.9085 - val_loss: 0.2513 - val_acc: 0.9016\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2567 - acc: 0.9091 - val_loss: 0.2488 - val_acc: 0.9004\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2552 - acc: 0.9097 - val_loss: 0.2493 - val_acc: 0.8994\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2518 - acc: 0.9101 - val_loss: 0.2493 - val_acc: 0.8986\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2509 - acc: 0.9110 - val_loss: 0.2540 - val_acc: 0.8989\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2477 - acc: 0.9114 - val_loss: 0.2427 - val_acc: 0.9008\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2489 - acc: 0.9107 - val_loss: 0.2461 - val_acc: 0.9008\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.2455 - acc: 0.9121 - val_loss: 0.2406 - val_acc: 0.9001\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2439 - acc: 0.9124 - val_loss: 0.2471 - val_acc: 0.8981\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2411 - acc: 0.9129 - val_loss: 0.2425 - val_acc: 0.9003\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2390 - acc: 0.9141 - val_loss: 0.2472 - val_acc: 0.8993\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2370 - acc: 0.9142 - val_loss: 0.2428 - val_acc: 0.8985\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2375 - acc: 0.9140 - val_loss: 0.2448 - val_acc: 0.8985\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2369 - acc: 0.9134 - val_loss: 0.2395 - val_acc: 0.9044\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2337 - acc: 0.9143 - val_loss: 0.2457 - val_acc: 0.8997\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2322 - acc: 0.9158 - val_loss: 0.2415 - val_acc: 0.9015\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2304 - acc: 0.9158 - val_loss: 0.2385 - val_acc: 0.9024\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.2291 - acc: 0.9159 - val_loss: 0.2382 - val_acc: 0.9032\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2283 - acc: 0.9168 - val_loss: 0.2405 - val_acc: 0.9030\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2249 - acc: 0.9173 - val_loss: 0.2415 - val_acc: 0.9026\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2246 - acc: 0.9173 - val_loss: 0.2357 - val_acc: 0.9055\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2216 - acc: 0.9187 - val_loss: 0.2312 - val_acc: 0.9067\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2224 - acc: 0.9180 - val_loss: 0.2441 - val_acc: 0.9029\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2199 - acc: 0.9186 - val_loss: 0.2423 - val_acc: 0.9053\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2196 - acc: 0.9186 - val_loss: 0.2352 - val_acc: 0.9045\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2171 - acc: 0.9196 - val_loss: 0.2355 - val_acc: 0.9060\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2158 - acc: 0.9203 - val_loss: 0.2380 - val_acc: 0.9087\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2145 - acc: 0.9203 - val_loss: 0.2366 - val_acc: 0.9061\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2140 - acc: 0.9199 - val_loss: 0.2482 - val_acc: 0.9063\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2123 - acc: 0.9209 - val_loss: 0.2421 - val_acc: 0.9057\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2104 - acc: 0.9213 - val_loss: 0.2504 - val_acc: 0.9001\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2083 - acc: 0.9222 - val_loss: 0.2377 - val_acc: 0.9056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddaf825850>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddaf825e50>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddaf825a50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9753839720403484, AP:0.16457622300037908')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for window in windows:\n",
    "    def time_series_generator(x,y):\n",
    "        Xn=[]\n",
    "        yn=[]\n",
    "        for n in range(len(x)-time_it):\n",
    "            if n+1>window:\n",
    "                X_train=x[n+1-window:n+1]\n",
    "            else:\n",
    "                X_train=x[0:n+1]\n",
    "                X_train=np.pad(X_train, mode='constant', pad_width=((0,window-X_train.shape[0]),(0,0)),\\\n",
    "                               constant_values=-5)\n",
    "\n",
    "            Xn.append(X_train)\n",
    "            y_train=y[n+time_it]\n",
    "            yn.append(y_train)\n",
    "\n",
    "        return Xn, yn\n",
    "\n",
    "    Xt_time = []\n",
    "    Xt_static = []\n",
    "    yt_time = []\n",
    "\n",
    "    ## positive data gathering\n",
    "    for idx in range(len(list_time_X)):\n",
    "        Xt_time_i = list_time_X[idx]\n",
    "        yt_time_i = list_time_y[idx]\n",
    "        Xt, yt = time_series_generator(Xt_time_i,yt_time_i)\n",
    "        time_static_data = list_time_static[idx]\n",
    "\n",
    "        for n in range(len(Xt)):\n",
    "            Xt_time.append(Xt[n])\n",
    "            Xt_static.append(time_static_data)\n",
    "            yt_time.append(yt[n])\n",
    "\n",
    "    positive_index = [i for i,result in enumerate(yt_time) if result==1]\n",
    "    negative_index = [i for i,result in enumerate(yt_time) if result==0]\n",
    "\n",
    "    positive_x = [Xt_time[idx] for idx in positive_index]\n",
    "    positive_x_static = [Xt_static[idx] for idx in positive_index]\n",
    "    positive_y = [yt_time[idx] for idx in positive_index]\n",
    "\n",
    "    negative_x = [Xt_time[idx] for idx in negative_index]\n",
    "    negative_x_static = [Xt_static[idx] for idx in negative_index]\n",
    "    negative_y = [yt_time[idx] for idx in negative_index]\n",
    "\n",
    "    true_X_time = np.array(positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "    true_X_static = np.array(positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "    true_y_total = np.array(positive_y, dtype=\"float32\").reshape(-1,)\n",
    "    false_X_time = np.array(negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "    false_X_static = np.array(negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "    false_y_total = np.array(negative_y, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "    def val_generator(x,y):\n",
    "        Xn=[]\n",
    "        yn=[]\n",
    "        for n in range(len(x)-time_it):\n",
    "            if n+1>window:\n",
    "                X_train=x[n+1-window:n+1]\n",
    "            else:\n",
    "                X_train=x[0:n+1]\n",
    "                X_train=np.pad(X_train, mode='constant', pad_width=((0,window-X_train.shape[0]),(0,0)),\\\n",
    "                               constant_values=-5)\n",
    "\n",
    "            Xn.append(X_train)\n",
    "            y_train=y[n+time_it]\n",
    "            yn.append(y_train)\n",
    "\n",
    "        return Xn, yn\n",
    "\n",
    "    val_Xt_time = []\n",
    "    val_Xt_static = []\n",
    "    val_yt_time = []\n",
    "\n",
    "    ## positive data gathering\n",
    "    for idx in range(len(valid_time_X)):\n",
    "        Xt_time_i = valid_time_X[idx]\n",
    "        yt_time_i = valid_time_y[idx]\n",
    "        Xt, yt = val_generator(Xt_time_i,yt_time_i)\n",
    "        time_static_data = valid_time_static[idx]\n",
    "\n",
    "        for n in range(len(Xt)):\n",
    "            val_Xt_time.append(Xt[n])\n",
    "            val_Xt_static.append(time_static_data)\n",
    "            val_yt_time.append(yt[n])\n",
    "\n",
    "    val_positive_index = [idx for idx,result in enumerate(val_yt_time) if result==1]\n",
    "    val_negative_index = [idx for idx,result in enumerate(val_yt_time) if result==0]\n",
    "\n",
    "    val_positive_x = [val_Xt_time[idx] for idx in val_positive_index]\n",
    "    val_positive_x_static = [val_Xt_static[idx] for idx in val_positive_index]\n",
    "    val_positive_y = [val_yt_time[idx] for idx in val_positive_index]\n",
    "\n",
    "    val_negative_x = [val_Xt_time[idx] for idx in val_negative_index]\n",
    "    val_negative_x_static = [val_Xt_static[idx] for idx in val_negative_index]\n",
    "    val_negative_y = [val_yt_time[idx] for idx in val_negative_index]\n",
    "\n",
    "    val_true_X_time = np.array(val_positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "    val_true_X_static = np.array(val_positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "    val_true_y_total = np.array(val_positive_y, dtype=\"float32\").reshape(-1,)\n",
    "    val_false_X_time = np.array(val_negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "    val_false_X_static = np.array(val_negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "    val_false_y_total = np.array(val_negative_y, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "    ### valid data generator\n",
    "\n",
    "    test_generator = file_generator_valid(window = window,\n",
    "                                           time_it =time_it, \n",
    "                                           feature = feature,\n",
    "                                           list_time_Xt = test_time_X, \n",
    "                                           list_time_yt = test_time_y, \n",
    "                                           list_time_tstatic = test_time_static, \n",
    "                                           list_time_Xn = test_time_Xn, \n",
    "                                           list_time_yn = test_time_yn, \n",
    "                                           list_time_nstatic = test_time_staticn)\n",
    "\n",
    "    test_X, test_static_X, test_y = test_generator.get_data()\n",
    "\n",
    "    params = {\"window\" : window,\n",
    "               \"time_it\" :time_it, \n",
    "              \"feature\": feature,\n",
    "               \"true_X_time\" : true_X_time, \n",
    "               \"true_X_static\" : true_X_static, \n",
    "               \"true_y_total\" : true_y_total, \n",
    "\n",
    "               \"false_X_time\" : false_X_time, \n",
    "               \"false_X_static\" : false_X_static, \n",
    "               \"false_y_total\" : false_y_total, \n",
    "\n",
    "               \"list_time_Xn\" : list_time_Xn, \n",
    "               \"list_time_yn\" : list_time_yn, \n",
    "               \"list_time_nstatic\" : list_time_nstatic,\n",
    "               \"size\" : 2500,\n",
    "               \"fraction\" : 0.2,\n",
    "                \"ratio\" : 1,\n",
    "               \"repeat\":3}\n",
    "    valparams = {\"window\" : window,\n",
    "               \"time_it\" :time_it, \n",
    "              \"feature\": feature,\n",
    "               \"true_X_time\" : val_true_X_time, \n",
    "               \"true_X_static\" : val_true_X_static, \n",
    "               \"true_y_total\" : val_true_y_total, \n",
    "\n",
    "               \"false_X_time\" : val_false_X_time, \n",
    "               \"false_X_static\" : val_false_X_static, \n",
    "               \"false_y_total\" : val_false_y_total, \n",
    "\n",
    "               \"list_time_Xn\" : valid_time_Xn, \n",
    "               \"list_time_yn\" : valid_time_yn, \n",
    "               \"list_time_nstatic\" : valid_time_staticn,\n",
    "                \"size\" : 600,\n",
    "                \"fraction\" : 0.2,\n",
    "                 \"ratio\" : 1,\n",
    "                \"repeat\":3}\n",
    "\n",
    "    filepath = \".hdf5\"\n",
    "\n",
    "\n",
    "    traingen = DataGenerator(**params)\n",
    "    valid_gen = DataGenerator(**valparams)\n",
    "\n",
    "    #GRU layer\n",
    "    time_input= Input(shape=(None, feature), name='time')\n",
    "    x0=layers.Masking(mask_value=-5)(time_input)\n",
    "\n",
    "    x1=layers.Dense(layer_set[activation_k][0], activation=activation_set[activation_i][0])(x0)\n",
    "    x11=layers.BatchNormalization()(x1)\n",
    "    x12=layers.Dropout(0.5)(x11)\n",
    "\n",
    "    x2=layers.GRU(layer_set[activation_k][1], activation=activation_set[activation_i][1], return_sequences=False)(x12)\n",
    "    x4=layers.BatchNormalization()(x2)\n",
    "    x5=layers.Dropout(0.5)(x4)\n",
    "\n",
    "    #static layer\n",
    "    static_input=Input(shape=(38,),  name='static')\n",
    "    x31 = layers.Dense(layer_set[activation_k][2],activation=activation_set[activation_i][2])(static_input)\n",
    "    x32=layers.BatchNormalization()(x31)\n",
    "    x33=layers.Dropout(0.5)(x32)\n",
    "    #합친 모양\n",
    "    concatenated = layers.concatenate([x5,x33], axis=-1)\n",
    "\n",
    "    x7=layers.Dense(layer_set[activation_k][3], activation=activation_set[activation_i][3])(concatenated)\n",
    "    x8=layers.BatchNormalization()(x7)\n",
    "    x9=layers.Dropout(0.5)(x8)\n",
    "    x10=layers.Dense(1, activation=activation_set[activation_i][4])(x9)\n",
    "\n",
    "    model=Model([time_input,static_input], x10)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=learning_rate[activation_j]), metrics=['accuracy'])\n",
    "\n",
    "    MODEL_SAVE_FOLDER_PATH = file_foler+'_{}/'.format(window)\n",
    "    filename = 'rnn_{}_{}_{}'.format(activation_i,activation_j,activation_k)\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "      os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + filename+filepath\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=0,\n",
    "                                 save_best_only=True, mode='min')\n",
    "\n",
    "    rnn_operating_code=model.to_json()\n",
    "    with open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"w\") as json_file : \n",
    "        json_file.write(rnn_operating_code)\n",
    "    history = History()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    rnn_train = model.fit_generator(generator= traingen,\n",
    "                                     validation_data= valid_gen,\n",
    "                                    steps_per_epoch=int(np.floor(len(baseline)*0.8/1300)),\n",
    "                                    validation_steps=10,\n",
    "                                    nb_epoch = 500, verbose=1, \n",
    "                                    callbacks = [history, checkpoint,early_stopping],\n",
    "                                    workers=-1, use_multiprocessing=True\n",
    "                                   )\n",
    "\n",
    "    json_file = open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"r\")\n",
    "    loaded_model_json = json_file.read() \n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=.003), metrics=['accuracy'])\n",
    "    loaded_model.load_weights(model_path)\n",
    "\n",
    "    model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "    roc_val_test = roc_auc_score(test_y, model_test)\n",
    "    #mAP\n",
    "    test_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "    history1.append({\"activation\":activation_i,\"window\":window})\n",
    "    history_test_auc.append(roc_val_test)\n",
    "    history_test_precision.append(test_precision)\n",
    "\n",
    "    fig = plt.figure(111)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(rnn_train.history['val_loss'], label='val loss')\n",
    "    ax.plot(rnn_train.history['loss'], label='train_loss')\n",
    "    ax.legend()\n",
    "    plt.title('auc: {}, AP:{}'.format(roc_val_test,test_precision))\n",
    "    plt.savefig(MODEL_SAVE_FOLDER_PATH+filename+\".png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "indexing=pd.MultiIndex.from_frame(pd.DataFrame(history1))\n",
    "\n",
    "outcome_list=pd.DataFrame( {\"history_test_auc\": history_test_auc,\n",
    "              \"history_test_precision\":history_test_precision}, index = indexing)\n",
    "\n",
    "\n",
    "outcome_list.to_csv(file_foler+\"window.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9290255222505227, 0.15433726547743043)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_val_test, test_precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
