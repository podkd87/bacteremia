{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #\"last expr -> all로 바꾸면 전체가 나온다. \"\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import RMSprop,adam\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras_radam import RAdam\n",
    "from keras import layers\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from inspect import signature\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import model_from_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backup1007.big_generator import file_generator \n",
    "from backup1007.big_generator import file_generator_valid\n",
    "from backup1007.big_generator import file_generator_cul\n",
    "from backup1007.big_generator import generator_cul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = pd.read_csv(\"./datafile/pd_baseline.csv\")\n",
    "baseline=baseline.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.idx.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "positive_folder = './time3_pos_total200107/'\n",
    "negative_folder = './time3_neg_total200107/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(positive_folder+\"list_time_X.txt\", \"rb\") as fp:\n",
    "    list_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"list_time_y.txt\", \"rb\") as fp:\n",
    "    list_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"list_time_static.txt\", \"rb\") as fp:\n",
    "    list_time_static=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_X.txt\", \"rb\") as fp:\n",
    "    valid_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_y.txt\", \"rb\") as fp:\n",
    "    valid_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_static.txt\", \"rb\") as fp:\n",
    "    valid_time_static=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_X.txt\", \"rb\") as fp:\n",
    "    test_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_y.txt\", \"rb\") as fp:\n",
    "    test_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_static.txt\", \"rb\") as fp:\n",
    "    test_time_static=pickle.load( fp)\n",
    "\n",
    "with open(negative_folder+\"list_time_Xn.txt\", \"rb\") as fp:\n",
    "    list_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"list_time_yn.txt\", \"rb\") as fp:\n",
    "    list_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"list_time_nstatic.txt\", \"rb\") as fp:\n",
    "    list_time_nstatic=pickle.load( fp)\n",
    "\n",
    "with open(negative_folder+\"valid_time_Xn.txt\", \"rb\") as fp:\n",
    "    valid_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"valid_time_yn.txt\", \"rb\") as fp:\n",
    "    valid_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"valid_time_staticn.txt\", \"rb\") as fp:\n",
    "    valid_time_staticn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_Xn.txt\", \"rb\") as fp:\n",
    "    test_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_yn.txt\", \"rb\") as fp:\n",
    "    test_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_staticn.txt\", \"rb\") as fp:\n",
    "    test_time_staticn=pickle.load( fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, window,  time_it, feature,\n",
    "                 true_X_time, true_X_static,true_y_total,\n",
    "                 false_X_time, false_X_static, false_y_total,\n",
    "                 list_time_Xn, list_time_yn, list_time_nstatic,\n",
    "                size, fraction, ratio, repeat):\n",
    "        'Initialization'\n",
    "        self.window = window\n",
    "        self.time_it = time_it\n",
    "        self.feature = feature\n",
    "        self.true_X_time = true_X_time\n",
    "        self.true_X_static = true_X_static\n",
    "        self.true_y_total = true_y_total\n",
    "        self.false_X_time = false_X_time\n",
    "        self.false_X_static = false_X_static\n",
    "        self.false_y_total = false_y_total\n",
    "        self.list_time_Xn = list_time_Xn\n",
    "        self.list_time_yn = list_time_yn\n",
    "        self.list_time_nstatic = list_time_nstatic\n",
    "        self.size = size\n",
    "        self.fraction = fraction\n",
    "        self.ratio = ratio\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_time_Xn) / 40))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        file_generator1 = file_generator(window = self.window,\n",
    "                                       time_it =self.time_it,\n",
    "                                       feature = self.feature,\n",
    "                                       true_X_time = self.true_X_time,\n",
    "                                       true_X_static = self.true_X_static,\n",
    "                                       true_y_total = self.true_y_total,\n",
    "\n",
    "                                       false_X_time = self.false_X_time,\n",
    "                                       false_X_static = self.false_X_static,\n",
    "                                       false_y_total = self.false_y_total,\n",
    "\n",
    "                                       list_time_Xn = self.list_time_Xn,\n",
    "                                       list_time_yn = self.list_time_yn,\n",
    "                                       list_time_nstatic = self.list_time_nstatic,\n",
    "                                        random_sampling_size = self.size,\n",
    "                                         fraction_per_case = self.fraction,\n",
    "                                         true_false_ratio = self.ratio,\n",
    "                                        repeat = self.repeat)\n",
    "        # Generate data\n",
    "        batch_X, batch_static_X, batch_y = file_generator1.get_data(index)\n",
    "\n",
    "        return ({\"time\":batch_X, \"static\":batch_static_X}, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### culture perfrom load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "culture_folder = './scale_cul_total/'\n",
    "with open(culture_folder+\"cul_valid_X.txt\", \"rb\") as fp:\n",
    "    cul_valid_X=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_valid_y.txt\", \"rb\") as fp:\n",
    "    cul_valid_y=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_valid_static.txt\", \"rb\") as fp:\n",
    "    cul_valid_static=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_val_y.txt\", \"rb\") as fp:\n",
    "    cul_val_y=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_valid_X3.txt\", \"rb\") as fp:\n",
    "    cul_valid_X3=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_valid_y3.txt\", \"rb\") as fp:\n",
    "    cul_valid_y3=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_valid_static3.txt\", \"rb\") as fp:\n",
    "    cul_valid_static3=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_val_y3.txt\", \"rb\") as fp:\n",
    "    cul_val_y3=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_test_X.txt\", \"rb\") as fp:\n",
    "    cul_test_X=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_test_y.txt\", \"rb\") as fp:\n",
    "    cul_test_y=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_test_static.txt\", \"rb\") as fp:\n",
    "    cul_test_static=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_test_yc.txt\", \"rb\") as fp:\n",
    "    cul_test_yc=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_test_X3.txt\", \"rb\") as fp:\n",
    "    cul_test_X3=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_test_y3.txt\", \"rb\") as fp:\n",
    "    cul_test_y3=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_test_static3.txt\", \"rb\") as fp:\n",
    "    cul_test_static3=pickle.load(fp)\n",
    "with open(culture_folder+\"cul_test_yc3.txt\", \"rb\") as fp:\n",
    "    cul_test_yc3=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the time_it, window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_it = 0\n",
    "window = 30\n",
    "# windows = [3,6,12,18,24,30,36]\n",
    "feature = 100\n",
    "\n",
    "\n",
    "\n",
    "activation_set = [['tanh','relu','tanh','relu','sigmoid'],\n",
    "                 ['relu','relu','relu','relu','sigmoid']]\n",
    "                 \n",
    "learning_rate = (0.003,0.0003)\n",
    "\n",
    "# layer_set = ((16,8,8,16),\n",
    "#             (16,8,8,8),\n",
    "#             (8,8,8,8),\n",
    "#             (32,16,16,16),\n",
    "#             (32,16,8,16),\n",
    "#             (128,128,128,64))\n",
    "\n",
    "layer_set = ((32,16,16,16),\n",
    "            (32,16,8,16),\n",
    "            (128,128,128,64),\n",
    "            (256,128,256,128))\n",
    "\n",
    "history1 = []\n",
    "history_val_auc = []\n",
    "history_val_precision = []\n",
    "history_test_auc = []\n",
    "history_test_precision = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_foler = './train_200114/'\n",
    "if not os.path.exists(file_foler):\n",
    "  os.mkdir(file_foler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for window in windows:\n",
    "def time_series_generator(x,y):\n",
    "    Xn=[]\n",
    "    yn=[]\n",
    "    for n in range(len(x)-time_it):\n",
    "        if n+1>window:\n",
    "            X_train=x[n+1-window:n+1]\n",
    "        else:\n",
    "            X_train=x[0:n+1]\n",
    "            X_train=np.pad(X_train, mode='constant', pad_width=((0,window-X_train.shape[0]),(0,0)),\\\n",
    "                           constant_values=-5)\n",
    "\n",
    "        Xn.append(X_train)\n",
    "        y_train=y[n+time_it]\n",
    "        yn.append(y_train)\n",
    "\n",
    "    return Xn, yn\n",
    "\n",
    "Xt_time = []\n",
    "Xt_static = []\n",
    "yt_time = []\n",
    "\n",
    "## positive data gathering\n",
    "for idx in range(len(list_time_X)):\n",
    "    Xt_time_i = list_time_X[idx]\n",
    "    yt_time_i = list_time_y[idx]\n",
    "    Xt, yt = time_series_generator(Xt_time_i,yt_time_i)\n",
    "    time_static_data = list_time_static[idx]\n",
    "\n",
    "    for n in range(len(Xt)):\n",
    "        Xt_time.append(Xt[n])\n",
    "        Xt_static.append(time_static_data)\n",
    "        yt_time.append(yt[n])\n",
    "\n",
    "positive_index = [i for i,result in enumerate(yt_time) if result==1]\n",
    "negative_index = [i for i,result in enumerate(yt_time) if result==0]\n",
    "\n",
    "positive_x = [Xt_time[idx] for idx in positive_index]\n",
    "positive_x_static = [Xt_static[idx] for idx in positive_index]\n",
    "positive_y = [yt_time[idx] for idx in positive_index]\n",
    "\n",
    "negative_x = [Xt_time[idx] for idx in negative_index]\n",
    "negative_x_static = [Xt_static[idx] for idx in negative_index]\n",
    "negative_y = [yt_time[idx] for idx in negative_index]\n",
    "\n",
    "true_X_time = np.array(positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "true_X_static = np.array(positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "true_y_total = np.array(positive_y, dtype=\"float32\").reshape(-1,)\n",
    "false_X_time = np.array(negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "false_X_static = np.array(negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "false_y_total = np.array(negative_y, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "def val_generator(x,y):\n",
    "    Xn=[]\n",
    "    yn=[]\n",
    "    for n in range(len(x)-time_it):\n",
    "        if n+1>window:\n",
    "            X_train=x[n+1-window:n+1]\n",
    "        else:\n",
    "            X_train=x[0:n+1]\n",
    "            X_train=np.pad(X_train, mode='constant', pad_width=((0,window-X_train.shape[0]),(0,0)),\\\n",
    "                           constant_values=-5)\n",
    "\n",
    "        Xn.append(X_train)\n",
    "        y_train=y[n+time_it]\n",
    "        yn.append(y_train)\n",
    "\n",
    "    return Xn, yn\n",
    "\n",
    "val_Xt_time = []\n",
    "val_Xt_static = []\n",
    "val_yt_time = []\n",
    "\n",
    "## positive data gathering\n",
    "for idx in range(len(valid_time_X)):\n",
    "    Xt_time_i = valid_time_X[idx]\n",
    "    yt_time_i = valid_time_y[idx]\n",
    "    Xt, yt = val_generator(Xt_time_i,yt_time_i)\n",
    "    time_static_data = valid_time_static[idx]\n",
    "\n",
    "    for n in range(len(Xt)):\n",
    "        val_Xt_time.append(Xt[n])\n",
    "        val_Xt_static.append(time_static_data)\n",
    "        val_yt_time.append(yt[n])\n",
    "\n",
    "val_positive_index = [idx for idx,result in enumerate(val_yt_time) if result==1]\n",
    "val_negative_index = [idx for idx,result in enumerate(val_yt_time) if result==0]\n",
    "\n",
    "val_positive_x = [val_Xt_time[idx] for idx in val_positive_index]\n",
    "val_positive_x_static = [val_Xt_static[idx] for idx in val_positive_index]\n",
    "val_positive_y = [val_yt_time[idx] for idx in val_positive_index]\n",
    "\n",
    "val_negative_x = [val_Xt_time[idx] for idx in val_negative_index]\n",
    "val_negative_x_static = [val_Xt_static[idx] for idx in val_negative_index]\n",
    "val_negative_y = [val_yt_time[idx] for idx in val_negative_index]\n",
    "\n",
    "val_true_X_time = np.array(val_positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "val_true_X_static = np.array(val_positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "val_true_y_total = np.array(val_positive_y, dtype=\"float32\").reshape(-1,)\n",
    "val_false_X_time = np.array(val_negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "val_false_X_static = np.array(val_negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "val_false_y_total = np.array(val_negative_y, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "### valid data generator\n",
    "\n",
    "test_generator = file_generator_valid(window = window,\n",
    "                                       time_it =time_it, \n",
    "                                       feature = feature,\n",
    "                                       list_time_Xt = test_time_X, \n",
    "                                       list_time_yt = test_time_y, \n",
    "                                       list_time_tstatic = test_time_static, \n",
    "                                       list_time_Xn = test_time_Xn, \n",
    "                                       list_time_yn = test_time_yn, \n",
    "                                       list_time_nstatic = test_time_staticn)\n",
    "\n",
    "test_X, test_static_X, test_y = test_generator.get_data()\n",
    "\n",
    "params = {\"window\" : window,\n",
    "           \"time_it\" :time_it, \n",
    "          \"feature\": feature,\n",
    "           \"true_X_time\" : true_X_time, \n",
    "           \"true_X_static\" : true_X_static, \n",
    "           \"true_y_total\" : true_y_total, \n",
    "\n",
    "           \"false_X_time\" : false_X_time, \n",
    "           \"false_X_static\" : false_X_static, \n",
    "           \"false_y_total\" : false_y_total, \n",
    "\n",
    "           \"list_time_Xn\" : list_time_Xn, \n",
    "           \"list_time_yn\" : list_time_yn, \n",
    "           \"list_time_nstatic\" : list_time_nstatic,\n",
    "           \"size\" : 2500,\n",
    "           \"fraction\" : 0.2,\n",
    "            \"ratio\" : 1,\n",
    "           \"repeat\":3}\n",
    "valparams = {\"window\" : window,\n",
    "           \"time_it\" :time_it, \n",
    "          \"feature\": feature,\n",
    "           \"true_X_time\" : val_true_X_time, \n",
    "           \"true_X_static\" : val_true_X_static, \n",
    "           \"true_y_total\" : val_true_y_total, \n",
    "\n",
    "           \"false_X_time\" : val_false_X_time, \n",
    "           \"false_X_static\" : val_false_X_static, \n",
    "           \"false_y_total\" : val_false_y_total, \n",
    "\n",
    "           \"list_time_Xn\" : valid_time_Xn, \n",
    "           \"list_time_yn\" : valid_time_yn, \n",
    "           \"list_time_nstatic\" : valid_time_staticn,\n",
    "            \"size\" : 600,\n",
    "            \"fraction\" : 0.2,\n",
    "             \"ratio\" : 1,\n",
    "            \"repeat\":3}\n",
    "\n",
    "filepath = \".hdf5\"\n",
    "\n",
    "\n",
    "traingen = DataGenerator(**params)\n",
    "valid_gen = DataGenerator(**valparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4afadbd8cabb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m                                        list_time_nstatic = test_time_staticn)\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_static_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m params = {\"window\" : window,\n",
      "\u001b[0;32m~/learning/backup1007/big_generator.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mX_time_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_time_Xn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0my_time_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_time_yn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mXn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_time_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_time_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mtime_static_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_time_nstatic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learning/backup1007/big_generator.py\u001b[0m in \u001b[0;36mtime_series_generator\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 X_train=np.pad(X_train, mode='constant', pad_width=((0,self.window-X_train.shape[0]),(0,0)),\\\n\u001b[0;32m--> 187\u001b[0;31m                                constant_values=-5)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mXn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mpad\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/park/lib/python3.7/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;31m# Create array with final shape and original values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;31m# (padded area is undefined)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_area_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pad_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m     \u001b[0;31m# And prepare iteration over all dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0;31m# (zipping may be more readable than using enumerate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/park/lib/python3.7/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_pad_simple\u001b[0;34m(array, pad_width, fill_value)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     )\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mpadded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_area_slice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_area_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#GRU layer\n",
    "time_input= Input(shape=(None, feature), name='time')\n",
    "x0=layers.Masking(mask_value=-5)(time_input)\n",
    "\n",
    "x1=layers.Dense(layer_set[k][0], activation=activation_set[i][0])(x0)\n",
    "x11=layers.BatchNormalization()(x1)\n",
    "x12=layers.Dropout(0.5)(x11)\n",
    "\n",
    "x2=layers.GRU(layer_set[k][1], activation=activation_set[i][1], return_sequences=False)(x12)\n",
    "x4=layers.BatchNormalization()(x2)\n",
    "x5=layers.Dropout(0.5)(x4)\n",
    "\n",
    "#static layer\n",
    "static_input=Input(shape=(38,),  name='static')\n",
    "x31 = layers.Dense(layer_set[k][2],activation=activation_set[i][2])(static_input)\n",
    "x32=layers.BatchNormalization()(x31)\n",
    "x33=layers.Dropout(0.5)(x32)\n",
    "#합친 모양\n",
    "concatenated = layers.concatenate([x5,x33], axis=-1)\n",
    "\n",
    "x7=layers.Dense(layer_set[k][3], activation=activation_set[i][3])(concatenated)\n",
    "x8=layers.BatchNormalization()(x7)\n",
    "x9=layers.Dropout(0.5)(x8)\n",
    "x10=layers.Dense(1, activation=activation_set[i][4])(x9)\n",
    "\n",
    "model=Model([time_input,static_input], x10)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=learning_rate[j]), metrics=['accuracy'])\n",
    "\n",
    "MODEL_SAVE_FOLDER_PATH = file_foler+'_{}/'.format(window)\n",
    "filename = 'rnn_{}_{}_{}'.format(i,j,k)\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "  os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "model_path = MODEL_SAVE_FOLDER_PATH + filename+filepath\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=0,\n",
    "                             save_best_only=True, mode='min')\n",
    "\n",
    "rnn_operating_code=model.to_json()\n",
    "with open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"w\") as json_file : \n",
    "    json_file.write(rnn_operating_code)\n",
    "history = History()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "rnn_train = model.fit_generator(generator= traingen,\n",
    "                                 validation_data= valid_gen,\n",
    "                                steps_per_epoch=int(np.floor(len(baseline)*0.8/1300)),\n",
    "                                validation_steps=10,\n",
    "                                nb_epoch = 500, verbose=0, \n",
    "                                callbacks = [history, checkpoint,early_stopping],\n",
    "                                workers=-1, use_multiprocessing=True\n",
    "                               )\n",
    "\n",
    "\n",
    "json_file = open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"r\")\n",
    "loaded_model_json = json_file.read() \n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=.003), metrics=['accuracy'])\n",
    "loaded_model.load_weights(model_path)\n",
    "\n",
    "model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "roc_val_test = roc_auc_score(test_y, model_test)\n",
    "#mAP\n",
    "test_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "history1.append({\"activation\":i,\"window\":window})\n",
    "history_test_auc.append(roc_val_test)\n",
    "history_test_precision.append(test_precision)\n",
    "\n",
    "fig = plt.figure(111)\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(rnn_train.history['val_loss'], label='val loss')\n",
    "ax.plot(rnn_train.history['loss'], label='train_loss')\n",
    "ax.legend()\n",
    "plt.title('auc: {}, AP:{}'.format(roc_val_test,test_precision))\n",
    "plt.savefig(MODEL_SAVE_FOLDER_PATH+filename+\".png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# indexing=pd.MultiIndex.from_frame(pd.DataFrame(history1))\n",
    "\n",
    "# outcome_list=pd.DataFrame( {\"history_test_auc\": history_test_auc,\n",
    "#               \"history_test_precision\":history_test_precision}, index = indexing)\n",
    "\n",
    "\n",
    "# outcome_list.to_csv(file_foler+\"window.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이전 연습좌표 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 37s 2s/step - loss: 0.7713 - acc: 0.6189 - val_loss: 0.3540 - val_acc: 0.8542\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5047 - acc: 0.7933 - val_loss: 0.2983 - val_acc: 0.8835\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3964 - acc: 0.8686 - val_loss: 0.2945 - val_acc: 0.8869\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3371 - acc: 0.8908 - val_loss: 0.2702 - val_acc: 0.8974\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2992 - acc: 0.8997 - val_loss: 0.2613 - val_acc: 0.9074\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2718 - acc: 0.9056 - val_loss: 0.2573 - val_acc: 0.9112\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2537 - acc: 0.9086 - val_loss: 0.2434 - val_acc: 0.9085\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2403 - acc: 0.9115 - val_loss: 0.2316 - val_acc: 0.9111\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2270 - acc: 0.9151 - val_loss: 0.2553 - val_acc: 0.8989\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2172 - acc: 0.9176 - val_loss: 0.2540 - val_acc: 0.9022\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2080 - acc: 0.9198 - val_loss: 0.2366 - val_acc: 0.9078\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2008 - acc: 0.9222 - val_loss: 0.2770 - val_acc: 0.9028\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1948 - acc: 0.9247 - val_loss: 0.2645 - val_acc: 0.9020\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1868 - acc: 0.9275 - val_loss: 0.2484 - val_acc: 0.9069\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1835 - acc: 0.9287 - val_loss: 0.2472 - val_acc: 0.9044\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1781 - acc: 0.9308 - val_loss: 0.2625 - val_acc: 0.9041\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1754 - acc: 0.9321 - val_loss: 0.2970 - val_acc: 0.9004\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1716 - acc: 0.9342 - val_loss: 0.2719 - val_acc: 0.9006\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbb2c33ba90>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbb2c341c50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbb2c341c10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9786469751894183, AP:0.16629098721513508')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7224"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 36s 2s/step - loss: 0.7384 - acc: 0.6310 - val_loss: 0.4765 - val_acc: 0.8458\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4901 - acc: 0.8067 - val_loss: 0.3250 - val_acc: 0.8973\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3757 - acc: 0.8766 - val_loss: 0.2737 - val_acc: 0.9046\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3109 - acc: 0.8972 - val_loss: 0.2550 - val_acc: 0.9096\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2795 - acc: 0.9027 - val_loss: 0.2547 - val_acc: 0.9087\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2592 - acc: 0.9064 - val_loss: 0.2972 - val_acc: 0.8924\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2440 - acc: 0.9092 - val_loss: 0.2480 - val_acc: 0.8966\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2326 - acc: 0.9116 - val_loss: 0.2717 - val_acc: 0.9004\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2220 - acc: 0.9145 - val_loss: 0.2490 - val_acc: 0.8989\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2139 - acc: 0.9164 - val_loss: 0.2783 - val_acc: 0.8941\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2075 - acc: 0.9181 - val_loss: 0.2426 - val_acc: 0.8941\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2020 - acc: 0.9194 - val_loss: 0.2760 - val_acc: 0.8869\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1964 - acc: 0.9214 - val_loss: 0.2671 - val_acc: 0.8935\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1895 - acc: 0.9245 - val_loss: 0.3072 - val_acc: 0.8912\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1839 - acc: 0.9272 - val_loss: 0.2975 - val_acc: 0.8917\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1786 - acc: 0.9291 - val_loss: 0.3060 - val_acc: 0.8945\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1754 - acc: 0.9301 - val_loss: 0.3113 - val_acc: 0.8904\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1717 - acc: 0.9323 - val_loss: 0.3255 - val_acc: 0.8846\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1671 - acc: 0.9338 - val_loss: 0.3042 - val_acc: 0.8939\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1644 - acc: 0.9353 - val_loss: 0.2996 - val_acc: 0.8985\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1602 - acc: 0.9374 - val_loss: 0.3255 - val_acc: 0.8938\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbaa6667b90>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbaa6677d50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbaa6677c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9773826709063576, AP:0.16817647861035742')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7255"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 131s 6s/step - loss: 0.5438 - acc: 0.7869 - val_loss: 0.3038 - val_acc: 0.8982\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.2716 - acc: 0.9125 - val_loss: 0.3319 - val_acc: 0.8975\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.2095 - acc: 0.9266 - val_loss: 0.2604 - val_acc: 0.9000\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.1736 - acc: 0.9380 - val_loss: 0.3082 - val_acc: 0.9051\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.1453 - acc: 0.9483 - val_loss: 0.3880 - val_acc: 0.8924\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.1158 - acc: 0.9594 - val_loss: 0.4424 - val_acc: 0.8884\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.1004 - acc: 0.9660 - val_loss: 0.5019 - val_acc: 0.8935\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.0844 - acc: 0.9720 - val_loss: 0.4906 - val_acc: 0.8961\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.0757 - acc: 0.9747 - val_loss: 0.4726 - val_acc: 0.8932\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.0666 - acc: 0.9778 - val_loss: 0.5959 - val_acc: 0.8794\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 130s 6s/step - loss: 0.0607 - acc: 0.9801 - val_loss: 0.5611 - val_acc: 0.8870\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.0552 - acc: 0.9819 - val_loss: 0.5820 - val_acc: 0.8844\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 131s 6s/step - loss: 0.0507 - acc: 0.9835 - val_loss: 0.5108 - val_acc: 0.8929\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9fa61c50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9fa6ee10>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fba9fa6ecd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9798095931920139, AP:0.17710040212037842')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7265"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 186s 8s/step - loss: 0.4865 - acc: 0.8106 - val_loss: 0.3235 - val_acc: 0.8894\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.2332 - acc: 0.9232 - val_loss: 0.5103 - val_acc: 0.8603\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.1731 - acc: 0.9396 - val_loss: 0.5858 - val_acc: 0.8590\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.1322 - acc: 0.9534 - val_loss: 0.3768 - val_acc: 0.8934\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.1028 - acc: 0.9647 - val_loss: 0.4642 - val_acc: 0.8909\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.0851 - acc: 0.9712 - val_loss: 0.5748 - val_acc: 0.8790\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 184s 8s/step - loss: 0.0707 - acc: 0.9762 - val_loss: 0.5177 - val_acc: 0.8877\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 184s 8s/step - loss: 0.0629 - acc: 0.9792 - val_loss: 0.5798 - val_acc: 0.8795\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 186s 8s/step - loss: 0.0548 - acc: 0.9821 - val_loss: 0.5419 - val_acc: 0.8881\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.0530 - acc: 0.9824 - val_loss: 0.6330 - val_acc: 0.8793\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.0447 - acc: 0.9854 - val_loss: 0.5647 - val_acc: 0.8838\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9ce62790>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9ce72950>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fba9ce72850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9725251497464504, AP:0.14983103188541194')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7261"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 37s 2s/step - loss: 0.9897 - acc: 0.5192 - val_loss: 0.6384 - val_acc: 0.6357\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.8903 - acc: 0.5571 - val_loss: 0.5609 - val_acc: 0.7660\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.8103 - acc: 0.5935 - val_loss: 0.5423 - val_acc: 0.7974\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.7449 - acc: 0.6294 - val_loss: 0.4998 - val_acc: 0.8383\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6902 - acc: 0.6637 - val_loss: 0.4704 - val_acc: 0.8492\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6477 - acc: 0.6942 - val_loss: 0.4290 - val_acc: 0.8591\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6132 - acc: 0.7215 - val_loss: 0.3897 - val_acc: 0.8666\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5812 - acc: 0.7466 - val_loss: 0.3738 - val_acc: 0.8698\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5525 - acc: 0.7681 - val_loss: 0.3513 - val_acc: 0.8725\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5259 - acc: 0.7895 - val_loss: 0.3411 - val_acc: 0.8696\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5064 - acc: 0.8066 - val_loss: 0.3389 - val_acc: 0.8667\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4829 - acc: 0.8232 - val_loss: 0.3267 - val_acc: 0.8733\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4661 - acc: 0.8366 - val_loss: 0.3145 - val_acc: 0.8799\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4490 - acc: 0.8469 - val_loss: 0.3138 - val_acc: 0.8872\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4351 - acc: 0.8558 - val_loss: 0.3165 - val_acc: 0.8846\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4223 - acc: 0.8632 - val_loss: 0.3093 - val_acc: 0.8915\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4085 - acc: 0.8705 - val_loss: 0.3138 - val_acc: 0.8909\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3968 - acc: 0.8752 - val_loss: 0.3117 - val_acc: 0.8928\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3880 - acc: 0.8796 - val_loss: 0.3074 - val_acc: 0.8981\n",
      "Epoch 20/500\n",
      "17/22 [======================>.......] - ETA: 7s - loss: 0.3808 - acc: 0.8826"
     ]
    }
   ],
   "source": [
    "history1 = []\n",
    "history_val_auc = []\n",
    "history_val_precision = []\n",
    "history_test_auc = []\n",
    "history_test_precision = []\n",
    "\n",
    "for i in range(len(activation_set)):\n",
    "    for j in range(len(learning_rate)):\n",
    "        for k in range(len(layer_set)):\n",
    "            #GRU layer\n",
    "            time_input= Input(shape=(None, feature), name='time')\n",
    "            x0=layers.Masking(mask_value=-5)(time_input)\n",
    "\n",
    "            x1=layers.Dense(layer_set[k][0], activation=activation_set[i][0])(x0)\n",
    "            x11=layers.BatchNormalization()(x1)\n",
    "            x12=layers.Dropout(0.5)(x11)\n",
    "\n",
    "            x2=layers.GRU(layer_set[k][1], activation=activation_set[i][1], return_sequences=False)(x12)\n",
    "            # x3 = layers.Dense(8)(x2)\n",
    "            x4=layers.BatchNormalization()(x2)\n",
    "            x5=layers.Dropout(0.5)(x4)\n",
    "            # x6=layers.Dense(64)(x5)\n",
    "            #static layer\n",
    "            static_input=Input(shape=(38,),  name='static')\n",
    "            x31 = layers.Dense(layer_set[k][2],activation=activation_set[i][2])(static_input)\n",
    "            x32=layers.BatchNormalization()(x31)\n",
    "            x33=layers.Dropout(0.5)(x32)\n",
    "            #합친 모양\n",
    "            concatenated = layers.concatenate([x5,x33], axis=-1)\n",
    "\n",
    "            x7=layers.Dense(layer_set[k][3], activation=activation_set[i][3])(concatenated)\n",
    "            x8=layers.BatchNormalization()(x7)\n",
    "            x9=layers.Dropout(0.5)(x8)\n",
    "            x10=layers.Dense(1, activation=activation_set[i][4])(x9)\n",
    "\n",
    "            model=Model([time_input,static_input], x10)\n",
    "            model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=learning_rate[j]), metrics=['accuracy'])\n",
    "\n",
    "            MODEL_SAVE_FOLDER_PATH = file_foler+'_{}_{}_{}/'.format(i,j,k)\n",
    "            filename = 'rnn_{}_{}_{}'.format(i,j,k)\n",
    "            if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "              os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "            model_path = MODEL_SAVE_FOLDER_PATH + filename+filepath\n",
    "            checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=0,\n",
    "                                         save_best_only=True, mode='min')\n",
    "\n",
    "            rnn_operating_code=model.to_json()\n",
    "            with open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"w\") as json_file : \n",
    "                json_file.write(rnn_operating_code)\n",
    "            history = History()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=2, mode='auto')\n",
    "            \n",
    "            rnn_train = model.fit_generator(generator= traingen,\n",
    "                                             validation_data= valid_gen,\n",
    "                                            steps_per_epoch=int(np.floor(len(baseline)*0.8/1300)),\n",
    "                                            validation_steps=10,\n",
    "                                            nb_epoch = 500, verbose=1, \n",
    "                                            callbacks = [history, checkpoint\n",
    "                                                         ,early_stopping\n",
    "                                                        ],\n",
    "                                            workers=-1, use_multiprocessing=True\n",
    "                                           )\n",
    "\n",
    "         \n",
    "            json_file = open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"r\")\n",
    "            loaded_model_json = json_file.read() \n",
    "            json_file.close()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "            loaded_model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=.003), metrics=['accuracy'])\n",
    "            loaded_model.load_weights(model_path)\n",
    "            \n",
    "            model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "            roc_val_test = roc_auc_score(test_y, model_test)\n",
    "            #mAP\n",
    "            test_precision = average_precision_score(test_y, model_test)\n",
    "            \n",
    "            history1.append({\"activation set\":i, \"learning rate\":j, \"hyperparamter_num\":k})\n",
    "            history_test_auc.append(roc_val_test)\n",
    "            history_test_precision.append(test_precision)\n",
    "            \n",
    "            fig = plt.figure(111)\n",
    "            ax = plt.subplot(111)\n",
    "            ax.plot(rnn_train.history['val_loss'], label='val loss')\n",
    "            ax.plot(rnn_train.history['loss'], label='train_loss')\n",
    "            ax.legend()\n",
    "            plt.title('auc: {}, AP:{}'.format(roc_val_test,test_precision))\n",
    "            plt.savefig(MODEL_SAVE_FOLDER_PATH+filename+\".png\")\n",
    "            plt.close()\n",
    "            \n",
    "indexing=pd.MultiIndex.from_frame(pd.DataFrame(history1))\n",
    "\n",
    "outcome_list=pd.DataFrame( {\"history_test_auc\": history_test_auc,\n",
    "              \"history_test_precision\":history_test_precision}, index = indexing)\n",
    "\n",
    "\n",
    "outcome_list.to_csv(file_foler+\"accuracy01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Figure' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-72f5897539fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Figure' object has no attribute 'plot'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.figure(111)\n",
    "ax.plot(rnn_train.history['val_loss'], label='val loss')\n",
    "ax.plot(rnn_train.history['loss'], label='train_loss')\n",
    "ax.legend()\n",
    "plt.title('training')\n",
    "plt.savefig(MODEL_SAVE_FOLDER_PATH+filename+\"_{}_{}_{}.png\".format(i,j,k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_SAVE_FOLDER_PATH = './train_200114/_0_1_1/'\n",
    "filename = 'rnn_0_1_1'\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "  os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "filepath = \".hdf5\"\n",
    "model_path = MODEL_SAVE_FOLDER_PATH + filename+filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json \n",
    "json_file = open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"r\")\n",
    "loaded_model_json = json_file.read() \n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=.003), metrics=['accuracy'])\n",
    "loaded_model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### occluding method\n",
    "* column style\n",
    "* vital, RBC, electrolyte, creatinine, inflammatory marker, WBC, ABGA, liver function test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = file_generator_valid(window = window,\n",
    "                                       time_it =time_it, \n",
    "                                       feature = feature,\n",
    "                                       list_time_Xt = test_time_X, \n",
    "                                       list_time_yt = test_time_y, \n",
    "                                       list_time_tstatic = test_time_static, \n",
    "                                       list_time_Xn = test_time_Xn, \n",
    "                                       list_time_yn = test_time_yn, \n",
    "                                       list_time_nstatic = test_time_staticn)\n",
    "\n",
    "test_X, test_static_X, test_y = test_generator.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "start = [16,28,40,52,64,76,88]\n",
    "n=12\n",
    "\n",
    "original_test_X = test_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_result = []\n",
    "for point in start:\n",
    "    test_X = original_test_X.copy()\n",
    "    test_X[:,:,point:point+n][test_X[:,:,point:point+n]!=-5]=0\n",
    "    model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "    roc_val_test = roc_auc_score(test_y, model_test)\n",
    "    test_precision = average_precision_score(test_y, model_test)\n",
    "    # roc_val, average_precision, \n",
    "    outcome_result({\"roc\":roc_val_test , \"PR\":test_precision})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9780392439841585, 0.17217443445493316)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AUROC\n",
    "# model_val = loaded_model.predict({\"time\":valid_X, \"static\":valid_static_X})\n",
    "# roc_val = roc_auc_score(valid_y, model_val)\n",
    "model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "roc_val_test = roc_auc_score(test_y, model_test)\n",
    "#mAP\n",
    "# average_precision = average_precision_score(valid_y, model_val)\n",
    "test_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "# roc_val, average_precision, \n",
    "roc_val_test , test_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fpr</th>\n",
       "      <th>tpr</th>\n",
       "      <th>1-fpr</th>\n",
       "      <th>tf</th>\n",
       "      <th>thresholds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>0.084781</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.915219</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.081195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fpr       tpr     1-fpr        tf  thresholds\n",
       "3940  0.084781  0.915222  0.915219  0.000002    0.081195"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds =roc_curve(test_y, model_test)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "i = np.arange(len(tpr)) # index for df\n",
    "roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i), '1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i), 'thresholds' : pd.Series(thresholds, index = i)})\n",
    "roc.ix[(roc.tf-0).abs().argsort()[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc.ix[(roc.tf-0).abs().argsort()[:100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여기까지 끝 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>1-recall</th>\n",
       "      <th>tf</th>\n",
       "      <th>thresholds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>108058</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.693642</td>\n",
       "      <td>0.306358</td>\n",
       "      <td>-0.256358</td>\n",
       "      <td>0.564122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        precision    recall  1-recall        tf  thresholds\n",
       "108058       0.05  0.693642  0.306358 -0.256358    0.564122"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(test_y, model_test)\n",
    "# roc_auc = auc(precision, recall)\n",
    "\n",
    "i = np.arange(len(recall)-1) # index for df\n",
    "PRgraph = pd.DataFrame({'precision' : pd.Series(precision[:-1], index=i),'recall' : pd.Series(recall[:-1], index = i), '1-recall' : pd.Series(1-recall[:-1], index = i), 'tf' : pd.Series(recall[:-1] - (1-precision[:-1]), index = i), 'thresholds' : pd.Series(thresholds, index = i)})\n",
    "PRgraph.ix[(PRgraph.precision==0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 360x360 with 0 Axes>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba7bea5bd0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x7fba7b5a6f10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Recall')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Precision')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '2-class Precision-Recall curve: AP=0.17')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "average_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(test_y, model_test)\n",
    "# precision, recall, _ = precision_recall_curve(a2, y_pred_val)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "\n",
    "plt.savefig(MODEL_SAVE_FOLDER_PATH+filename+\"PR_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8hour, 16hour reply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "positive_folder = './dataset/time3_pos_total200107/'\n",
    "negative_folder = './dataset/time3_neg_total200107/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(positive_folder+\"list_time_X.txt\", \"rb\") as fp:\n",
    "    list_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"list_time_y.txt\", \"rb\") as fp:\n",
    "    list_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"list_time_static.txt\", \"rb\") as fp:\n",
    "    list_time_static=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_X.txt\", \"rb\") as fp:\n",
    "    valid_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_y.txt\", \"rb\") as fp:\n",
    "    valid_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"valid_time_static.txt\", \"rb\") as fp:\n",
    "    valid_time_static=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_X.txt\", \"rb\") as fp:\n",
    "    test_time_X=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_y.txt\", \"rb\") as fp:\n",
    "    test_time_y=pickle.load( fp)\n",
    "with open(positive_folder+\"test_time_static.txt\", \"rb\") as fp:\n",
    "    test_time_static=pickle.load( fp)\n",
    "\n",
    "with open(negative_folder+\"list_time_Xn.txt\", \"rb\") as fp:\n",
    "    list_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"list_time_yn.txt\", \"rb\") as fp:\n",
    "    list_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"list_time_nstatic.txt\", \"rb\") as fp:\n",
    "    list_time_nstatic=pickle.load( fp)\n",
    "\n",
    "with open(negative_folder+\"valid_time_Xn.txt\", \"rb\") as fp:\n",
    "    valid_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"valid_time_yn.txt\", \"rb\") as fp:\n",
    "    valid_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"valid_time_staticn.txt\", \"rb\") as fp:\n",
    "    valid_time_staticn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_Xn.txt\", \"rb\") as fp:\n",
    "    test_time_Xn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_yn.txt\", \"rb\") as fp:\n",
    "    test_time_yn=pickle.load( fp)\n",
    "with open(negative_folder+\"test_time_staticn.txt\", \"rb\") as fp:\n",
    "    test_time_staticn=pickle.load( fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, window,  time_it, feature,\n",
    "                 true_X_time, true_X_static,true_y_total,\n",
    "                 false_X_time, false_X_static, false_y_total,\n",
    "                 list_time_Xn, list_time_yn, list_time_nstatic,\n",
    "                size, fraction, ratio, repeat):\n",
    "        'Initialization'\n",
    "        self.window = window\n",
    "        self.time_it = time_it\n",
    "        self.feature = feature\n",
    "        self.true_X_time = true_X_time\n",
    "        self.true_X_static = true_X_static\n",
    "        self.true_y_total = true_y_total\n",
    "        self.false_X_time = false_X_time\n",
    "        self.false_X_static = false_X_static\n",
    "        self.false_y_total = false_y_total\n",
    "        self.list_time_Xn = list_time_Xn\n",
    "        self.list_time_yn = list_time_yn\n",
    "        self.list_time_nstatic = list_time_nstatic\n",
    "        self.size = size\n",
    "        self.fraction = fraction\n",
    "        self.ratio = ratio\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_time_Xn) / 40))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        file_generator1 = file_generator(window = self.window,\n",
    "                                       time_it =self.time_it,\n",
    "                                       feature = self.feature,\n",
    "                                       true_X_time = self.true_X_time,\n",
    "                                       true_X_static = self.true_X_static,\n",
    "                                       true_y_total = self.true_y_total,\n",
    "\n",
    "                                       false_X_time = self.false_X_time,\n",
    "                                       false_X_static = self.false_X_static,\n",
    "                                       false_y_total = self.false_y_total,\n",
    "\n",
    "                                       list_time_Xn = self.list_time_Xn,\n",
    "                                       list_time_yn = self.list_time_yn,\n",
    "                                       list_time_nstatic = self.list_time_nstatic,\n",
    "                                        random_sampling_size = self.size,\n",
    "                                         fraction_per_case = self.fraction,\n",
    "                                         true_false_ratio = self.ratio,\n",
    "                                        repeat = self.repeat)\n",
    "        # Generate data\n",
    "        batch_X, batch_static_X, batch_y = file_generator1.get_data(index)\n",
    "\n",
    "        return ({\"time\":batch_X, \"static\":batch_static_X}, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_foler = './sindow_200119/'\n",
    "if not os.path.exists(file_foler):\n",
    "  os.mkdir(file_foler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_it = 0\n",
    "windows = [3,6,12,18,24,30,36]\n",
    "feature = 100\n",
    "\n",
    "\n",
    "\n",
    "activation_set = [['tanh','relu','tanh','relu','sigmoid'],\n",
    "                 ['relu','relu','relu','relu','sigmoid']]\n",
    "                 \n",
    "learning_rate = (0.003,0.0003)\n",
    "\n",
    "# layer_set = ((16,8,8,16),\n",
    "#             (16,8,8,8),\n",
    "#             (8,8,8,8),\n",
    "#             (32,16,16,16),\n",
    "#             (32,16,8,16),\n",
    "#             (128,128,128,64))\n",
    "\n",
    "layer_set = ((32,16,16,16),\n",
    "            (32,16,8,16),\n",
    "            (128,128,128,64),\n",
    "            (256,128,256,128))\n",
    "\n",
    "history1 = []\n",
    "history_val_auc = []\n",
    "history_val_precision = []\n",
    "history_test_auc = []\n",
    "history_test_precision = []\n",
    "\n",
    "activation_i = 0\n",
    "activation_j = 1\n",
    "activation_k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7260"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 10s 443ms/step - loss: 1.0235 - acc: 0.5562 - val_loss: 0.5877 - val_acc: 0.7262\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.8973 - acc: 0.6009 - val_loss: 0.5227 - val_acc: 0.8008\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.8108 - acc: 0.6392 - val_loss: 0.4853 - val_acc: 0.8297\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.7463 - acc: 0.6710 - val_loss: 0.4867 - val_acc: 0.8406\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.6899 - acc: 0.7024 - val_loss: 0.4289 - val_acc: 0.8680\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.6452 - acc: 0.7283 - val_loss: 0.4097 - val_acc: 0.8760\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.6064 - acc: 0.7528 - val_loss: 0.3929 - val_acc: 0.8869\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.5738 - acc: 0.7721 - val_loss: 0.3825 - val_acc: 0.8907\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.5420 - acc: 0.7898 - val_loss: 0.3658 - val_acc: 0.9005\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.5169 - acc: 0.8068 - val_loss: 0.3631 - val_acc: 0.9021\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.4955 - acc: 0.8211 - val_loss: 0.3457 - val_acc: 0.8990\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.4737 - acc: 0.8327 - val_loss: 0.3325 - val_acc: 0.8970\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.4595 - acc: 0.8417 - val_loss: 0.3259 - val_acc: 0.9004\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.4442 - acc: 0.8502 - val_loss: 0.3215 - val_acc: 0.8922\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.4312 - acc: 0.8564 - val_loss: 0.3108 - val_acc: 0.8923\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.4203 - acc: 0.8620 - val_loss: 0.3093 - val_acc: 0.8913\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.4117 - acc: 0.8668 - val_loss: 0.3096 - val_acc: 0.8969\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.4035 - acc: 0.8702 - val_loss: 0.3005 - val_acc: 0.8954\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.3922 - acc: 0.8741 - val_loss: 0.3031 - val_acc: 0.8937\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3877 - acc: 0.8765 - val_loss: 0.2875 - val_acc: 0.8955\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3782 - acc: 0.8797 - val_loss: 0.2828 - val_acc: 0.8947\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3747 - acc: 0.8806 - val_loss: 0.2836 - val_acc: 0.9007\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.3668 - acc: 0.8826 - val_loss: 0.2816 - val_acc: 0.8942\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3619 - acc: 0.8840 - val_loss: 0.2791 - val_acc: 0.9028\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3566 - acc: 0.8855 - val_loss: 0.2814 - val_acc: 0.8980\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3516 - acc: 0.8866 - val_loss: 0.2715 - val_acc: 0.9035\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 7s 309ms/step - loss: 0.3449 - acc: 0.8882 - val_loss: 0.2702 - val_acc: 0.9041\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.3418 - acc: 0.8889 - val_loss: 0.2745 - val_acc: 0.8984\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3381 - acc: 0.8896 - val_loss: 0.2658 - val_acc: 0.8979\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3310 - acc: 0.8907 - val_loss: 0.2668 - val_acc: 0.9013\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3287 - acc: 0.8916 - val_loss: 0.2601 - val_acc: 0.9001\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.3274 - acc: 0.8910 - val_loss: 0.2646 - val_acc: 0.8977\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3230 - acc: 0.8924 - val_loss: 0.2631 - val_acc: 0.8958\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.3177 - acc: 0.8934 - val_loss: 0.2557 - val_acc: 0.9005\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 7s 317ms/step - loss: 0.3153 - acc: 0.8930 - val_loss: 0.2566 - val_acc: 0.8986\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3133 - acc: 0.8936 - val_loss: 0.2603 - val_acc: 0.8949\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3112 - acc: 0.8937 - val_loss: 0.2578 - val_acc: 0.8964\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.3072 - acc: 0.8943 - val_loss: 0.2547 - val_acc: 0.8997\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.3033 - acc: 0.8948 - val_loss: 0.2546 - val_acc: 0.8973\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.3019 - acc: 0.8954 - val_loss: 0.2554 - val_acc: 0.8984\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.3003 - acc: 0.8957 - val_loss: 0.2536 - val_acc: 0.8971\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2956 - acc: 0.8965 - val_loss: 0.2525 - val_acc: 0.9003\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2947 - acc: 0.8957 - val_loss: 0.2432 - val_acc: 0.9029\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2900 - acc: 0.8968 - val_loss: 0.2476 - val_acc: 0.9024\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2893 - acc: 0.8971 - val_loss: 0.2448 - val_acc: 0.9008\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2858 - acc: 0.8982 - val_loss: 0.2495 - val_acc: 0.9004\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.2854 - acc: 0.8981 - val_loss: 0.2456 - val_acc: 0.9000\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2807 - acc: 0.8984 - val_loss: 0.2446 - val_acc: 0.9044\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2793 - acc: 0.8985 - val_loss: 0.2424 - val_acc: 0.9035\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2797 - acc: 0.8983 - val_loss: 0.2402 - val_acc: 0.9026\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2770 - acc: 0.8988 - val_loss: 0.2435 - val_acc: 0.9009\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2758 - acc: 0.8989 - val_loss: 0.2400 - val_acc: 0.9032\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2741 - acc: 0.8988 - val_loss: 0.2387 - val_acc: 0.9019\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2703 - acc: 0.9001 - val_loss: 0.2418 - val_acc: 0.9023\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2693 - acc: 0.8995 - val_loss: 0.2386 - val_acc: 0.9033\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2653 - acc: 0.9011 - val_loss: 0.2356 - val_acc: 0.9042\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 7s 318ms/step - loss: 0.2666 - acc: 0.9005 - val_loss: 0.2418 - val_acc: 0.9020\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2631 - acc: 0.9015 - val_loss: 0.2343 - val_acc: 0.9027\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2629 - acc: 0.9014 - val_loss: 0.2353 - val_acc: 0.9019\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2579 - acc: 0.9025 - val_loss: 0.2355 - val_acc: 0.9022\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.2576 - acc: 0.9028 - val_loss: 0.2394 - val_acc: 0.9038\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2570 - acc: 0.9028 - val_loss: 0.2359 - val_acc: 0.9000\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2557 - acc: 0.9030 - val_loss: 0.2331 - val_acc: 0.9022\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.2537 - acc: 0.9034 - val_loss: 0.2376 - val_acc: 0.9046\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2522 - acc: 0.9035 - val_loss: 0.2399 - val_acc: 0.9026\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2497 - acc: 0.9050 - val_loss: 0.2444 - val_acc: 0.9016\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 7s 317ms/step - loss: 0.2486 - acc: 0.9047 - val_loss: 0.2362 - val_acc: 0.9037\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2484 - acc: 0.9046 - val_loss: 0.2360 - val_acc: 0.9019\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 7s 308ms/step - loss: 0.2475 - acc: 0.9048 - val_loss: 0.2365 - val_acc: 0.9011\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2454 - acc: 0.9053 - val_loss: 0.2296 - val_acc: 0.9066\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 7s 316ms/step - loss: 0.2440 - acc: 0.9058 - val_loss: 0.2356 - val_acc: 0.9020\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2434 - acc: 0.9061 - val_loss: 0.2306 - val_acc: 0.9035\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 7s 315ms/step - loss: 0.2400 - acc: 0.9072 - val_loss: 0.2346 - val_acc: 0.9036\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 7s 317ms/step - loss: 0.2400 - acc: 0.9069 - val_loss: 0.2269 - val_acc: 0.9052\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2398 - acc: 0.9064 - val_loss: 0.2288 - val_acc: 0.9031\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2373 - acc: 0.9078 - val_loss: 0.2340 - val_acc: 0.9048\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2371 - acc: 0.9074 - val_loss: 0.2330 - val_acc: 0.9036\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 7s 311ms/step - loss: 0.2348 - acc: 0.9087 - val_loss: 0.2346 - val_acc: 0.9049\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2328 - acc: 0.9089 - val_loss: 0.2338 - val_acc: 0.9028\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 7s 312ms/step - loss: 0.2338 - acc: 0.9085 - val_loss: 0.2377 - val_acc: 0.9019\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 7s 310ms/step - loss: 0.2301 - acc: 0.9105 - val_loss: 0.2354 - val_acc: 0.9023\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 7s 313ms/step - loss: 0.2291 - acc: 0.9102 - val_loss: 0.2328 - val_acc: 0.9053\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2295 - acc: 0.9103 - val_loss: 0.2388 - val_acc: 0.9037\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 7s 314ms/step - loss: 0.2248 - acc: 0.9118 - val_loss: 0.2399 - val_acc: 0.9023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddbf155390>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddbf1559d0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddbf14f410>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.976641173874518, AP:0.13882905761826614')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7260"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 13s 602ms/step - loss: 0.9723 - acc: 0.5050 - val_loss: 0.7817 - val_acc: 0.5183\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.8743 - acc: 0.5422 - val_loss: 0.7213 - val_acc: 0.5921\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 10s 466ms/step - loss: 0.8028 - acc: 0.5774 - val_loss: 0.5554 - val_acc: 0.7629\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.7472 - acc: 0.6092 - val_loss: 0.4768 - val_acc: 0.8459\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.7021 - acc: 0.6370 - val_loss: 0.4204 - val_acc: 0.8680\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.6659 - acc: 0.6620 - val_loss: 0.3991 - val_acc: 0.8762\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.6335 - acc: 0.6855 - val_loss: 0.3871 - val_acc: 0.8661\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.6079 - acc: 0.7070 - val_loss: 0.3765 - val_acc: 0.8639\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 10s 475ms/step - loss: 0.5817 - acc: 0.7270 - val_loss: 0.3623 - val_acc: 0.8604\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.5585 - acc: 0.7455 - val_loss: 0.3575 - val_acc: 0.8594\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.5384 - acc: 0.7630 - val_loss: 0.3487 - val_acc: 0.8512\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.5209 - acc: 0.7784 - val_loss: 0.3416 - val_acc: 0.8545\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.5024 - acc: 0.7931 - val_loss: 0.3352 - val_acc: 0.8565\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.4858 - acc: 0.8061 - val_loss: 0.3324 - val_acc: 0.8585\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.4696 - acc: 0.8187 - val_loss: 0.3239 - val_acc: 0.8648\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.4568 - acc: 0.8281 - val_loss: 0.3119 - val_acc: 0.8717\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.4440 - acc: 0.8381 - val_loss: 0.3076 - val_acc: 0.8768\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.4330 - acc: 0.8465 - val_loss: 0.3076 - val_acc: 0.8820\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.4235 - acc: 0.8532 - val_loss: 0.2926 - val_acc: 0.8916\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.4111 - acc: 0.8592 - val_loss: 0.2915 - val_acc: 0.8921\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.4005 - acc: 0.8653 - val_loss: 0.2917 - val_acc: 0.8934\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.3919 - acc: 0.8700 - val_loss: 0.2871 - val_acc: 0.8953\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3826 - acc: 0.8748 - val_loss: 0.2827 - val_acc: 0.8979\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.3758 - acc: 0.8779 - val_loss: 0.2783 - val_acc: 0.8990\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.3666 - acc: 0.8812 - val_loss: 0.2811 - val_acc: 0.8955\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.3584 - acc: 0.8850 - val_loss: 0.2819 - val_acc: 0.8970\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.3510 - acc: 0.8866 - val_loss: 0.2831 - val_acc: 0.8951\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 10s 477ms/step - loss: 0.3468 - acc: 0.8891 - val_loss: 0.2744 - val_acc: 0.8977\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3392 - acc: 0.8915 - val_loss: 0.2710 - val_acc: 0.9006\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.3332 - acc: 0.8933 - val_loss: 0.2674 - val_acc: 0.9042\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3264 - acc: 0.8953 - val_loss: 0.2685 - val_acc: 0.9056\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3220 - acc: 0.8972 - val_loss: 0.2716 - val_acc: 0.9038\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.3145 - acc: 0.8989 - val_loss: 0.2671 - val_acc: 0.9065\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.3107 - acc: 0.9001 - val_loss: 0.2663 - val_acc: 0.9078\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.3058 - acc: 0.9007 - val_loss: 0.2576 - val_acc: 0.9082\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.3022 - acc: 0.9022 - val_loss: 0.2576 - val_acc: 0.9062\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2970 - acc: 0.9037 - val_loss: 0.2566 - val_acc: 0.9078\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2921 - acc: 0.9042 - val_loss: 0.2630 - val_acc: 0.9051\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2900 - acc: 0.9049 - val_loss: 0.2518 - val_acc: 0.9089\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2851 - acc: 0.9063 - val_loss: 0.2480 - val_acc: 0.9083\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2810 - acc: 0.9075 - val_loss: 0.2491 - val_acc: 0.9091\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.2775 - acc: 0.9080 - val_loss: 0.2477 - val_acc: 0.9065\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2738 - acc: 0.9090 - val_loss: 0.2412 - val_acc: 0.9087\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 10s 476ms/step - loss: 0.2716 - acc: 0.9089 - val_loss: 0.2565 - val_acc: 0.9050\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2702 - acc: 0.9094 - val_loss: 0.2491 - val_acc: 0.9052\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2648 - acc: 0.9105 - val_loss: 0.2459 - val_acc: 0.9080\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2644 - acc: 0.9105 - val_loss: 0.2470 - val_acc: 0.9070\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 10s 467ms/step - loss: 0.2607 - acc: 0.9110 - val_loss: 0.2464 - val_acc: 0.9061\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2589 - acc: 0.9117 - val_loss: 0.2410 - val_acc: 0.9084\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2567 - acc: 0.9124 - val_loss: 0.2394 - val_acc: 0.9068\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2542 - acc: 0.9130 - val_loss: 0.2429 - val_acc: 0.9085\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2515 - acc: 0.9132 - val_loss: 0.2387 - val_acc: 0.9068\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.2498 - acc: 0.9136 - val_loss: 0.2378 - val_acc: 0.9080\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2474 - acc: 0.9143 - val_loss: 0.2392 - val_acc: 0.9074\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.2448 - acc: 0.9148 - val_loss: 0.2400 - val_acc: 0.9103\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2443 - acc: 0.9145 - val_loss: 0.2396 - val_acc: 0.9073\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2429 - acc: 0.9146 - val_loss: 0.2375 - val_acc: 0.9080\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.2400 - acc: 0.9159 - val_loss: 0.2402 - val_acc: 0.9080\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 10s 469ms/step - loss: 0.2386 - acc: 0.9155 - val_loss: 0.2383 - val_acc: 0.9074\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 10s 471ms/step - loss: 0.2351 - acc: 0.9169 - val_loss: 0.2347 - val_acc: 0.9069\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2350 - acc: 0.9163 - val_loss: 0.2364 - val_acc: 0.9072\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 10s 468ms/step - loss: 0.2319 - acc: 0.9181 - val_loss: 0.2424 - val_acc: 0.9081\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2303 - acc: 0.9179 - val_loss: 0.2379 - val_acc: 0.9038\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2280 - acc: 0.9185 - val_loss: 0.2414 - val_acc: 0.9047\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 10s 466ms/step - loss: 0.2274 - acc: 0.9183 - val_loss: 0.2404 - val_acc: 0.9045\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 10s 470ms/step - loss: 0.2251 - acc: 0.9190 - val_loss: 0.2528 - val_acc: 0.9004\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 10s 474ms/step - loss: 0.2239 - acc: 0.9194 - val_loss: 0.2356 - val_acc: 0.9065\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2235 - acc: 0.9196 - val_loss: 0.2436 - val_acc: 0.9040\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 10s 472ms/step - loss: 0.2192 - acc: 0.9211 - val_loss: 0.2402 - val_acc: 0.9035\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 10s 473ms/step - loss: 0.2183 - acc: 0.9210 - val_loss: 0.2413 - val_acc: 0.9052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd8cf0f250>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdd8cf0f850>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdd8cf0f910>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9772220156094413, AP:0.14655615978660674')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7260"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 20s 909ms/step - loss: 1.1156 - acc: 0.4821 - val_loss: 0.8597 - val_acc: 0.4757\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.9870 - acc: 0.5227 - val_loss: 0.6496 - val_acc: 0.6589\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.8981 - acc: 0.5552 - val_loss: 0.5394 - val_acc: 0.7475\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 17s 780ms/step - loss: 0.8314 - acc: 0.5848 - val_loss: 0.4931 - val_acc: 0.7861\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.7738 - acc: 0.6128 - val_loss: 0.4405 - val_acc: 0.8224\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.7242 - acc: 0.6379 - val_loss: 0.3937 - val_acc: 0.8513\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.6836 - acc: 0.6614 - val_loss: 0.3704 - val_acc: 0.8565\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.6492 - acc: 0.6838 - val_loss: 0.3575 - val_acc: 0.8536\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.6156 - acc: 0.7055 - val_loss: 0.3445 - val_acc: 0.8580\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.5881 - acc: 0.7239 - val_loss: 0.3375 - val_acc: 0.8601\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.5642 - acc: 0.7408 - val_loss: 0.3369 - val_acc: 0.8655\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.5419 - acc: 0.7562 - val_loss: 0.3277 - val_acc: 0.8809\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 17s 770ms/step - loss: 0.5232 - acc: 0.7723 - val_loss: 0.3274 - val_acc: 0.8843\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.5045 - acc: 0.7866 - val_loss: 0.3266 - val_acc: 0.8863\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.4898 - acc: 0.7977 - val_loss: 0.3187 - val_acc: 0.8953\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.4748 - acc: 0.8098 - val_loss: 0.3206 - val_acc: 0.8951\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.4623 - acc: 0.8189 - val_loss: 0.3203 - val_acc: 0.8951\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.4533 - acc: 0.8269 - val_loss: 0.3176 - val_acc: 0.8955\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.4413 - acc: 0.8347 - val_loss: 0.3180 - val_acc: 0.8962\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.4296 - acc: 0.8436 - val_loss: 0.3085 - val_acc: 0.8971\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.4200 - acc: 0.8491 - val_loss: 0.3173 - val_acc: 0.8971\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.4133 - acc: 0.8539 - val_loss: 0.3096 - val_acc: 0.8957\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.4030 - acc: 0.8596 - val_loss: 0.3120 - val_acc: 0.8996\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.3963 - acc: 0.8640 - val_loss: 0.3047 - val_acc: 0.8978\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.3883 - acc: 0.8677 - val_loss: 0.3039 - val_acc: 0.9028\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.3798 - acc: 0.8721 - val_loss: 0.2964 - val_acc: 0.9021\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 17s 768ms/step - loss: 0.3722 - acc: 0.8755 - val_loss: 0.2969 - val_acc: 0.9057\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.3669 - acc: 0.8782 - val_loss: 0.2912 - val_acc: 0.9049\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 17s 780ms/step - loss: 0.3611 - acc: 0.8803 - val_loss: 0.2941 - val_acc: 0.9034\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.3549 - acc: 0.8830 - val_loss: 0.2879 - val_acc: 0.9065\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 17s 770ms/step - loss: 0.3480 - acc: 0.8853 - val_loss: 0.2898 - val_acc: 0.9022\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.3450 - acc: 0.8865 - val_loss: 0.2821 - val_acc: 0.9027\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.3406 - acc: 0.8876 - val_loss: 0.2802 - val_acc: 0.9075\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.3348 - acc: 0.8893 - val_loss: 0.2861 - val_acc: 0.9043\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.3292 - acc: 0.8913 - val_loss: 0.2848 - val_acc: 0.9079\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.3255 - acc: 0.8928 - val_loss: 0.2745 - val_acc: 0.9070\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.3215 - acc: 0.8935 - val_loss: 0.2836 - val_acc: 0.9055\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.3171 - acc: 0.8945 - val_loss: 0.2812 - val_acc: 0.9077\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.3118 - acc: 0.8965 - val_loss: 0.2715 - val_acc: 0.9102\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.3078 - acc: 0.8971 - val_loss: 0.2759 - val_acc: 0.9103\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.3032 - acc: 0.8983 - val_loss: 0.2680 - val_acc: 0.9088\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 17s 779ms/step - loss: 0.2997 - acc: 0.8992 - val_loss: 0.2681 - val_acc: 0.9101\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 17s 779ms/step - loss: 0.2946 - acc: 0.9000 - val_loss: 0.2612 - val_acc: 0.9104\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.2921 - acc: 0.9007 - val_loss: 0.2652 - val_acc: 0.9091\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.2876 - acc: 0.9021 - val_loss: 0.2646 - val_acc: 0.9084\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.2840 - acc: 0.9030 - val_loss: 0.2544 - val_acc: 0.9122\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.2790 - acc: 0.9046 - val_loss: 0.2572 - val_acc: 0.9112\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2772 - acc: 0.9045 - val_loss: 0.2525 - val_acc: 0.9113\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2727 - acc: 0.9058 - val_loss: 0.2531 - val_acc: 0.9096\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2696 - acc: 0.9062 - val_loss: 0.2508 - val_acc: 0.9106\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2677 - acc: 0.9071 - val_loss: 0.2507 - val_acc: 0.9106\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2642 - acc: 0.9081 - val_loss: 0.2538 - val_acc: 0.9077\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2604 - acc: 0.9091 - val_loss: 0.2425 - val_acc: 0.9075\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2576 - acc: 0.9099 - val_loss: 0.2529 - val_acc: 0.9091\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.2559 - acc: 0.9106 - val_loss: 0.2460 - val_acc: 0.9090\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.2542 - acc: 0.9106 - val_loss: 0.2438 - val_acc: 0.9115\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2494 - acc: 0.9121 - val_loss: 0.2465 - val_acc: 0.9090\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 17s 782ms/step - loss: 0.2474 - acc: 0.9128 - val_loss: 0.2414 - val_acc: 0.9079\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.2439 - acc: 0.9136 - val_loss: 0.2465 - val_acc: 0.9066\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.2440 - acc: 0.9130 - val_loss: 0.2375 - val_acc: 0.9088\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2415 - acc: 0.9140 - val_loss: 0.2367 - val_acc: 0.9082\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2385 - acc: 0.9143 - val_loss: 0.2411 - val_acc: 0.9064\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2368 - acc: 0.9152 - val_loss: 0.2403 - val_acc: 0.9083\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2360 - acc: 0.9153 - val_loss: 0.2394 - val_acc: 0.9091\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2342 - acc: 0.9160 - val_loss: 0.2337 - val_acc: 0.9089\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 17s 773ms/step - loss: 0.2339 - acc: 0.9156 - val_loss: 0.2388 - val_acc: 0.9095\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.2280 - acc: 0.9174 - val_loss: 0.2356 - val_acc: 0.9063\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 17s 772ms/step - loss: 0.2279 - acc: 0.9182 - val_loss: 0.2298 - val_acc: 0.9087\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2236 - acc: 0.9194 - val_loss: 0.2382 - val_acc: 0.9091\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2250 - acc: 0.9187 - val_loss: 0.2416 - val_acc: 0.9103\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2209 - acc: 0.9202 - val_loss: 0.2336 - val_acc: 0.9090\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.2202 - acc: 0.9198 - val_loss: 0.2356 - val_acc: 0.9080\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2193 - acc: 0.9204 - val_loss: 0.2357 - val_acc: 0.9091\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 17s 776ms/step - loss: 0.2169 - acc: 0.9208 - val_loss: 0.2378 - val_acc: 0.9107\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 17s 778ms/step - loss: 0.2147 - acc: 0.9221 - val_loss: 0.2334 - val_acc: 0.9095\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 17s 771ms/step - loss: 0.2145 - acc: 0.9213 - val_loss: 0.2320 - val_acc: 0.9105\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 17s 775ms/step - loss: 0.2125 - acc: 0.9219 - val_loss: 0.2432 - val_acc: 0.9091\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 17s 774ms/step - loss: 0.2120 - acc: 0.9219 - val_loss: 0.2345 - val_acc: 0.9104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddba22bad0>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddba23e150>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddba23e610>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9778939925221608, AP:0.15259699828410422')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7260"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 27s 1s/step - loss: 0.8891 - acc: 0.5667 - val_loss: 0.5267 - val_acc: 0.7816\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.7921 - acc: 0.6143 - val_loss: 0.4426 - val_acc: 0.8308\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.7193 - acc: 0.6605 - val_loss: 0.4188 - val_acc: 0.8255\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.6626 - acc: 0.7024 - val_loss: 0.4226 - val_acc: 0.8202\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.6164 - acc: 0.7381 - val_loss: 0.4238 - val_acc: 0.8216\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.5795 - acc: 0.7679 - val_loss: 0.4149 - val_acc: 0.8282\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.5502 - acc: 0.7923 - val_loss: 0.4066 - val_acc: 0.8341\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.5229 - acc: 0.8121 - val_loss: 0.3978 - val_acc: 0.8414\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.5057 - acc: 0.8262 - val_loss: 0.3955 - val_acc: 0.8443\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4871 - acc: 0.8376 - val_loss: 0.3765 - val_acc: 0.8526\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4680 - acc: 0.8484 - val_loss: 0.3651 - val_acc: 0.8600\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4568 - acc: 0.8553 - val_loss: 0.3628 - val_acc: 0.8697\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4427 - acc: 0.8625 - val_loss: 0.3486 - val_acc: 0.8768\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4311 - acc: 0.8675 - val_loss: 0.3402 - val_acc: 0.8786\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4184 - acc: 0.8724 - val_loss: 0.3281 - val_acc: 0.8843\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4090 - acc: 0.8765 - val_loss: 0.3260 - val_acc: 0.8847\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.4016 - acc: 0.8787 - val_loss: 0.3211 - val_acc: 0.8849\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 23s 1s/step - loss: 0.3907 - acc: 0.8818 - val_loss: 0.3152 - val_acc: 0.8848\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 23s 1s/step - loss: 0.3800 - acc: 0.8849 - val_loss: 0.3208 - val_acc: 0.8821\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3748 - acc: 0.8871 - val_loss: 0.3183 - val_acc: 0.8831\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3646 - acc: 0.8888 - val_loss: 0.3014 - val_acc: 0.8861\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3585 - acc: 0.8903 - val_loss: 0.3013 - val_acc: 0.8880\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3498 - acc: 0.8929 - val_loss: 0.2983 - val_acc: 0.8886\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3448 - acc: 0.8936 - val_loss: 0.2904 - val_acc: 0.8927\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3385 - acc: 0.8945 - val_loss: 0.2921 - val_acc: 0.8943\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3297 - acc: 0.8973 - val_loss: 0.2926 - val_acc: 0.8917\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3237 - acc: 0.8985 - val_loss: 0.2878 - val_acc: 0.8949\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3185 - acc: 0.8994 - val_loss: 0.2897 - val_acc: 0.8936\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3147 - acc: 0.8997 - val_loss: 0.2826 - val_acc: 0.8937\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3100 - acc: 0.9006 - val_loss: 0.2794 - val_acc: 0.8950\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3063 - acc: 0.9011 - val_loss: 0.2779 - val_acc: 0.8986\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.3013 - acc: 0.9026 - val_loss: 0.2839 - val_acc: 0.8930\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2974 - acc: 0.9031 - val_loss: 0.2791 - val_acc: 0.8960\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2955 - acc: 0.9033 - val_loss: 0.2717 - val_acc: 0.9010\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2916 - acc: 0.9037 - val_loss: 0.2727 - val_acc: 0.8967\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2871 - acc: 0.9054 - val_loss: 0.2725 - val_acc: 0.8958\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2828 - acc: 0.9057 - val_loss: 0.2655 - val_acc: 0.8978\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2770 - acc: 0.9073 - val_loss: 0.2608 - val_acc: 0.9006\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2774 - acc: 0.9071 - val_loss: 0.2653 - val_acc: 0.8977\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2745 - acc: 0.9072 - val_loss: 0.2591 - val_acc: 0.9012\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2722 - acc: 0.9075 - val_loss: 0.2590 - val_acc: 0.8977\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2687 - acc: 0.9082 - val_loss: 0.2661 - val_acc: 0.8964\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2661 - acc: 0.9084 - val_loss: 0.2550 - val_acc: 0.8975\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2650 - acc: 0.9083 - val_loss: 0.2592 - val_acc: 0.9003\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2612 - acc: 0.9094 - val_loss: 0.2557 - val_acc: 0.9022\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2613 - acc: 0.9090 - val_loss: 0.2550 - val_acc: 0.8971\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2573 - acc: 0.9096 - val_loss: 0.2496 - val_acc: 0.9034\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2554 - acc: 0.9100 - val_loss: 0.2556 - val_acc: 0.8988\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2530 - acc: 0.9106 - val_loss: 0.2489 - val_acc: 0.9020\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2516 - acc: 0.9108 - val_loss: 0.2475 - val_acc: 0.9035\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2479 - acc: 0.9121 - val_loss: 0.2496 - val_acc: 0.9007\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2484 - acc: 0.9110 - val_loss: 0.2572 - val_acc: 0.8956\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2455 - acc: 0.9123 - val_loss: 0.2550 - val_acc: 0.8977\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2428 - acc: 0.9127 - val_loss: 0.2530 - val_acc: 0.8973\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2416 - acc: 0.9130 - val_loss: 0.2527 - val_acc: 0.8990\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2407 - acc: 0.9128 - val_loss: 0.2534 - val_acc: 0.8957\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2375 - acc: 0.9135 - val_loss: 0.2480 - val_acc: 0.8971\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2385 - acc: 0.9132 - val_loss: 0.2476 - val_acc: 0.8986\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2348 - acc: 0.9143 - val_loss: 0.2602 - val_acc: 0.8958\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 24s 1s/step - loss: 0.2335 - acc: 0.9142 - val_loss: 0.2561 - val_acc: 0.8950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb7468750>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb7468d50>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddb7468950>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9779521546493605, AP:0.14983381135490878')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7270"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 33s 2s/step - loss: 0.9065 - acc: 0.5445 - val_loss: 0.6091 - val_acc: 0.6765\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.7921 - acc: 0.5930 - val_loss: 0.4949 - val_acc: 0.8106\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.7328 - acc: 0.6255 - val_loss: 0.4639 - val_acc: 0.8435\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.6870 - acc: 0.6529 - val_loss: 0.4644 - val_acc: 0.8506\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.6528 - acc: 0.6782 - val_loss: 0.4408 - val_acc: 0.8676\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.6234 - acc: 0.6995 - val_loss: 0.4226 - val_acc: 0.8775\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5950 - acc: 0.7221 - val_loss: 0.4129 - val_acc: 0.8784\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5737 - acc: 0.7423 - val_loss: 0.3979 - val_acc: 0.8836\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5526 - acc: 0.7590 - val_loss: 0.3902 - val_acc: 0.8877\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5323 - acc: 0.7767 - val_loss: 0.3713 - val_acc: 0.8883\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.5161 - acc: 0.7923 - val_loss: 0.3649 - val_acc: 0.8899\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4971 - acc: 0.8072 - val_loss: 0.3495 - val_acc: 0.8903\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4800 - acc: 0.8201 - val_loss: 0.3463 - val_acc: 0.8923\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4674 - acc: 0.8302 - val_loss: 0.3395 - val_acc: 0.8936\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4543 - acc: 0.8414 - val_loss: 0.3247 - val_acc: 0.8908\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4409 - acc: 0.8498 - val_loss: 0.3236 - val_acc: 0.8870\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4282 - acc: 0.8579 - val_loss: 0.3197 - val_acc: 0.8997\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4174 - acc: 0.8641 - val_loss: 0.3074 - val_acc: 0.9014\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.4070 - acc: 0.8694 - val_loss: 0.3134 - val_acc: 0.9000\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3958 - acc: 0.8748 - val_loss: 0.3008 - val_acc: 0.9029\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3876 - acc: 0.8785 - val_loss: 0.2985 - val_acc: 0.9006\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3773 - acc: 0.8822 - val_loss: 0.2992 - val_acc: 0.9042\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3687 - acc: 0.8854 - val_loss: 0.3031 - val_acc: 0.8986\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3630 - acc: 0.8885 - val_loss: 0.2954 - val_acc: 0.9033\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3537 - acc: 0.8905 - val_loss: 0.2866 - val_acc: 0.9043\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3500 - acc: 0.8922 - val_loss: 0.2911 - val_acc: 0.9010\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3432 - acc: 0.8939 - val_loss: 0.2795 - val_acc: 0.9042\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3381 - acc: 0.8949 - val_loss: 0.2839 - val_acc: 0.9060\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3304 - acc: 0.8974 - val_loss: 0.2790 - val_acc: 0.9047\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3269 - acc: 0.8983 - val_loss: 0.2858 - val_acc: 0.9055\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3213 - acc: 0.8990 - val_loss: 0.2789 - val_acc: 0.9020\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3139 - acc: 0.9012 - val_loss: 0.2718 - val_acc: 0.9040\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3104 - acc: 0.9018 - val_loss: 0.2692 - val_acc: 0.9045\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3059 - acc: 0.9026 - val_loss: 0.2735 - val_acc: 0.9028\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.3028 - acc: 0.9027 - val_loss: 0.2652 - val_acc: 0.9054\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2978 - acc: 0.9034 - val_loss: 0.2644 - val_acc: 0.9045\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2940 - acc: 0.9046 - val_loss: 0.2660 - val_acc: 0.9029\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2924 - acc: 0.9049 - val_loss: 0.2634 - val_acc: 0.9061\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2894 - acc: 0.9045 - val_loss: 0.2632 - val_acc: 0.9051\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2881 - acc: 0.9048 - val_loss: 0.2566 - val_acc: 0.9055\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2829 - acc: 0.9060 - val_loss: 0.2475 - val_acc: 0.9073\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2803 - acc: 0.9062 - val_loss: 0.2565 - val_acc: 0.9064\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2790 - acc: 0.9061 - val_loss: 0.2571 - val_acc: 0.9046\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2775 - acc: 0.9068 - val_loss: 0.2476 - val_acc: 0.9044\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2739 - acc: 0.9063 - val_loss: 0.2477 - val_acc: 0.9075\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2734 - acc: 0.9069 - val_loss: 0.2466 - val_acc: 0.9059\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2695 - acc: 0.9080 - val_loss: 0.2506 - val_acc: 0.9068\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2661 - acc: 0.9084 - val_loss: 0.2432 - val_acc: 0.9053\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2640 - acc: 0.9086 - val_loss: 0.2420 - val_acc: 0.9069\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2619 - acc: 0.9086 - val_loss: 0.2461 - val_acc: 0.9043\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2598 - acc: 0.9099 - val_loss: 0.2438 - val_acc: 0.9079\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2578 - acc: 0.9096 - val_loss: 0.2422 - val_acc: 0.9049\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2560 - acc: 0.9103 - val_loss: 0.2421 - val_acc: 0.9049\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2562 - acc: 0.9096 - val_loss: 0.2381 - val_acc: 0.9059\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2539 - acc: 0.9099 - val_loss: 0.2339 - val_acc: 0.9062\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2522 - acc: 0.9105 - val_loss: 0.2378 - val_acc: 0.9056\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2486 - acc: 0.9111 - val_loss: 0.2379 - val_acc: 0.9041\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2475 - acc: 0.9112 - val_loss: 0.2333 - val_acc: 0.9082\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2473 - acc: 0.9113 - val_loss: 0.2381 - val_acc: 0.9048\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2465 - acc: 0.9116 - val_loss: 0.2401 - val_acc: 0.9039\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2426 - acc: 0.9123 - val_loss: 0.2400 - val_acc: 0.9060\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2413 - acc: 0.9127 - val_loss: 0.2325 - val_acc: 0.9086\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2393 - acc: 0.9132 - val_loss: 0.2334 - val_acc: 0.9067\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2387 - acc: 0.9131 - val_loss: 0.2372 - val_acc: 0.9060\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2354 - acc: 0.9146 - val_loss: 0.2355 - val_acc: 0.9048\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2356 - acc: 0.9135 - val_loss: 0.2342 - val_acc: 0.9035\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2342 - acc: 0.9142 - val_loss: 0.2312 - val_acc: 0.9083\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2320 - acc: 0.9149 - val_loss: 0.2314 - val_acc: 0.9064\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2320 - acc: 0.9147 - val_loss: 0.2349 - val_acc: 0.9075\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2306 - acc: 0.9150 - val_loss: 0.2343 - val_acc: 0.9054\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2296 - acc: 0.9157 - val_loss: 0.2355 - val_acc: 0.9057\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2280 - acc: 0.9160 - val_loss: 0.2306 - val_acc: 0.9060\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2277 - acc: 0.9157 - val_loss: 0.2360 - val_acc: 0.9068\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2248 - acc: 0.9163 - val_loss: 0.2314 - val_acc: 0.9064\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2236 - acc: 0.9166 - val_loss: 0.2385 - val_acc: 0.9049\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2220 - acc: 0.9172 - val_loss: 0.2291 - val_acc: 0.9082\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2231 - acc: 0.9161 - val_loss: 0.2332 - val_acc: 0.9068\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2199 - acc: 0.9179 - val_loss: 0.2311 - val_acc: 0.9066\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2197 - acc: 0.9173 - val_loss: 0.2300 - val_acc: 0.9062\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2183 - acc: 0.9184 - val_loss: 0.2333 - val_acc: 0.9050\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2197 - acc: 0.9176 - val_loss: 0.2306 - val_acc: 0.9058\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2182 - acc: 0.9176 - val_loss: 0.2327 - val_acc: 0.9062\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2154 - acc: 0.9187 - val_loss: 0.2249 - val_acc: 0.9067\n",
      "Epoch 84/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2143 - acc: 0.9190 - val_loss: 0.2319 - val_acc: 0.9066\n",
      "Epoch 85/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2152 - acc: 0.9184 - val_loss: 0.2347 - val_acc: 0.9048\n",
      "Epoch 86/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2136 - acc: 0.9192 - val_loss: 0.2287 - val_acc: 0.9075\n",
      "Epoch 87/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2127 - acc: 0.9200 - val_loss: 0.2313 - val_acc: 0.9061\n",
      "Epoch 88/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2112 - acc: 0.9194 - val_loss: 0.2284 - val_acc: 0.9076\n",
      "Epoch 89/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2090 - acc: 0.9210 - val_loss: 0.2373 - val_acc: 0.9073\n",
      "Epoch 90/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2077 - acc: 0.9211 - val_loss: 0.2282 - val_acc: 0.9067\n",
      "Epoch 91/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2079 - acc: 0.9214 - val_loss: 0.2297 - val_acc: 0.9056\n",
      "Epoch 92/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2062 - acc: 0.9212 - val_loss: 0.2275 - val_acc: 0.9063\n",
      "Epoch 93/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2061 - acc: 0.9217 - val_loss: 0.2233 - val_acc: 0.9102\n",
      "Epoch 94/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2075 - acc: 0.9210 - val_loss: 0.2260 - val_acc: 0.9086\n",
      "Epoch 95/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2042 - acc: 0.9218 - val_loss: 0.2252 - val_acc: 0.9120\n",
      "Epoch 96/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2029 - acc: 0.9224 - val_loss: 0.2285 - val_acc: 0.9090\n",
      "Epoch 97/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2023 - acc: 0.9226 - val_loss: 0.2303 - val_acc: 0.9033\n",
      "Epoch 98/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2024 - acc: 0.9222 - val_loss: 0.2213 - val_acc: 0.9076\n",
      "Epoch 99/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2004 - acc: 0.9237 - val_loss: 0.2332 - val_acc: 0.9031\n",
      "Epoch 100/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.2000 - acc: 0.9229 - val_loss: 0.2297 - val_acc: 0.9060\n",
      "Epoch 101/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1987 - acc: 0.9242 - val_loss: 0.2225 - val_acc: 0.9101\n",
      "Epoch 102/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1970 - acc: 0.9246 - val_loss: 0.2246 - val_acc: 0.9092\n",
      "Epoch 103/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1963 - acc: 0.9244 - val_loss: 0.2251 - val_acc: 0.9098\n",
      "Epoch 104/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1950 - acc: 0.9248 - val_loss: 0.2287 - val_acc: 0.9087\n",
      "Epoch 105/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1964 - acc: 0.9241 - val_loss: 0.2350 - val_acc: 0.9039\n",
      "Epoch 106/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1946 - acc: 0.9252 - val_loss: 0.2223 - val_acc: 0.9091\n",
      "Epoch 107/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1934 - acc: 0.9256 - val_loss: 0.2351 - val_acc: 0.9085\n",
      "Epoch 108/500\n",
      "22/22 [==============================] - 30s 1s/step - loss: 0.1937 - acc: 0.9251 - val_loss: 0.2366 - val_acc: 0.9083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb4c4cf90>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb4c53610>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddb4c53850>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9710349666186499, AP:0.1612372173839275')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7270"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 39s 2s/step - loss: 1.0158 - acc: 0.4996 - val_loss: 0.7104 - val_acc: 0.5446\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.9310 - acc: 0.5271 - val_loss: 0.6083 - val_acc: 0.6688\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.8686 - acc: 0.5494 - val_loss: 0.5286 - val_acc: 0.7462\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.8131 - acc: 0.5753 - val_loss: 0.4409 - val_acc: 0.8002\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.7630 - acc: 0.6026 - val_loss: 0.3924 - val_acc: 0.8207\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.7187 - acc: 0.6297 - val_loss: 0.3793 - val_acc: 0.8295\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6814 - acc: 0.6550 - val_loss: 0.3796 - val_acc: 0.8320\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6485 - acc: 0.6784 - val_loss: 0.3748 - val_acc: 0.8340\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6152 - acc: 0.7016 - val_loss: 0.3646 - val_acc: 0.8437\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5909 - acc: 0.7214 - val_loss: 0.3575 - val_acc: 0.8469\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5649 - acc: 0.7416 - val_loss: 0.3420 - val_acc: 0.8553\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5441 - acc: 0.7583 - val_loss: 0.3325 - val_acc: 0.8715\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5275 - acc: 0.7733 - val_loss: 0.3285 - val_acc: 0.8740\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5067 - acc: 0.7878 - val_loss: 0.3192 - val_acc: 0.8789\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4928 - acc: 0.7997 - val_loss: 0.3140 - val_acc: 0.8845\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4758 - acc: 0.8116 - val_loss: 0.3121 - val_acc: 0.8875\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4631 - acc: 0.8219 - val_loss: 0.3132 - val_acc: 0.8878\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4514 - acc: 0.8315 - val_loss: 0.3026 - val_acc: 0.8890\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4365 - acc: 0.8398 - val_loss: 0.3000 - val_acc: 0.8903\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4259 - acc: 0.8476 - val_loss: 0.2992 - val_acc: 0.8885\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4157 - acc: 0.8539 - val_loss: 0.3016 - val_acc: 0.8896\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4043 - acc: 0.8607 - val_loss: 0.2996 - val_acc: 0.8881\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3944 - acc: 0.8665 - val_loss: 0.3004 - val_acc: 0.8896\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3853 - acc: 0.8711 - val_loss: 0.2940 - val_acc: 0.8929\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3767 - acc: 0.8751 - val_loss: 0.2999 - val_acc: 0.8867\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3686 - acc: 0.8791 - val_loss: 0.2981 - val_acc: 0.8934\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3616 - acc: 0.8825 - val_loss: 0.2918 - val_acc: 0.8967\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3530 - acc: 0.8853 - val_loss: 0.2879 - val_acc: 0.8923\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3478 - acc: 0.8874 - val_loss: 0.2851 - val_acc: 0.8981\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3407 - acc: 0.8900 - val_loss: 0.2791 - val_acc: 0.8966\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3341 - acc: 0.8930 - val_loss: 0.2781 - val_acc: 0.9009\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3274 - acc: 0.8950 - val_loss: 0.2755 - val_acc: 0.8966\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3225 - acc: 0.8961 - val_loss: 0.2780 - val_acc: 0.8943\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3172 - acc: 0.8989 - val_loss: 0.2803 - val_acc: 0.8935\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3128 - acc: 0.8993 - val_loss: 0.2672 - val_acc: 0.9013\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3091 - acc: 0.9007 - val_loss: 0.2700 - val_acc: 0.9011\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3027 - acc: 0.9025 - val_loss: 0.2668 - val_acc: 0.8970\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3029 - acc: 0.9023 - val_loss: 0.2715 - val_acc: 0.8979\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2962 - acc: 0.9042 - val_loss: 0.2609 - val_acc: 0.9002\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2922 - acc: 0.9049 - val_loss: 0.2711 - val_acc: 0.8970\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2890 - acc: 0.9060 - val_loss: 0.2619 - val_acc: 0.9002\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2878 - acc: 0.9060 - val_loss: 0.2638 - val_acc: 0.8966\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2812 - acc: 0.9073 - val_loss: 0.2715 - val_acc: 0.8944\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2791 - acc: 0.9083 - val_loss: 0.2720 - val_acc: 0.8945\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2766 - acc: 0.9083 - val_loss: 0.2529 - val_acc: 0.9010\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2730 - acc: 0.9091 - val_loss: 0.2512 - val_acc: 0.9018\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2722 - acc: 0.9091 - val_loss: 0.2553 - val_acc: 0.9018\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2667 - acc: 0.9109 - val_loss: 0.2514 - val_acc: 0.9029\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2664 - acc: 0.9109 - val_loss: 0.2541 - val_acc: 0.9017\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2629 - acc: 0.9110 - val_loss: 0.2622 - val_acc: 0.8978\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2602 - acc: 0.9118 - val_loss: 0.2542 - val_acc: 0.9011\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2597 - acc: 0.9122 - val_loss: 0.2671 - val_acc: 0.8966\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2571 - acc: 0.9125 - val_loss: 0.2518 - val_acc: 0.9028\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2529 - acc: 0.9131 - val_loss: 0.2553 - val_acc: 0.9027\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2522 - acc: 0.9129 - val_loss: 0.2574 - val_acc: 0.9011\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2511 - acc: 0.9138 - val_loss: 0.2486 - val_acc: 0.9058\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2479 - acc: 0.9143 - val_loss: 0.2593 - val_acc: 0.8975\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2457 - acc: 0.9141 - val_loss: 0.2532 - val_acc: 0.9024\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2444 - acc: 0.9150 - val_loss: 0.2550 - val_acc: 0.8999\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2438 - acc: 0.9154 - val_loss: 0.2636 - val_acc: 0.8996\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2410 - acc: 0.9157 - val_loss: 0.2541 - val_acc: 0.9052\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2398 - acc: 0.9164 - val_loss: 0.2656 - val_acc: 0.9011\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2369 - acc: 0.9162 - val_loss: 0.2544 - val_acc: 0.9026\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2344 - acc: 0.9168 - val_loss: 0.2474 - val_acc: 0.9075\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2350 - acc: 0.9170 - val_loss: 0.2610 - val_acc: 0.9008\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2312 - acc: 0.9182 - val_loss: 0.2618 - val_acc: 0.9027\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2323 - acc: 0.9179 - val_loss: 0.2501 - val_acc: 0.9064\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2272 - acc: 0.9193 - val_loss: 0.2573 - val_acc: 0.9057\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2275 - acc: 0.9190 - val_loss: 0.2585 - val_acc: 0.9023\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2256 - acc: 0.9190 - val_loss: 0.2578 - val_acc: 0.9028\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2232 - acc: 0.9199 - val_loss: 0.2526 - val_acc: 0.9057\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2243 - acc: 0.9196 - val_loss: 0.2517 - val_acc: 0.9070\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2212 - acc: 0.9202 - val_loss: 0.2611 - val_acc: 0.9038\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2205 - acc: 0.9205 - val_loss: 0.2615 - val_acc: 0.9013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb21c2f10>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddb21d4590>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddb21d4650>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9742290035302044, AP:0.15433801758943058')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7270"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:210: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 44s 2s/step - loss: 1.0635 - acc: 0.5150 - val_loss: 0.5241 - val_acc: 0.7364\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.9688 - acc: 0.5406 - val_loss: 0.4736 - val_acc: 0.7775\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.8907 - acc: 0.5680 - val_loss: 0.4167 - val_acc: 0.8252\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.8237 - acc: 0.5942 - val_loss: 0.3682 - val_acc: 0.8407\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.7640 - acc: 0.6205 - val_loss: 0.3560 - val_acc: 0.8381\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.7170 - acc: 0.6438 - val_loss: 0.3483 - val_acc: 0.8413\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.6746 - acc: 0.6657 - val_loss: 0.3397 - val_acc: 0.8472\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.6402 - acc: 0.6873 - val_loss: 0.3384 - val_acc: 0.8491\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.6108 - acc: 0.7076 - val_loss: 0.3310 - val_acc: 0.8540\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5871 - acc: 0.7258 - val_loss: 0.3288 - val_acc: 0.8566\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5597 - acc: 0.7446 - val_loss: 0.3251 - val_acc: 0.8701\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5404 - acc: 0.7599 - val_loss: 0.3195 - val_acc: 0.8762\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5204 - acc: 0.7750 - val_loss: 0.3180 - val_acc: 0.8799\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.5048 - acc: 0.7881 - val_loss: 0.3179 - val_acc: 0.8869\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4874 - acc: 0.7996 - val_loss: 0.3174 - val_acc: 0.8892\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4702 - acc: 0.8121 - val_loss: 0.3199 - val_acc: 0.8901\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4584 - acc: 0.8211 - val_loss: 0.3213 - val_acc: 0.8900\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4446 - acc: 0.8302 - val_loss: 0.3181 - val_acc: 0.8907\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.4326 - acc: 0.8385 - val_loss: 0.3189 - val_acc: 0.8900\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4222 - acc: 0.8459 - val_loss: 0.3175 - val_acc: 0.8924\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4110 - acc: 0.8523 - val_loss: 0.3157 - val_acc: 0.8959\n",
      "Epoch 22/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.4027 - acc: 0.8564 - val_loss: 0.3014 - val_acc: 0.8942\n",
      "Epoch 23/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3940 - acc: 0.8619 - val_loss: 0.3098 - val_acc: 0.8952\n",
      "Epoch 24/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3823 - acc: 0.8667 - val_loss: 0.3001 - val_acc: 0.8951\n",
      "Epoch 25/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3748 - acc: 0.8714 - val_loss: 0.2950 - val_acc: 0.8978\n",
      "Epoch 26/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3658 - acc: 0.8749 - val_loss: 0.2869 - val_acc: 0.9011\n",
      "Epoch 27/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3587 - acc: 0.8784 - val_loss: 0.2883 - val_acc: 0.8996\n",
      "Epoch 28/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3529 - acc: 0.8807 - val_loss: 0.2856 - val_acc: 0.8998\n",
      "Epoch 29/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3445 - acc: 0.8841 - val_loss: 0.2877 - val_acc: 0.8985\n",
      "Epoch 30/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.3392 - acc: 0.8865 - val_loss: 0.2812 - val_acc: 0.9006\n",
      "Epoch 31/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3348 - acc: 0.8880 - val_loss: 0.2830 - val_acc: 0.8993\n",
      "Epoch 32/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3271 - acc: 0.8906 - val_loss: 0.2800 - val_acc: 0.9018\n",
      "Epoch 33/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3207 - acc: 0.8927 - val_loss: 0.2808 - val_acc: 0.9044\n",
      "Epoch 34/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3177 - acc: 0.8936 - val_loss: 0.2750 - val_acc: 0.9009\n",
      "Epoch 35/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3109 - acc: 0.8958 - val_loss: 0.2705 - val_acc: 0.9049\n",
      "Epoch 36/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3078 - acc: 0.8963 - val_loss: 0.2706 - val_acc: 0.9011\n",
      "Epoch 37/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3036 - acc: 0.8972 - val_loss: 0.2742 - val_acc: 0.8998\n",
      "Epoch 38/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.3013 - acc: 0.8983 - val_loss: 0.2695 - val_acc: 0.8990\n",
      "Epoch 39/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2956 - acc: 0.8997 - val_loss: 0.2787 - val_acc: 0.8985\n",
      "Epoch 40/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2923 - acc: 0.9010 - val_loss: 0.2643 - val_acc: 0.9004\n",
      "Epoch 41/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2903 - acc: 0.9015 - val_loss: 0.2705 - val_acc: 0.8989\n",
      "Epoch 42/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.2854 - acc: 0.9023 - val_loss: 0.2640 - val_acc: 0.9012\n",
      "Epoch 43/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.2844 - acc: 0.9032 - val_loss: 0.2603 - val_acc: 0.9002\n",
      "Epoch 44/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2790 - acc: 0.9041 - val_loss: 0.2661 - val_acc: 0.8995\n",
      "Epoch 45/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2763 - acc: 0.9044 - val_loss: 0.2631 - val_acc: 0.8978\n",
      "Epoch 46/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2751 - acc: 0.9044 - val_loss: 0.2659 - val_acc: 0.9013\n",
      "Epoch 47/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2718 - acc: 0.9055 - val_loss: 0.2600 - val_acc: 0.9003\n",
      "Epoch 48/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2686 - acc: 0.9063 - val_loss: 0.2531 - val_acc: 0.9003\n",
      "Epoch 49/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2673 - acc: 0.9069 - val_loss: 0.2519 - val_acc: 0.9024\n",
      "Epoch 50/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2648 - acc: 0.9074 - val_loss: 0.2609 - val_acc: 0.8961\n",
      "Epoch 51/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2608 - acc: 0.9087 - val_loss: 0.2508 - val_acc: 0.8994\n",
      "Epoch 52/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2587 - acc: 0.9085 - val_loss: 0.2513 - val_acc: 0.9016\n",
      "Epoch 53/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2567 - acc: 0.9091 - val_loss: 0.2488 - val_acc: 0.9004\n",
      "Epoch 54/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2552 - acc: 0.9097 - val_loss: 0.2493 - val_acc: 0.8994\n",
      "Epoch 55/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2518 - acc: 0.9101 - val_loss: 0.2493 - val_acc: 0.8986\n",
      "Epoch 56/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2509 - acc: 0.9110 - val_loss: 0.2540 - val_acc: 0.8989\n",
      "Epoch 57/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2477 - acc: 0.9114 - val_loss: 0.2427 - val_acc: 0.9008\n",
      "Epoch 58/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2489 - acc: 0.9107 - val_loss: 0.2461 - val_acc: 0.9008\n",
      "Epoch 59/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.2455 - acc: 0.9121 - val_loss: 0.2406 - val_acc: 0.9001\n",
      "Epoch 60/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2439 - acc: 0.9124 - val_loss: 0.2471 - val_acc: 0.8981\n",
      "Epoch 61/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2411 - acc: 0.9129 - val_loss: 0.2425 - val_acc: 0.9003\n",
      "Epoch 62/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2390 - acc: 0.9141 - val_loss: 0.2472 - val_acc: 0.8993\n",
      "Epoch 63/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2370 - acc: 0.9142 - val_loss: 0.2428 - val_acc: 0.8985\n",
      "Epoch 64/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2375 - acc: 0.9140 - val_loss: 0.2448 - val_acc: 0.8985\n",
      "Epoch 65/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2369 - acc: 0.9134 - val_loss: 0.2395 - val_acc: 0.9044\n",
      "Epoch 66/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2337 - acc: 0.9143 - val_loss: 0.2457 - val_acc: 0.8997\n",
      "Epoch 67/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2322 - acc: 0.9158 - val_loss: 0.2415 - val_acc: 0.9015\n",
      "Epoch 68/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2304 - acc: 0.9158 - val_loss: 0.2385 - val_acc: 0.9024\n",
      "Epoch 69/500\n",
      "22/22 [==============================] - 41s 2s/step - loss: 0.2291 - acc: 0.9159 - val_loss: 0.2382 - val_acc: 0.9032\n",
      "Epoch 70/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2283 - acc: 0.9168 - val_loss: 0.2405 - val_acc: 0.9030\n",
      "Epoch 71/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2249 - acc: 0.9173 - val_loss: 0.2415 - val_acc: 0.9026\n",
      "Epoch 72/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2246 - acc: 0.9173 - val_loss: 0.2357 - val_acc: 0.9055\n",
      "Epoch 73/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2216 - acc: 0.9187 - val_loss: 0.2312 - val_acc: 0.9067\n",
      "Epoch 74/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2224 - acc: 0.9180 - val_loss: 0.2441 - val_acc: 0.9029\n",
      "Epoch 75/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2199 - acc: 0.9186 - val_loss: 0.2423 - val_acc: 0.9053\n",
      "Epoch 76/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2196 - acc: 0.9186 - val_loss: 0.2352 - val_acc: 0.9045\n",
      "Epoch 77/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2171 - acc: 0.9196 - val_loss: 0.2355 - val_acc: 0.9060\n",
      "Epoch 78/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2158 - acc: 0.9203 - val_loss: 0.2380 - val_acc: 0.9087\n",
      "Epoch 79/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2145 - acc: 0.9203 - val_loss: 0.2366 - val_acc: 0.9061\n",
      "Epoch 80/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2140 - acc: 0.9199 - val_loss: 0.2482 - val_acc: 0.9063\n",
      "Epoch 81/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2123 - acc: 0.9209 - val_loss: 0.2421 - val_acc: 0.9057\n",
      "Epoch 82/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2104 - acc: 0.9213 - val_loss: 0.2504 - val_acc: 0.9001\n",
      "Epoch 83/500\n",
      "22/22 [==============================] - 40s 2s/step - loss: 0.2083 - acc: 0.9222 - val_loss: 0.2377 - val_acc: 0.9056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddaf825850>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fddaf825e50>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fddaf825a50>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9753839720403484, AP:0.16457622300037908')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for window in windows:\n",
    "    def time_series_generator(x,y):\n",
    "        Xn=[]\n",
    "        yn=[]\n",
    "        for n in range(len(x)-time_it):\n",
    "            if n+1>window:\n",
    "                X_train=x[n+1-window:n+1]\n",
    "            else:\n",
    "                X_train=x[0:n+1]\n",
    "                X_train=np.pad(X_train, mode='constant', pad_width=((0,window-X_train.shape[0]),(0,0)),\\\n",
    "                               constant_values=-5)\n",
    "\n",
    "            Xn.append(X_train)\n",
    "            y_train=y[n+time_it]\n",
    "            yn.append(y_train)\n",
    "\n",
    "        return Xn, yn\n",
    "\n",
    "    Xt_time = []\n",
    "    Xt_static = []\n",
    "    yt_time = []\n",
    "\n",
    "    ## positive data gathering\n",
    "    for idx in range(len(list_time_X)):\n",
    "        Xt_time_i = list_time_X[idx]\n",
    "        yt_time_i = list_time_y[idx]\n",
    "        Xt, yt = time_series_generator(Xt_time_i,yt_time_i)\n",
    "        time_static_data = list_time_static[idx]\n",
    "\n",
    "        for n in range(len(Xt)):\n",
    "            Xt_time.append(Xt[n])\n",
    "            Xt_static.append(time_static_data)\n",
    "            yt_time.append(yt[n])\n",
    "\n",
    "    positive_index = [i for i,result in enumerate(yt_time) if result==1]\n",
    "    negative_index = [i for i,result in enumerate(yt_time) if result==0]\n",
    "\n",
    "    positive_x = [Xt_time[idx] for idx in positive_index]\n",
    "    positive_x_static = [Xt_static[idx] for idx in positive_index]\n",
    "    positive_y = [yt_time[idx] for idx in positive_index]\n",
    "\n",
    "    negative_x = [Xt_time[idx] for idx in negative_index]\n",
    "    negative_x_static = [Xt_static[idx] for idx in negative_index]\n",
    "    negative_y = [yt_time[idx] for idx in negative_index]\n",
    "\n",
    "    true_X_time = np.array(positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "    true_X_static = np.array(positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "    true_y_total = np.array(positive_y, dtype=\"float32\").reshape(-1,)\n",
    "    false_X_time = np.array(negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "    false_X_static = np.array(negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "    false_y_total = np.array(negative_y, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "    def val_generator(x,y):\n",
    "        Xn=[]\n",
    "        yn=[]\n",
    "        for n in range(len(x)-time_it):\n",
    "            if n+1>window:\n",
    "                X_train=x[n+1-window:n+1]\n",
    "            else:\n",
    "                X_train=x[0:n+1]\n",
    "                X_train=np.pad(X_train, mode='constant', pad_width=((0,window-X_train.shape[0]),(0,0)),\\\n",
    "                               constant_values=-5)\n",
    "\n",
    "            Xn.append(X_train)\n",
    "            y_train=y[n+time_it]\n",
    "            yn.append(y_train)\n",
    "\n",
    "        return Xn, yn\n",
    "\n",
    "    val_Xt_time = []\n",
    "    val_Xt_static = []\n",
    "    val_yt_time = []\n",
    "\n",
    "    ## positive data gathering\n",
    "    for idx in range(len(valid_time_X)):\n",
    "        Xt_time_i = valid_time_X[idx]\n",
    "        yt_time_i = valid_time_y[idx]\n",
    "        Xt, yt = val_generator(Xt_time_i,yt_time_i)\n",
    "        time_static_data = valid_time_static[idx]\n",
    "\n",
    "        for n in range(len(Xt)):\n",
    "            val_Xt_time.append(Xt[n])\n",
    "            val_Xt_static.append(time_static_data)\n",
    "            val_yt_time.append(yt[n])\n",
    "\n",
    "    val_positive_index = [idx for idx,result in enumerate(val_yt_time) if result==1]\n",
    "    val_negative_index = [idx for idx,result in enumerate(val_yt_time) if result==0]\n",
    "\n",
    "    val_positive_x = [val_Xt_time[idx] for idx in val_positive_index]\n",
    "    val_positive_x_static = [val_Xt_static[idx] for idx in val_positive_index]\n",
    "    val_positive_y = [val_yt_time[idx] for idx in val_positive_index]\n",
    "\n",
    "    val_negative_x = [val_Xt_time[idx] for idx in val_negative_index]\n",
    "    val_negative_x_static = [val_Xt_static[idx] for idx in val_negative_index]\n",
    "    val_negative_y = [val_yt_time[idx] for idx in val_negative_index]\n",
    "\n",
    "    val_true_X_time = np.array(val_positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "    val_true_X_static = np.array(val_positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "    val_true_y_total = np.array(val_positive_y, dtype=\"float32\").reshape(-1,)\n",
    "    val_false_X_time = np.array(val_negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "    val_false_X_static = np.array(val_negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "    val_false_y_total = np.array(val_negative_y, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "    ### valid data generator\n",
    "\n",
    "    test_generator = file_generator_valid(window = window,\n",
    "                                           time_it =time_it, \n",
    "                                           feature = feature,\n",
    "                                           list_time_Xt = test_time_X, \n",
    "                                           list_time_yt = test_time_y, \n",
    "                                           list_time_tstatic = test_time_static, \n",
    "                                           list_time_Xn = test_time_Xn, \n",
    "                                           list_time_yn = test_time_yn, \n",
    "                                           list_time_nstatic = test_time_staticn)\n",
    "\n",
    "    test_X, test_static_X, test_y = test_generator.get_data()\n",
    "\n",
    "    params = {\"window\" : window,\n",
    "               \"time_it\" :time_it, \n",
    "              \"feature\": feature,\n",
    "               \"true_X_time\" : true_X_time, \n",
    "               \"true_X_static\" : true_X_static, \n",
    "               \"true_y_total\" : true_y_total, \n",
    "\n",
    "               \"false_X_time\" : false_X_time, \n",
    "               \"false_X_static\" : false_X_static, \n",
    "               \"false_y_total\" : false_y_total, \n",
    "\n",
    "               \"list_time_Xn\" : list_time_Xn, \n",
    "               \"list_time_yn\" : list_time_yn, \n",
    "               \"list_time_nstatic\" : list_time_nstatic,\n",
    "               \"size\" : 2500,\n",
    "               \"fraction\" : 0.2,\n",
    "                \"ratio\" : 1,\n",
    "               \"repeat\":3}\n",
    "    valparams = {\"window\" : window,\n",
    "               \"time_it\" :time_it, \n",
    "              \"feature\": feature,\n",
    "               \"true_X_time\" : val_true_X_time, \n",
    "               \"true_X_static\" : val_true_X_static, \n",
    "               \"true_y_total\" : val_true_y_total, \n",
    "\n",
    "               \"false_X_time\" : val_false_X_time, \n",
    "               \"false_X_static\" : val_false_X_static, \n",
    "               \"false_y_total\" : val_false_y_total, \n",
    "\n",
    "               \"list_time_Xn\" : valid_time_Xn, \n",
    "               \"list_time_yn\" : valid_time_yn, \n",
    "               \"list_time_nstatic\" : valid_time_staticn,\n",
    "                \"size\" : 600,\n",
    "                \"fraction\" : 0.2,\n",
    "                 \"ratio\" : 1,\n",
    "                \"repeat\":3}\n",
    "\n",
    "    filepath = \".hdf5\"\n",
    "\n",
    "\n",
    "    traingen = DataGenerator(**params)\n",
    "    valid_gen = DataGenerator(**valparams)\n",
    "\n",
    "    #GRU layer\n",
    "    time_input= Input(shape=(None, feature), name='time')\n",
    "    x0=layers.Masking(mask_value=-5)(time_input)\n",
    "\n",
    "    x1=layers.Dense(layer_set[activation_k][0], activation=activation_set[activation_i][0])(x0)\n",
    "    x11=layers.BatchNormalization()(x1)\n",
    "    x12=layers.Dropout(0.5)(x11)\n",
    "\n",
    "    x2=layers.GRU(layer_set[activation_k][1], activation=activation_set[activation_i][1], return_sequences=False)(x12)\n",
    "    x4=layers.BatchNormalization()(x2)\n",
    "    x5=layers.Dropout(0.5)(x4)\n",
    "\n",
    "    #static layer\n",
    "    static_input=Input(shape=(38,),  name='static')\n",
    "    x31 = layers.Dense(layer_set[activation_k][2],activation=activation_set[activation_i][2])(static_input)\n",
    "    x32=layers.BatchNormalization()(x31)\n",
    "    x33=layers.Dropout(0.5)(x32)\n",
    "    #합친 모양\n",
    "    concatenated = layers.concatenate([x5,x33], axis=-1)\n",
    "\n",
    "    x7=layers.Dense(layer_set[activation_k][3], activation=activation_set[activation_i][3])(concatenated)\n",
    "    x8=layers.BatchNormalization()(x7)\n",
    "    x9=layers.Dropout(0.5)(x8)\n",
    "    x10=layers.Dense(1, activation=activation_set[activation_i][4])(x9)\n",
    "\n",
    "    model=Model([time_input,static_input], x10)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=learning_rate[activation_j]), metrics=['accuracy'])\n",
    "\n",
    "    MODEL_SAVE_FOLDER_PATH = file_foler+'_{}/'.format(window)\n",
    "    filename = 'rnn_{}_{}_{}'.format(activation_i,activation_j,activation_k)\n",
    "    if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "      os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "    model_path = MODEL_SAVE_FOLDER_PATH + filename+filepath\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=0,\n",
    "                                 save_best_only=True, mode='min')\n",
    "\n",
    "    rnn_operating_code=model.to_json()\n",
    "    with open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"w\") as json_file : \n",
    "        json_file.write(rnn_operating_code)\n",
    "    history = History()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "\n",
    "    rnn_train = model.fit_generator(generator= traingen,\n",
    "                                     validation_data= valid_gen,\n",
    "                                    steps_per_epoch=int(np.floor(len(baseline)*0.8/1300)),\n",
    "                                    validation_steps=10,\n",
    "                                    nb_epoch = 500, verbose=1, \n",
    "                                    callbacks = [history, checkpoint,early_stopping],\n",
    "                                    workers=-1, use_multiprocessing=True\n",
    "                                   )\n",
    "\n",
    "    json_file = open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"r\")\n",
    "    loaded_model_json = json_file.read() \n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=.003), metrics=['accuracy'])\n",
    "    loaded_model.load_weights(model_path)\n",
    "\n",
    "    model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "    roc_val_test = roc_auc_score(test_y, model_test)\n",
    "    #mAP\n",
    "    test_precision = average_precision_score(test_y, model_test)\n",
    "\n",
    "    history1.append({\"activation\":activation_i,\"window\":window})\n",
    "    history_test_auc.append(roc_val_test)\n",
    "    history_test_precision.append(test_precision)\n",
    "\n",
    "    fig = plt.figure(111)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(rnn_train.history['val_loss'], label='val loss')\n",
    "    ax.plot(rnn_train.history['loss'], label='train_loss')\n",
    "    ax.legend()\n",
    "    plt.title('auc: {}, AP:{}'.format(roc_val_test,test_precision))\n",
    "    plt.savefig(MODEL_SAVE_FOLDER_PATH+filename+\".png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "indexing=pd.MultiIndex.from_frame(pd.DataFrame(history1))\n",
    "\n",
    "outcome_list=pd.DataFrame( {\"history_test_auc\": history_test_auc,\n",
    "              \"history_test_precision\":history_test_precision}, index = indexing)\n",
    "\n",
    "\n",
    "outcome_list.to_csv(file_foler+\"window.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9290255222505227, 0.15433726547743043)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_val_test, test_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이전 연습좌표 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7225"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 37s 2s/step - loss: 0.7713 - acc: 0.6189 - val_loss: 0.3540 - val_acc: 0.8542\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5047 - acc: 0.7933 - val_loss: 0.2983 - val_acc: 0.8835\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3964 - acc: 0.8686 - val_loss: 0.2945 - val_acc: 0.8869\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3371 - acc: 0.8908 - val_loss: 0.2702 - val_acc: 0.8974\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2992 - acc: 0.8997 - val_loss: 0.2613 - val_acc: 0.9074\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2718 - acc: 0.9056 - val_loss: 0.2573 - val_acc: 0.9112\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2537 - acc: 0.9086 - val_loss: 0.2434 - val_acc: 0.9085\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2403 - acc: 0.9115 - val_loss: 0.2316 - val_acc: 0.9111\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2270 - acc: 0.9151 - val_loss: 0.2553 - val_acc: 0.8989\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2172 - acc: 0.9176 - val_loss: 0.2540 - val_acc: 0.9022\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2080 - acc: 0.9198 - val_loss: 0.2366 - val_acc: 0.9078\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2008 - acc: 0.9222 - val_loss: 0.2770 - val_acc: 0.9028\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1948 - acc: 0.9247 - val_loss: 0.2645 - val_acc: 0.9020\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1868 - acc: 0.9275 - val_loss: 0.2484 - val_acc: 0.9069\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1835 - acc: 0.9287 - val_loss: 0.2472 - val_acc: 0.9044\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1781 - acc: 0.9308 - val_loss: 0.2625 - val_acc: 0.9041\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1754 - acc: 0.9321 - val_loss: 0.2970 - val_acc: 0.9004\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1716 - acc: 0.9342 - val_loss: 0.2719 - val_acc: 0.9006\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbb2c33ba90>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbb2c341c50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbb2c341c10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9786469751894183, AP:0.16629098721513508')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7224"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 36s 2s/step - loss: 0.7384 - acc: 0.6310 - val_loss: 0.4765 - val_acc: 0.8458\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4901 - acc: 0.8067 - val_loss: 0.3250 - val_acc: 0.8973\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3757 - acc: 0.8766 - val_loss: 0.2737 - val_acc: 0.9046\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3109 - acc: 0.8972 - val_loss: 0.2550 - val_acc: 0.9096\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2795 - acc: 0.9027 - val_loss: 0.2547 - val_acc: 0.9087\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2592 - acc: 0.9064 - val_loss: 0.2972 - val_acc: 0.8924\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2440 - acc: 0.9092 - val_loss: 0.2480 - val_acc: 0.8966\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2326 - acc: 0.9116 - val_loss: 0.2717 - val_acc: 0.9004\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2220 - acc: 0.9145 - val_loss: 0.2490 - val_acc: 0.8989\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2139 - acc: 0.9164 - val_loss: 0.2783 - val_acc: 0.8941\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2075 - acc: 0.9181 - val_loss: 0.2426 - val_acc: 0.8941\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.2020 - acc: 0.9194 - val_loss: 0.2760 - val_acc: 0.8869\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1964 - acc: 0.9214 - val_loss: 0.2671 - val_acc: 0.8935\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1895 - acc: 0.9245 - val_loss: 0.3072 - val_acc: 0.8912\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1839 - acc: 0.9272 - val_loss: 0.2975 - val_acc: 0.8917\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1786 - acc: 0.9291 - val_loss: 0.3060 - val_acc: 0.8945\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1754 - acc: 0.9301 - val_loss: 0.3113 - val_acc: 0.8904\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1717 - acc: 0.9323 - val_loss: 0.3255 - val_acc: 0.8846\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1671 - acc: 0.9338 - val_loss: 0.3042 - val_acc: 0.8939\n",
      "Epoch 20/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1644 - acc: 0.9353 - val_loss: 0.2996 - val_acc: 0.8985\n",
      "Epoch 21/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.1602 - acc: 0.9374 - val_loss: 0.3255 - val_acc: 0.8938\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbaa6667b90>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbaa6677d50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbaa6677c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9773826709063576, AP:0.16817647861035742')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7255"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 131s 6s/step - loss: 0.5438 - acc: 0.7869 - val_loss: 0.3038 - val_acc: 0.8982\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.2716 - acc: 0.9125 - val_loss: 0.3319 - val_acc: 0.8975\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.2095 - acc: 0.9266 - val_loss: 0.2604 - val_acc: 0.9000\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.1736 - acc: 0.9380 - val_loss: 0.3082 - val_acc: 0.9051\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.1453 - acc: 0.9483 - val_loss: 0.3880 - val_acc: 0.8924\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.1158 - acc: 0.9594 - val_loss: 0.4424 - val_acc: 0.8884\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.1004 - acc: 0.9660 - val_loss: 0.5019 - val_acc: 0.8935\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.0844 - acc: 0.9720 - val_loss: 0.4906 - val_acc: 0.8961\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.0757 - acc: 0.9747 - val_loss: 0.4726 - val_acc: 0.8932\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.0666 - acc: 0.9778 - val_loss: 0.5959 - val_acc: 0.8794\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 130s 6s/step - loss: 0.0607 - acc: 0.9801 - val_loss: 0.5611 - val_acc: 0.8870\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 129s 6s/step - loss: 0.0552 - acc: 0.9819 - val_loss: 0.5820 - val_acc: 0.8844\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 131s 6s/step - loss: 0.0507 - acc: 0.9835 - val_loss: 0.5108 - val_acc: 0.8929\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9fa61c50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9fa6ee10>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fba9fa6ecd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9798095931920139, AP:0.17710040212037842')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7265"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 186s 8s/step - loss: 0.4865 - acc: 0.8106 - val_loss: 0.3235 - val_acc: 0.8894\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.2332 - acc: 0.9232 - val_loss: 0.5103 - val_acc: 0.8603\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.1731 - acc: 0.9396 - val_loss: 0.5858 - val_acc: 0.8590\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.1322 - acc: 0.9534 - val_loss: 0.3768 - val_acc: 0.8934\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.1028 - acc: 0.9647 - val_loss: 0.4642 - val_acc: 0.8909\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.0851 - acc: 0.9712 - val_loss: 0.5748 - val_acc: 0.8790\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 184s 8s/step - loss: 0.0707 - acc: 0.9762 - val_loss: 0.5177 - val_acc: 0.8877\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 184s 8s/step - loss: 0.0629 - acc: 0.9792 - val_loss: 0.5798 - val_acc: 0.8795\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 186s 8s/step - loss: 0.0548 - acc: 0.9821 - val_loss: 0.5419 - val_acc: 0.8881\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.0530 - acc: 0.9824 - val_loss: 0.6330 - val_acc: 0.8793\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 185s 8s/step - loss: 0.0447 - acc: 0.9854 - val_loss: 0.5647 - val_acc: 0.8838\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9ce62790>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fba9ce72950>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fba9ce72850>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'auc: 0.9725251497464504, AP:0.14983103188541194')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "7261"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=22, validation_steps=10, verbose=1, callbacks=[<keras.ca..., workers=-1, use_multiprocessing=True, epochs=500)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22/22 [==============================] - 37s 2s/step - loss: 0.9897 - acc: 0.5192 - val_loss: 0.6384 - val_acc: 0.6357\n",
      "Epoch 2/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.8903 - acc: 0.5571 - val_loss: 0.5609 - val_acc: 0.7660\n",
      "Epoch 3/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.8103 - acc: 0.5935 - val_loss: 0.5423 - val_acc: 0.7974\n",
      "Epoch 4/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.7449 - acc: 0.6294 - val_loss: 0.4998 - val_acc: 0.8383\n",
      "Epoch 5/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6902 - acc: 0.6637 - val_loss: 0.4704 - val_acc: 0.8492\n",
      "Epoch 6/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6477 - acc: 0.6942 - val_loss: 0.4290 - val_acc: 0.8591\n",
      "Epoch 7/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.6132 - acc: 0.7215 - val_loss: 0.3897 - val_acc: 0.8666\n",
      "Epoch 8/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5812 - acc: 0.7466 - val_loss: 0.3738 - val_acc: 0.8698\n",
      "Epoch 9/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5525 - acc: 0.7681 - val_loss: 0.3513 - val_acc: 0.8725\n",
      "Epoch 10/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5259 - acc: 0.7895 - val_loss: 0.3411 - val_acc: 0.8696\n",
      "Epoch 11/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.5064 - acc: 0.8066 - val_loss: 0.3389 - val_acc: 0.8667\n",
      "Epoch 12/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4829 - acc: 0.8232 - val_loss: 0.3267 - val_acc: 0.8733\n",
      "Epoch 13/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4661 - acc: 0.8366 - val_loss: 0.3145 - val_acc: 0.8799\n",
      "Epoch 14/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4490 - acc: 0.8469 - val_loss: 0.3138 - val_acc: 0.8872\n",
      "Epoch 15/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4351 - acc: 0.8558 - val_loss: 0.3165 - val_acc: 0.8846\n",
      "Epoch 16/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4223 - acc: 0.8632 - val_loss: 0.3093 - val_acc: 0.8915\n",
      "Epoch 17/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.4085 - acc: 0.8705 - val_loss: 0.3138 - val_acc: 0.8909\n",
      "Epoch 18/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3968 - acc: 0.8752 - val_loss: 0.3117 - val_acc: 0.8928\n",
      "Epoch 19/500\n",
      "22/22 [==============================] - 35s 2s/step - loss: 0.3880 - acc: 0.8796 - val_loss: 0.3074 - val_acc: 0.8981\n",
      "Epoch 20/500\n",
      "17/22 [======================>.......] - ETA: 7s - loss: 0.3808 - acc: 0.8826"
     ]
    }
   ],
   "source": [
    "history1 = []\n",
    "history_val_auc = []\n",
    "history_val_precision = []\n",
    "history_test_auc = []\n",
    "history_test_precision = []\n",
    "\n",
    "for i in range(len(activation_set)):\n",
    "    for j in range(len(learning_rate)):\n",
    "        for k in range(len(layer_set)):\n",
    "            #GRU layer\n",
    "            time_input= Input(shape=(None, feature), name='time')\n",
    "            x0=layers.Masking(mask_value=-5)(time_input)\n",
    "\n",
    "            x1=layers.Dense(layer_set[k][0], activation=activation_set[i][0])(x0)\n",
    "            x11=layers.BatchNormalization()(x1)\n",
    "            x12=layers.Dropout(0.5)(x11)\n",
    "\n",
    "            x2=layers.GRU(layer_set[k][1], activation=activation_set[i][1], return_sequences=False)(x12)\n",
    "            # x3 = layers.Dense(8)(x2)\n",
    "            x4=layers.BatchNormalization()(x2)\n",
    "            x5=layers.Dropout(0.5)(x4)\n",
    "            # x6=layers.Dense(64)(x5)\n",
    "            #static layer\n",
    "            static_input=Input(shape=(38,),  name='static')\n",
    "            x31 = layers.Dense(layer_set[k][2],activation=activation_set[i][2])(static_input)\n",
    "            x32=layers.BatchNormalization()(x31)\n",
    "            x33=layers.Dropout(0.5)(x32)\n",
    "            #합친 모양\n",
    "            concatenated = layers.concatenate([x5,x33], axis=-1)\n",
    "\n",
    "            x7=layers.Dense(layer_set[k][3], activation=activation_set[i][3])(concatenated)\n",
    "            x8=layers.BatchNormalization()(x7)\n",
    "            x9=layers.Dropout(0.5)(x8)\n",
    "            x10=layers.Dense(1, activation=activation_set[i][4])(x9)\n",
    "\n",
    "            model=Model([time_input,static_input], x10)\n",
    "            model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=learning_rate[j]), metrics=['accuracy'])\n",
    "\n",
    "            MODEL_SAVE_FOLDER_PATH = file_foler+'_{}_{}_{}/'.format(i,j,k)\n",
    "            filename = 'rnn_{}_{}_{}'.format(i,j,k)\n",
    "            if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "              os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "            model_path = MODEL_SAVE_FOLDER_PATH + filename+filepath\n",
    "            checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=0,\n",
    "                                         save_best_only=True, mode='min')\n",
    "\n",
    "            rnn_operating_code=model.to_json()\n",
    "            with open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"w\") as json_file : \n",
    "                json_file.write(rnn_operating_code)\n",
    "            history = History()\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=2, mode='auto')\n",
    "            \n",
    "            rnn_train = model.fit_generator(generator= traingen,\n",
    "                                             validation_data= valid_gen,\n",
    "                                            steps_per_epoch=int(np.floor(len(baseline)*0.8/1300)),\n",
    "                                            validation_steps=10,\n",
    "                                            nb_epoch = 500, verbose=1, \n",
    "                                            callbacks = [history, checkpoint\n",
    "                                                         ,early_stopping\n",
    "                                                        ],\n",
    "                                            workers=-1, use_multiprocessing=True\n",
    "                                           )\n",
    "\n",
    "         \n",
    "            json_file = open(MODEL_SAVE_FOLDER_PATH+filename+\".json\", \"r\")\n",
    "            loaded_model_json = json_file.read() \n",
    "            json_file.close()\n",
    "            loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "            loaded_model.compile(loss=\"binary_crossentropy\", optimizer=adam(lr=.003), metrics=['accuracy'])\n",
    "            loaded_model.load_weights(model_path)\n",
    "            \n",
    "            model_test = loaded_model.predict({\"time\":test_X, \"static\":test_static_X})\n",
    "            roc_val_test = roc_auc_score(test_y, model_test)\n",
    "            #mAP\n",
    "            test_precision = average_precision_score(test_y, model_test)\n",
    "            \n",
    "            history1.append({\"activation set\":i, \"learning rate\":j, \"hyperparamter_num\":k})\n",
    "            history_test_auc.append(roc_val_test)\n",
    "            history_test_precision.append(test_precision)\n",
    "            \n",
    "            fig = plt.figure(111)\n",
    "            ax = plt.subplot(111)\n",
    "            ax.plot(rnn_train.history['val_loss'], label='val loss')\n",
    "            ax.plot(rnn_train.history['loss'], label='train_loss')\n",
    "            ax.legend()\n",
    "            plt.title('auc: {}, AP:{}'.format(roc_val_test,test_precision))\n",
    "            plt.savefig(MODEL_SAVE_FOLDER_PATH+filename+\".png\")\n",
    "            plt.close()\n",
    "            \n",
    "indexing=pd.MultiIndex.from_frame(pd.DataFrame(history1))\n",
    "\n",
    "outcome_list=pd.DataFrame( {\"history_test_auc\": history_test_auc,\n",
    "              \"history_test_precision\":history_test_precision}, index = indexing)\n",
    "\n",
    "\n",
    "outcome_list.to_csv(file_foler+\"accuracy01.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
